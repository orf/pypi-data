{
    "0.0.1": {
        "info": {
            "author": "Phil Wang",
            "author_email": "lucidrains@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3.6",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/lucidrains/flash-attention-jax",
            "keywords": "artificial intelligence,deep learning,transformers,attention mechanism,jax",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "flash-attention-jax",
            "package_url": "https://pypi.org/project/flash-attention-jax/",
            "platform": null,
            "project_url": "https://pypi.org/project/flash-attention-jax/",
            "project_urls": {
                "Homepage": "https://github.com/lucidrains/flash-attention-jax"
            },
            "release_url": "https://pypi.org/project/flash-attention-jax/0.0.1/",
            "requires_dist": [
                "jax (>=0.2.20)"
            ],
            "requires_python": "",
            "summary": "Flash Attention - in Jax",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 14519453,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "67343dbd4dd1860d7cd19319adc9aea1",
                    "sha256": "9af4eb811ecf8f257607ab7c3e7a22ae31de21f9a7018bb9d52a7feb02d431e2"
                },
                "downloads": -1,
                "filename": "flash_attention_jax-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "67343dbd4dd1860d7cd19319adc9aea1",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 5822,
                "upload_time": "2022-07-22T19:11:19",
                "upload_time_iso_8601": "2022-07-22T19:11:19.367502Z",
                "url": "https://files.pythonhosted.org/packages/e0/24/c1d39cb82c954d33bd8e056a52be0ece420caf68386b078ff0a3f89a1eea/flash_attention_jax-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d9a3a73a64e87023f4dd31e5565e2aa7",
                    "sha256": "f503706bdd411c761a36327c98fd6574e0da0060bd7d942f428223a7a94fa5db"
                },
                "downloads": -1,
                "filename": "flash-attention-jax-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "d9a3a73a64e87023f4dd31e5565e2aa7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 5215,
                "upload_time": "2022-07-22T19:11:20",
                "upload_time_iso_8601": "2022-07-22T19:11:20.771422Z",
                "url": "https://files.pythonhosted.org/packages/71/57/76e135bae2de3895aa98e3136ee3a3c4bfb6baa9d90cf98b971d4cafab76/flash-attention-jax-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}