{
    "1.0.0": {
        "info": {
            "author": "",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "SageMaker FeatureStore Spark is a connector library for [Amazon SageMaker FeatureStore](https://aws.amazon.com/sagemaker/feature-store/).\n\nWith this spark connector, you can easily ingest data to FeatureGroup's online and offline store from Spark `DataFrame`. Also, this connector contains the functionality to automatically load feature definitions to help with creating feature groups.\n\n## Getting Started\n\n### Requires\n\nPySpark >= 3.0.0\n\nPython >= 3.6\n\nIf you\u2019re using EMR\n\nEMR release version > 6.x\n\n### Installation\n\nBefore installation, it is recommended to set `SPARK_HOME` environment variable to the path where your Spark is installed, because during installation the library will automatically copy some depedent jars to `SPARK_HOME`.\n\n```\npip3 install sagemaker-feature-store-pyspark --no-binary :all:\n```\nTo learn more info about installation, please also enable verbose mode by appending `--verbose` at the end.\n\n#### SageMaker Notebook\n\nIf you want to try out the connector on SageMaker notebook, extra steps of installation are needed.\n\nSince SageMaker Notebook instances are using older version of Spark which is not compatible with feature store spark connector. We have to override with newer Spark version on SageMaker Notebook instance.\n\nAdd a cell like this\uff1a\n\n```\nimport os\n\noriginal_spark_version = \"2.4.0\"\nos.environ['SPARK_HOME'] = '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark'\n\n# Install a newer versiion of Spark which is compatible with spark library\n!pip3 install pyspark==3.1.2\n!pip3 install sagemaker-feature-store-pyspark --no-binary :all:\n```\n\n#### EMR\n\nIf you are installing the spark connector on EMR, please set `SPARK_HOME` as `/usr/lib/spark` on master node.\n\nNote: If you want to install the dependent jars automatically to `SPARK_HOME`, please do not use EMR\u2019s bootstrap. Simply use custom steps or ssh to the instance directly to finish the installation.\n\n### Using Connector in Development Environment\n\nAfter installing the spark connector, in the Python interpretor:\n\n```\nfrom pyspark.sql import SparkSession\nfrom feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\nimport feature_store_pyspark\n\nextra_jars = \",\".join(feature_store_pyspark.classpath_jars())\nspark = SparkSession.builder \\\n                    .config(\"spark.jars\", jars) \\\n                    .getOrCreate()\n\n# Construct test DataFrame\ncolumns = [\"RecordIdentifier\", \"EventTime\"]\ndata = [(\"1\",\"2021-03-02T12:20:12Z\"), (\"2\", \"2021-03-02T12:20:13Z\"), (\"3\", \"2021-03-02T12:20:14Z\")]\n\ndf = spark.createDataFrame(data).toDF(*columns)\nfeature_store_manager= FeatureStoreManager()\n \n# Load the feature definitions from input schema. The feature definitions can be used to create a feature group\nfeature_definitions = feature_store_manager.load_feature_definitions_from_schema(df)\n\nfeature_group_arn = \"YOUR_FEATURE_GROUP_ARN\"\n\n# Ingest by default\nfeature_store_manager.ingest_data(input_data_frame=df, feature_group_arn=feature_group_arn)\n\n# Offline store direct ingestion, flip the flag of direct_offline_store\nfeature_store_manager.ingest_data(input_data_frame=df, feature_group_arn=feature_group_arn, direct_offline_store=true)\n```\n\n### Submitting Spark Job\n\nWhen submitting the spark job, please make sure the dependent jars are added to the classpath.\n\nIf you did not specify the `SPARK_HOME` during installation, `feature-store-pyspark-dependency-jars` is a python script installed by spark library to automatically fetch the paths to all jars needed for you.\n\n```\nspark-submit --jars `feature-store-pyspark-dependency-jars` PATH_TO_YOUR_SPARK_PYTHON_SCRIPT\n```\n\nIf you are running application on EMR, it\u2019s recommended to run the application in client mode, so that you do not need to distribute the dependent jars to other task nodes. Add one more step in EMR cluster with Spark argument like this:\n```\nspark-submit --deploy-mode client --master yarn PATH_TO_YOUR_SPARK_PYTHON_SCRIPT\n```",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "sagemaker-feature-store-pyspark",
            "package_url": "https://pypi.org/project/sagemaker-feature-store-pyspark/",
            "platform": "",
            "project_url": "https://pypi.org/project/sagemaker-feature-store-pyspark/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/sagemaker-feature-store-pyspark/1.0.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Amazon SageMaker FeatureStore PySpark Bindings",
            "version": "1.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12335757,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "47c3f0ca1a59b63c16bb8b9a67f52775",
                    "sha256": "24f0181e7516651c2787f7472c609320b8924ee85f49a7117bb2116e0f086523"
                },
                "downloads": -1,
                "filename": "sagemaker_feature_store_pyspark-1.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "47c3f0ca1a59b63c16bb8b9a67f52775",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 93180065,
                "upload_time": "2021-12-17T01:22:55",
                "upload_time_iso_8601": "2021-12-17T01:22:55.255263Z",
                "url": "https://files.pythonhosted.org/packages/9d/8d/055c83e46ea7e4db3f2ca5df9ba739db25f8fec152785e4d2592f0cd80bb/sagemaker_feature_store_pyspark-1.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}