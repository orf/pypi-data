{
    "0.1": {
        "info": {
            "author": "Avishek Das",
            "author_email": "avishek.das.ayan@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description": "# Bangla Feature Extractor(BFE)\n\nBFE is a Bangla Natural Language Processing based feature extractor.\n\n\n## Current Features\n\n  1. [CountVectorizer](#1-countvectorizer)\n  2. [TfIdf](#2-tfidf)\n  3. [Word Embedding](#3-word-embedding)\n      * [Word2Vec](#word2vec)\n      * [FastText](#fasttext)\n\n## Installation\n```\npip install bfe\n```\n## Example\n### 1. CountVectorizer\n  - Fit n Transform\n  - Transform\n  - Get Wordset\n\n**Fit n Transform**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nX = ct.fit_transform(X) # X is the word features\n#Output: the countVectorized matrix form of given features\n```\n\n**Transform**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nget_mat = ct.transform(\"\u09b0\u09be\u09b9\u09be\u09a4\")\n#Output: the countVectorized matrix form of given word\n```\n\n**Get Wordset**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nct.get_wordSet()\n#Output: get the raw wordset used in training model\n```\n\n### 2. TfIdf\n  - Fit n Transform\n  - Transform\n  - Coefficients\n\n **Fit n Transform**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"\u0995\u09be\u0993\u099b\u09be\u09b0 \u0986\u09b9\u09ae\u09c7\u09a6\", \"\u09b6\u09c1\u09ad \u09b9\u09be\u0987\u09a6\u09be\u09b0\"]\nmatrix1 = k.fit_transform(doc)\nprint(matrix1)\n\n'''\nOutput: \n[[0.150515 0.150515 0.       0.      ]\n [0.       0.       0.150515 0.150515]]\n'''\n```\n**Transform**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"\u0986\u09b9\u09ae\u09c7\u09a6 \u09b8\u09c1\u09ae\u09a8\", \"\u0995\u09be\u0993\u099b\u09be\u09b0 \u0995\u09b0\u09bf\u09ae\"]\nmatrix2 = k.transform(doc)\nprint(matrix2)\n\n'''\nOutput: \n[[0.150515 0.       0.       0.      ]\n [0.       0.150515 0.       0.      ]]\n'''\n```\n**Coefficients**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"\u0995\u09be\u0993\u099b\u09be\u09b0 \u0986\u09b9\u09ae\u09c7\u09a6\", \"\u09b6\u09c1\u09ad \u09b9\u09be\u0987\u09a6\u09be\u09b0\"]\nk.fit_transform(doc)\nwordset, idf = k.coefficients()\nprint(wordset)\n#Output: ['\u0986\u09b9\u09ae\u09c7\u09a6', '\u0995\u09be\u0993\u099b\u09be\u09b0', '\u09b9\u09be\u0987\u09a6\u09be\u09b0', '\u09b6\u09c1\u09ad']\n\nprint(idf)\n'''\nOutput: \n{'\u0986\u09b9\u09ae\u09c7\u09a6': 0.3010299956639812, '\u0995\u09be\u0993\u099b\u09be\u09b0': 0.3010299956639812, '\u09b9\u09be\u0987\u09a6\u09be\u09b0': 0.3010299956639812, '\u09b6\u09c1\u09ad': 0.3010299956639812}\n'''\n\n```\n\n### 3. Word Embedding\n- ### Word2Vec\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n    - Get Similarity Plot\n\n**Training**\n```py\nfrom bfe import BN_Word2Vec\n#Training Against Sentences\nw2v = BN_Word2Vec(sentences=[['\u0986\u09ae\u09be\u09b0', '\u09aa\u09cd\u09b0\u09bf\u09df', '\u099c\u09a8\u09cd\u09ae\u09ad\u09c2\u09ae\u09bf'], ['\u09ac\u09be\u0982\u09b2\u09be', '\u0986\u09ae\u09be\u09b0', '\u09ae\u09be\u09a4\u09c3\u09ad\u09be\u09b7\u09be']])\nw2v.train_Word2Vec()\n\n#Training Against one Dataset\nw2v = BN_Word2Vec(corpus_file=\"path to data or txt file\")\nw2v.train_Word2Vec()\n\n#Training Against Multiple Dataset\n'''\n    path\n      ->data\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nw2v = BN_Word2Vec(corpus_path=\"path/data\")\nw2v.train_Word2Vec(epochs=25)\n```\nAfter training is done the model \"w2v_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you use any pretrained model, specify it while initializing BN_Word2Vec() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_wordVector('\u0986\u09ae\u09be\u09b0')\n```\n\n**Get Similarity**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_similarity('\u09a2\u09be\u0995\u09be', '\u09b0\u09be\u099c\u09a7\u09be\u09a8\u09c0')\n\n#Output: 67.457879\n```\n\n**Get n Similar Words**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_n_similarWord(['\u09aa\u09a6\u09cd\u09ae\u09be'], n=10)\n#Output: \n'''\n[('\u09b8\u09c7\u09a4\u09c1\u09b0', 0.5857524275779724),\n ('\u09ae\u09c1\u09b2\u09ab\u09ce\u0997\u099e\u09cd\u099c', 0.5773632526397705),\n ('\u09ae\u09b9\u09be\u09a8\u09a8\u09cd\u09a6\u09be', 0.5634652376174927),\n (\"'\u09aa\u09a6\u09cd\u09ae\u09be\", 0.5617109537124634),\n ('\u0997\u09cb\u09ae\u09a4\u09c0', 0.5605217218399048),\n ('\u09aa\u09a6\u09cd\u09ae\u09be\u09b0', 0.5547558069229126),\n ('\u09a4\u09c1\u09b2\u09b8\u09c0\u0997\u0999\u09cd\u0997\u09be', 0.5274507999420166),\n ('\u09a8\u09a6\u09c0\u09b0', 0.5232067704200745),\n ('\u09b8\u09c7\u09a4\u09c1', 0.5225246548652649),\n ('\u09b8\u09c7\u09a4\u09c1\u09a4\u09c7', 0.5192927718162537)]\n'''\n```\n\n**Get Middle Word**\n\n    Get the probability distribution of the center word given words list.\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_outputWord(['\u09a2\u09be\u0995\u09be\u09df', '\u09ae\u09c3\u09a4\u09cd\u09af\u09c1'], n=2)\n\n#Output:  [(\"\u09b9\u09df\u09c7\u099b\u09c7\u0964',\", 0.05880642), ('\u09b6\u09cd\u09b0\u09ae\u09bf\u0995\u09c7\u09b0', 0.05639163)]\n```\n\n**Get Odd Words**\n\n    Get the most unmatched word out from given words list\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_oddWords(['\u099a\u09be\u09b2', '\u09a1\u09be\u09b2', '\u099a\u09bf\u09a8\u09bf', '\u0986\u0995\u09be\u09b6'])\n\n#Output: '\u0986\u0995\u09be\u09b6' \n```\n\n**Get Similarity Plot**\n\n    Creates a barplot of similar words with their probability \n\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_oddWords(['\u099a\u09be\u09b2', '\u09a1\u09be\u09b2', '\u099a\u09bf\u09a8\u09bf', '\u0986\u0995\u09be\u09b6'])\n```\n\n- ### FastText\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n\n\n**Training**\n```py\nfrom bfe import BN_FastText\n#Training Against Sentences\nft = FastText(sentences=[['\u0986\u09ae\u09be\u09b0', '\u09aa\u09cd\u09b0\u09bf\u09df', '\u099c\u09a8\u09cd\u09ae\u09ad\u09c2\u09ae\u09bf'], ['\u09ac\u09be\u0982\u09b2\u09be', '\u0986\u09ae\u09be\u09b0', '\u09ae\u09be\u09a4\u09c3\u09ad\u09be\u09b7\u09be']])\nft.train_fasttext()\n\n#Training Against one Dataset\nft = FastText(corpus_file=\"path to data or txt file\")\nft.train_fasttext()\n\n#Training Against Multiple Dataset\n'''\n    path\n      ->data\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nft = FastText(corpus_path=\"path/data\")\nft.train_fasttext(epochs=25)\n```\nAfter training is done the model \"ft_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you use any pretrained model, specify it while initializing BN_FastText() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_wordVector('\u0986\u09ae\u09be\u09b0')\n```\n\n**Get Similarity**\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_similarity('\u09a2\u09be\u0995\u09be', '\u09b0\u09be\u099c\u09a7\u09be\u09a8\u09c0')\n\n#Output: 70.56821120\n```\n\n**Get n Similar Words**\n```py\nfrom bfe\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_n_similarWord(['\u09aa\u09a6\u09cd\u09ae\u09be'], n=10)\n#Output: \n'''\n[('\u09aa\u09a6\u09cd\u09ae\u09be\u09df', 0.8103810548782349),\n ('\u09aa\u09a6\u09cd\u09ae\u09be\u09b0', 0.794012725353241),\n ('\u09aa\u09a6\u09cd\u09ae\u09be\u09a8\u09a6\u09c0\u09b0', 0.7747839689254761),\n ('\u09aa\u09a6\u09cd\u09ae\u09be-\u09ae\u09c7\u0998\u09a8\u09be\u09b0', 0.7573559284210205),\n ('\u09aa\u09a6\u09cd\u09ae\u09be.', 0.7470568418502808),\n ('\u2018\u09aa\u09a6\u09cd\u09ae\u09be', 0.7413997650146484),\n ('\u09aa\u09a6\u09cd\u09ae\u09be\u09b8\u09c7\u09a4\u09c1\u09b0', 0.716225266456604),\n ('\u09aa\u09a6\u09cd\u09ae', 0.7154797315597534),\n ('\u09aa\u09a6\u09cd\u09ae\u09b9\u09c7\u09ae', 0.6881639361381531),\n ('\u09aa\u09a6\u09cd\u09ae\u09be\u09ac\u09a4', 0.6682782173156738)]\n'''\n```\n\n**Get Odd Words**\n\n    Get the most unmatched word out from given words list\n```py\nfrom \"package_name\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_oddWords(['\u099a\u09be\u09b2', '\u09a1\u09be\u09b2', '\u099a\u09bf\u09a8\u09bf', '\u0986\u0995\u09be\u09b6'])\n\n#Output: '\u0986\u0995\u09be\u09b6' \n```\n\n**Get Similarity Plot**\n\n    Creates a barplot of similar words with their probability \n\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_oddWords(['\u099a\u09be\u09b2', '\u09a1\u09be\u09b2', '\u099a\u09bf\u09a8\u09bf', '\u0986\u0995\u09be\u09b6'])\n```\n\n\n\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "test-mark",
            "package_url": "https://pypi.org/project/test-mark/",
            "platform": "",
            "project_url": "https://pypi.org/project/test-mark/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/test-mark/0.1/",
            "requires_dist": [
                "markdown",
                "bre"
            ],
            "requires_python": "",
            "summary": "A simple function for mathematical operations",
            "version": "0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 7041780,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "0ab201eb75343413d3595f8a03b850db",
                    "sha256": "1e7f75691ea94a0530489efd8b7cccb9dcda4f2864b66fd1122b6417413888d0"
                },
                "downloads": -1,
                "filename": "test_mark-0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "0ab201eb75343413d3595f8a03b850db",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 4494,
                "upload_time": "2020-04-17T17:05:26",
                "upload_time_iso_8601": "2020-04-17T17:05:26.501132Z",
                "url": "https://files.pythonhosted.org/packages/62/18/322898fdd5ac334632316ee87cf2672075f5c53ba54ea883627817ca8fc6/test_mark-0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}