{
    "1.2": {
        "info": {
            "author": "Saimon Hossain",
            "author_email": "saimoncse19@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "License :: OSI Approved :: MIT License",
                "Natural Language :: Bengali",
                "Natural Language :: English",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3 :: Only",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8"
            ],
            "description": "# BLTK: The Bengali Natural Language Processing Toolkit\n\nA lightweight but robust toolkit for processing Bengali Language.\n\n[![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=102)](https://github.com/ellerbrock/open-source-badge/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/)\n\n\n## Overview\nBLTK is a lightweight but exceptionally robust language processing toolkit for the Bengali Language. I, Mr. Saimon Hossain, along with my friend, Mr. Liton Shil, have conducted research as a part of our undergraduate thesis under the supervision of our respected sir, Mr. Sowmitra Das. This is the outcome of our 6 month long research & development project. \n\nI have choosen this name after taking inspiration from the popular natural language processing toolkit - **NLTK**.\n\nBLTK is still in its childhood. It's maturing everyday. It'll receive updates in the days to come. \n\nIf you want to contribute to BLTK's growth, please read the contribution section at the end of this page. \n\n## Supported Functionalities\n- Word Tokenization\n- Sentence Tokenization\n- Sentence Splitting\n- Stopwords Filtering\n- Statistical Part-of-speech Tagging\n- Phrase Chunking/Named-Entity Recognition\n- Stemming\n\n\n\n## Installation\nTo get BLTK up and running, run the following command.\n```sh\npip install bltk\n```\n\n## Usage\n\n### 1) The Bengali Characters\nIn **BLTK**, the ***banglachars*** module contains 7 lists of characters specific to the Bengali Language.\n1. vowels\n2. vowel signs\n3. consonants\n4. digits\n5. punctuation marks\n6. operators\n7. others\n\n#### Code\n```python\nfrom bltk.langtools.banglachars import (vowels,\n                                        vowel_signs,\n                                        consonants,\n                                        digits,\n                                        operators,\n                                        punctuations,\n                                        others)\nprint(f'Vowels: {vowels}')\nprint(f'Vowel signs: {vowel_signs}')\nprint(f'Consonants: {consonants}')\nprint(f'Digits: {digits}')\nprint(f'Operators: {operators}')\nprint(f'Punctuation marks: {punctuations}')\nprint(f'Others: {others}')\n```\n\n#### Output\n```\nVowels: ['\u0985', '\u0986', '\u0987', '\u0988', '\u0989', '\u098a', '\u098b', '\u098c', '\u098f', '\u0990', '\u0993', '\u0994']\nVowel signs: ['\u09be', '\u09bf', '\u09c0', '\u09c1', '\u09c2', '\u09c3', '\u09c4', '\u09c7', '\u09c8', '\u09cb', '\u09cc']\nConsonants: ['\u0995', '\u0996', '\u0997', '\u0998', '\u0999', '\u099a', '\u099b', '\u099c', '\u099d', '\u099e', '\u099f', '\u09a0', '\u09a1', '\u09a2', '\u09a3', '\u09a4', '\u09a5', '\u09a6', '\u09a7', '\u09a8', '\u09aa', '\u09ab', '\u09ac', '\u09ad', '\u09ae', '\u09af', '\u09b0', '\u09b2', '\u09b6', '\u09b7', '\u09b8', '\u09b9', '\u09dc', '\u09dd', '\u09df', '\u09ce', '\u0982', '\u0983', '\u0981']\nDigits: ['\u09e6', '\u09e7', '\u09e8', '\u09e9', '\u09ea', '\u09eb', '\u09ec', '\u09ed', '\u09ee', '\u09ef']\nOperators: ['=', '+', '-', '*', '/', '%', '<', '>', '\u00d7', '\u00f7']\nPunctuation marks: ['\u0964', ',', ';', ':', '?', '!', \"'\", '.', '\"', '-', '[', ']', '{', '}', '(', ')', '\u2013', '\u2014', '\u2015', '~']\nOthers: ['\u09f3', '\u09fa', '\u09cd', '\u0980', '\u09bd', '#', '$']\n\n```\n\n\n\n### 2) Word Tokenization\nIn **BLTK**, the ***word_tokenizer(text: str)*** method of the Tokenizer class performs word tokenization. It takes a text string and returns a list of tokenized words. The Following code shows how it is done.\n\n#### Code\n```python\nfrom bltk.langtools import Tokenizer\n\n# Sample text\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \"\\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\n# Creating an instance\ntokenizer = Tokenizer()\n\n# Tokenizing words\nprint('TOKENIZED WORDS')\nwords = tokenizer.word_tokenizer(text)\nprint(words)\n```\n\n#### Output\n```\nTOKENIZED WORDS\n['\u0986\u09ae\u09bf', '\u099c\u09be\u09a8\u09bf', '\u0986\u09ae\u09be\u09b0', '\u098f\u0987', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u099c\u09a8\u09cd\u09af', '\u0986\u09ae\u09be\u0995\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09b9\u09ac\u09c7', ',', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u0964', '\u09b2\u09bf\u0996\u09c7', '\u0996\u09c1\u09ac', '\u0995\u09be\u099c', '\u09b9\u09df', '\u09b8\u09c7', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u0986\u09ae\u09be\u09b0', '\u09b9\u09be\u09a4\u09c7', '\u0996\u09c1\u09ac', '\u09ac\u09c7\u09b6\u09c0', '\u09a8\u09c7\u0987', '\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09a8\u09bf\u099c\u09c7\u09b0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09af\u09be\u09df', '\u09b8\u09c7\u099f\u09be\u0987', '\u0986\u09ae\u09be\u09b0', '\u099c\u09a8\u09cd\u09af\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0964']\n\n```\n\n### 3) Sentence Tokenization\nIn Bengali, most of the sentence delimiters are same as in English except full-stop. Statements and imperative sentences are terminated by \u0964 - the Devanagari Danda.\nQuestions and exclamatory sentences are terminated by ? and ! respectively. \n\nIn **BLTK**, the ***sentence_tokenizer(text: str)*** method of the Tokenizer class performs sentence tokenization. It takes a text string and returns a list of tokenized sentences. The Following code shows how it is done.\n\n#### Code\n```python\nfrom bltk.langtools import Tokenizer\n\n# Sample text\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\n# Creating an instance\ntokenizer = Tokenizer()\n\n\n# Tokenizing Sentences\nprint(\"TOKENIZED SENTENCES\")\nsentences = tokenizer.sentence_tokenizer(text)\nprint(sentences)\n```\n\n#### Output\n```\nTOKENIZED SENTENCES\n['\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964', '\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964']\n```\n\n\n\n### 4) Sentence Split\nThe ***sentence_splitter(sentence: list)*** method takes a list of tokened sentences and then splits them into their corresponding list of tokened words with the help of word_tokenizer() method. The return value is a list of tokened words lists.\n\n#### Code\n```python\nfrom bltk.langtools import Tokenizer\n\n# Sample text\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\n# Creating an instance\ntokenizer = Tokenizer()\n\n\n# Tokenizing Sentences\nsentences = tokenizer.sentence_tokenizer(text)\n\nprint(\"SPLIT SENTENCES\")\nsentence_list = tokenizer.sentence_splitter(sentences)\nprint(sentence_list)\n\nprint(\"INDIVIDUAL SENTENCE\")\nfor i in sentence_list:\n    print(i)\n\n```\n\n#### Output\n```\nSPLIT SENTENCES\n[['\u0986\u09ae\u09bf', '\u099c\u09be\u09a8\u09bf', '\u0986\u09ae\u09be\u09b0', '\u098f\u0987', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u099c\u09a8\u09cd\u09af', '\u0986\u09ae\u09be\u0995\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09b9\u09ac\u09c7', ',', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u0964'], ['\u09b2\u09bf\u0996\u09c7', '\u0996\u09c1\u09ac', '\u0995\u09be\u099c', '\u09b9\u09df', '\u09b8\u09c7', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u0986\u09ae\u09be\u09b0', '\u09b9\u09be\u09a4\u09c7', '\u0996\u09c1\u09ac', '\u09ac\u09c7\u09b6\u09c0', '\u09a8\u09c7\u0987', '\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09a8\u09bf\u099c\u09c7\u09b0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09af\u09be\u09df', '\u09b8\u09c7\u099f\u09be\u0987', '\u0986\u09ae\u09be\u09b0', '\u099c\u09a8\u09cd\u09af\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0964']]\n\nINDIVIDUAL SENTENCE\n\n['\u0986\u09ae\u09bf', '\u099c\u09be\u09a8\u09bf', '\u0986\u09ae\u09be\u09b0', '\u098f\u0987', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u099c\u09a8\u09cd\u09af', '\u0986\u09ae\u09be\u0995\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09b9\u09ac\u09c7', ',', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u0964']\n['\u09b2\u09bf\u0996\u09c7', '\u0996\u09c1\u09ac', '\u0995\u09be\u099c', '\u09b9\u09df', '\u09b8\u09c7', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u0986\u09ae\u09be\u09b0', '\u09b9\u09be\u09a4\u09c7', '\u0996\u09c1\u09ac', '\u09ac\u09c7\u09b6\u09c0', '\u09a8\u09c7\u0987', '\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09a8\u09bf\u099c\u09c7\u09b0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09af\u09be\u09df', '\u09b8\u09c7\u099f\u09be\u0987', '\u0986\u09ae\u09be\u09b0', '\u099c\u09a8\u09cd\u09af\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0964']\n\n\n```\n\n\n### 5) Stopwords Filtering\n**BLTK's** ***remove_stopwords(words: list, *,  level: str = \"soft\")*** function by default performs a soft stopwords elimination. It takes two parameters: **a list of words** and a **keyword argument** which can be either 'soft', 'moderate' or 'hard'. If no parameter is given, a soft elimination is performed.\n\nFiltering stopwords is not always an ideal choice. In any language, there is no universal list of stop words, and sometimes different researchers use different methods for eliminating stopwords. If you are not sure about which level to use, use the default.\n\n\n#### Code\n```python\nfrom bltk.langtools import remove_stopwords\nfrom bltk.langtools import Tokenizer\n\ntokenizer = Tokenizer()\n\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\ntokened_words = tokenizer.word_tokenizer(text)\n\nprint(f\"Len of words: {len(tokened_words)}\")\nprint(f\"After soft elimination: {(remove_stopwords(tokened_words))}\")\nprint(f\"Length after soft elimination: {len(remove_stopwords(tokened_words))}\")\nprint(f\"After moderate elimination: {(remove_stopwords(tokened_words, level='moderate'))}\")\nprint(f\"Length after moderate elimination: {len(remove_stopwords(tokened_words, level='moderate'))}\")\nprint(f\"After hard elimination: {(remove_stopwords(tokened_words, level='hard'))}\")\nprint(f\"Length after hard elimination: {len(remove_stopwords(tokened_words, level='hard'))}\")\n\n```\n\n#### Output\n```\nLen of words: 40\nAfter soft elimination: ['\u099c\u09be\u09a8\u09bf', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u09b2\u09bf\u0996\u09c7', '\u0995\u09be\u099c', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u09b9\u09be\u09a4\u09c7', '\u09ac\u09c7\u09b6\u09c0', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09b8\u09c7\u099f\u09be\u0987', '\u0985\u09a8\u09c7\u0995']\nLength after soft elimination: 20\nAfter moderate elimination: ['\u099c\u09be\u09a8\u09bf', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u09b2\u09bf\u0996\u09c7', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u09b9\u09be\u09a4\u09c7', '\u09ac\u09c7\u09b6\u09c0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u09af\u09be\u09df', '\u099c\u09a8\u09cd\u09af\u09c7']\nLength after moderate elimination: 15\nAfter hard elimination: ['\u099c\u09be\u09a8\u09bf', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u09b2\u09bf\u0996\u09c7', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u09b9\u09be\u09a4\u09c7', '\u09ac\u09c7\u09b6\u09c0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0']\nLength after hard elimination: 13\n\n```\n\n\n### 6) Statistical Part-of-speech Tagging\n**BLTK** includes a statistical POS tagger which has an overall system accuracy of 95.9%.\nThe POS tagger works in sentence-level which means that instead of tagging a word individually, it tags words in a sentence or a phrase, taking features such as ***previous word*** and ***next word*** into consideration. It relies on the Logistic Regression classifier. \n\nThe BLTK's PosTagger class has a method ***pos_tag()*** which takes a list of split sentences and returns a list of tagged sentences. \nEach tagged sentence is a list of ***tuples of length 2*** each, where the first index of the tuple holds the word itself and the last index holds its corresponding tag.\n\n\n#### Code\n```python\nfrom bltk.langtools import PosTagger\nfrom bltk.langtools import Tokenizer\n\npos_tagger = PosTagger()\ntokenizer = Tokenizer()\n\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\ntoken_text = tokenizer.sentence_tokenizer(text)\n\n\npos_tags = []\nfor text in token_text:\n    tokened = tokenizer.word_tokenizer(text)\n    tagged = pos_tagger.pos_tag(tokened)\n    pos_tags.append(tagged)\nprint(pos_tags)\n```\n\n#### Output\n```\n[[('\u0986\u09ae\u09bf', 'PPR'), ('\u099c\u09be\u09a8\u09bf', 'VM'), ('\u0986\u09ae\u09be\u09b0', 'PPR'), ('\u098f\u0987', 'DAB'), ('\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', 'NC'), ('\u099c\u09a8\u09cd\u09af', 'PP'), ('\u0986\u09ae\u09be\u0995\u09c7', 'PPR'), ('\u0985\u09a8\u09c7\u0995', 'JQ'), ('\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', 'NC'), ('\u09b6\u09c1\u09a8\u09a4\u09c7', 'VM'), ('\u09b9\u09ac\u09c7', 'VA'), (',', 'PU'), ('\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', 'ALC'), ('\u09b2\u09bf\u0996\u099b\u09bf', 'VM'), ('\u0964', 'PU')], [('\u09b2\u09bf\u0996\u09c7', 'VM'), ('\u0996\u09c1\u09ac', 'JQ'), ('\u0995\u09be\u099c', 'NC'), ('\u09b9\u09df', 'VM'), ('\u09b8\u09c7', 'PPR'), ('\u09b0\u0995\u09ae', 'NC'), ('\u0989\u09a6\u09be\u09b9\u09b0\u09a3', 'NC'), ('\u0986\u09ae\u09be\u09b0', 'PPR'), ('\u09b9\u09be\u09a4\u09c7', 'NC'), ('\u0996\u09c1\u09ac', 'JQ'), ('\u09ac\u09c7\u09b6\u09c0', 'JJ'), ('\u09a8\u09c7\u0987', 'VM'), ('\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', 'CSB'), ('\u0985\u09a8\u09cd\u09a4\u09a4', 'CSB'), ('\u09a8\u09bf\u099c\u09c7\u09b0', 'PRF'), ('\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', 'NST'), ('\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', 'NC'), ('\u09ac\u09c7\u09b0', 'VM'), ('\u0995\u09b0\u09be', 'NV'), ('\u09af\u09be\u09df', 'VM'), ('\u09b8\u09c7\u099f\u09be\u0987', 'PPR'), ('\u0986\u09ae\u09be\u09b0', 'PPR'), ('\u099c\u09a8\u09cd\u09af\u09c7', 'PP'), ('\u0985\u09a8\u09c7\u0995', 'JQ'), ('\u0964', 'PU')]]\n\n\n```\n\n\n\n### 7) Phrase Chunking/Named-Entity Recognition\n**BLTK's** phrase chunker can find out all the phrases in a given text as long as a correct grammatical syntax for that phrase is provided in the form of a regular expression. The performance of the chunker is unparalleled since it heavily relies on the BLTK's POS Tagger which has an outstanding accuracy and NLTK's Regular Expression Parser which is extremely powerful. \n\nBLTK's Chunker class has method named ***chunk()*** that takes two parameters: ***a grammar*** in the form of regular expression, and ***a text*** from which phrases will be extracted.\n\nThis section explains how to create a noun phrase chunker using BLTK's Chunker class and a regular expression grammar. A noun phrase begins with an optional demonstrative, followed by zero or more adjectives/quantifiers and terminates with a noun.\nSome examples of Bangla noun phrases are given below:\n\nNP: (NP \u0997\u09a3\u09a4\u09a8\u09cd\u09a4\u09cd\u09b0/NC) - a noun phrase with only one noun.\n    \nNP: (NP \u09ae\u09be\u09a8\u09ac\u09bf\u0995/JJ \u09ac\u09cb\u09a7/NC) - a noun phrase with an adjective followed by a noun.\n    \nNP: (NP \u098f\u0987/DAB \u09b8\u09c1\u09a8\u09cd\u09a6\u09b0/JJ \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0/NC) - a noun phrase with a demonstrative, followed by an adjective and terminated by a noun.\n\n\nThe grammar for extracting Bangla noun phrases can be constructed with the following regular expression.\n\n```\n NP: {<DAB|DRL>?<JJ|JQ>*<N.>} \n```\nThe tags used to construct the grammar as well as training the POS Tagger have been explained in the following table as specified by reserachers at **Microsof Research, India**. Grammars for verb phrases, named entities, etc. can also be constructed in the similar fashion. \n\n| Name                   | Tag     | Example    |\n|------------------------|---------|------------|\n| COMMON NOUN            | NC      |  \u09ae\u09be\u09a8\u09c1\u09b7      |\n| PROPER NOUN            | NP      | \u09b0\u09ac\u09c0\u09a8\u09cd\u09a6\u09cd\u09b0\u09a8\u09be\u09a5  |\n| VERBAL NOUN            | NV      | \u0998\u099f\u09be\u09a8\u09cb      |\n| SPATIO-TEMPORAL NOUN   | NST     | \u0989\u09aa\u09b0\u09c7       |\n| MAIN VERB              | VM      | \u0995\u09b0\u099b\u09bf\u09b2\u09c7\u09a8    |\n| AUXILIARY VERB         | VA      | \u098f\u09b8\u09c7        |\n| PRONOMINAL PRONOUN     | PPR     | \u0986\u09ae\u09be\u09a6\u09c7\u09b0     |\n| REFLEXIVE PRONOUN      | PRF     | \u09a8\u09bf\u099c        |\n| RECIPROCAL PRONOUN     | PRC     | \u09aa\u09b0\u09b8\u09cd\u09aa\u09b0      |\n| RELATIVE PRONOUN       | PRL     | \u09af\u09be\u09b9\u09be\u09b0      |\n| WH-PRONOUN             | PWH     | \u0995\u09c7\u09a8        |\n| ADJECTIVE              | JJ      | \u0997\u09c1\u09b0\u09c1\u09a4\u09cd\u09ac\u09aa\u09c2\u09b0\u09cd\u09a3    |\n| QUANTIFIER             | JQ      | \u0995\u09df\u09c7\u0995\u099f\u09bf     |\n| ABSOLUTE DEMONSTRATIVE | DAB     | \u098f\u0987         |\n| RELATIVE DEMONSTRATIVE | DRL     | \u09af\u09c7         |\n| WH-DEMONSTRATIVE       | DWH     | \u0995\u09c0         |\n| ADVERB of MANNER       | AMN     | \u0986\u09ac\u09be\u09b0       |\n| ADVERB of LOCATION     | ALC     | \u09af\u0996\u09a8        |\n| CONDITIONAL PARTICIPLE | LC      | \u09b9\u09b2\u09c7\u0987       |\n| VERBAL PARTICIPLE      | LV      | \u09ac\u0987\u09a4\u09c7-\u09ac\u0987\u09a4\u09c7\u0987 |\n| POSTPOSITION           | PP      | \u099c\u09a8\u09cd\u09af        |\n| COORDINATING PARTICLE  | CCD     | \u098f\u09ac\u0982        |\n| SUBORDINATING PARTICLE | CSB     | \u09b8\u09c1\u09a4\u09b0\u09be\u0982      |\n| CLASSIFIER PARTICLE    | CCL     | \u09aa\u09cd\u09b0\u09ae\u09c1\u0996       |\n| INTERJECTION           | CIN     | \u0986\u09b0\u09c7        |\n| OTHER PARTICLE         | CX      | \u09a4\u09be\u0987        |\n| PUNCTUATION            | PU      | \u09f7          |\n| FOREIGN WORD           | RDF     | Schedule   |\n| SYMBOL                 | RDS     | \\$         |\n| OTHER                  | RDX     | \u09e9\u09eb\u09ec        |\n\n\n\nLike grammar for noun phrases, grammar for verb phrases, postpositional phrases, etc. can be constructed with valid regular expressions.\n\n#### Code\n```python\nfrom bltk.langtools import Tokenizer\nfrom bltk.langtools import Chunker\n\n\ngrammar = r\"\"\"\n  NP: {<DAB>?<JJ|JQ>*<N.>}      \n  \"\"\"\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\ntokenizer = Tokenizer()\nsentences = tokenizer.sentence_tokenizer(text)\ntokened_text = [tokenizer.word_tokenizer(sentence) for sentence in sentences]\n\nnoun_phrases = []\nfor t in tokened_text:\n    chunky = Chunker(grammar=grammar, tokened_text=t)\n    chunk_tree = chunky.chunk()\n    for i in chunk_tree.subtrees():\n        if i.label() == \"NP\":\n            print(i)\n            noun_phrases.append(i)\n```\n\n#### Output\n```\n(NP \u098f\u0987/DAB \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0/NC)\n(NP \u0985\u09a8\u09c7\u0995/JQ \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6/NC)\n(NP \u0996\u09c1\u09ac/JQ \u0995\u09be\u099c/NC)\n(NP \u09b0\u0995\u09ae/NC)\n(NP \u0989\u09a6\u09be\u09b9\u09b0\u09a3/NC)\n(NP \u09b9\u09be\u09a4\u09c7/NC)\n(NP \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1/NC)\n(NP \u0995\u09b0\u09be/NV)\n\n```\n\n**Note:** BLTK's Phrase Chunker relies on BLTK's POS Tagger and NLTK's RegexParser. For a complete documentation on ***NLTK's Tree class***, which has been used in its RegexParser, follow [this link](https://www.nltk.org/api/nltk.html#nltk.tree.Tree).\n\n\n\n### 8) Stemming\nBLTK currently supports one stemmer - the **Ugra stemmer**. \nIt relies on some pre-arranged lists of suffixes and BLTK's POS Tagger for stemming Bangla words. The reason POS tagging is done before any stemming is even performed is that eliminating suffixes without determining part-of-speech of the words leads to serious miss-stemming issues.\n\nThe inflectional morpheme '\u0993'  or '\u0987' modifies a word such as  '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993' . Ugra eliminates '\u0993' or '\u0987' from the end of the words and makes sure that after elimination of it, the lengths of the words are greater than or equal to two in terms of the number of characters.\nIt should be noted that if '\u0993' or '\u0987' is an independent word, it's never removed.\n\n\n#### Code\n```python\nfrom bltk.langtools import UgraStemmer\nfrom bltk.langtools import Tokenizer\n\n\ntext = \"\u0986\u09ae\u09bf \u099c\u09be\u09a8\u09bf \u0986\u09ae\u09be\u09b0 \u098f\u0987 \u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0 \u099c\u09a8\u09cd\u09af \u0986\u09ae\u09be\u0995\u09c7 \u0985\u09a8\u09c7\u0995 \u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6 \u09b6\u09c1\u09a8\u09a4\u09c7 \u09b9\u09ac\u09c7, \u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993 \u09b2\u09bf\u0996\u099b\u09bf\u0964 \" \\\n       \"\u09b2\u09bf\u0996\u09c7 \u0996\u09c1\u09ac \u0995\u09be\u099c \u09b9\u09df \u09b8\u09c7 \u09b0\u0995\u09ae \u0989\u09a6\u09be\u09b9\u09b0\u09a3 \u0986\u09ae\u09be\u09b0 \u09b9\u09be\u09a4\u09c7 \u0996\u09c1\u09ac \u09ac\u09c7\u09b6\u09c0 \u09a8\u09c7\u0987 \u0995\u09bf\u09a8\u09cd\u09a4\u09c1 \u0985\u09a8\u09cd\u09a4\u09a4 \u09a8\u09bf\u099c\u09c7\u09b0 \u09ad\u09c7\u09a4\u09b0\u09c7\u09b0 \u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1 \u09ac\u09c7\u09b0 \u0995\u09b0\u09be \" \\\n       \"\u09af\u09be\u09df \u09b8\u09c7\u099f\u09be\u0987 \u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09af\u09c7 \u0985\u09a8\u09c7\u0995\u0964\"\n\nstemmer = UgraStemmer()\ntokenizer = Tokenizer()\ntokenized_text = tokenizer.word_tokenizer(text)\n\nstem = stemmer.stem(tokenized_text)\n\nprint(f\"Before stemming: {tokenized_text}\")\nprint(f'After stemming: {stem}')\n\n\n```\n\n#### Output\n```\nBefore stemming: ['\u0986\u09ae\u09bf', '\u099c\u09be\u09a8\u09bf', '\u0986\u09ae\u09be\u09b0', '\u098f\u0987', '\u09b2\u09c7\u0996\u09be\u099f\u09bf\u09b0', '\u099c\u09a8\u09cd\u09af', '\u0986\u09ae\u09be\u0995\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09b9\u09ac\u09c7', ',', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7\u0993', '\u09b2\u09bf\u0996\u099b\u09bf', '\u0964', '\u09b2\u09bf\u0996\u09c7', '\u0996\u09c1\u09ac', '\u0995\u09be\u099c', '\u09b9\u09df', '\u09b8\u09c7', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u0986\u09ae\u09be\u09b0', '\u09b9\u09be\u09a4\u09c7', '\u0996\u09c1\u09ac', '\u09ac\u09c7\u09b6\u09c0', '\u09a8\u09c7\u0987', '\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09a8\u09bf\u099c\u09c7\u09b0', '\u09ad\u09c7\u09a4\u09b0\u09c7\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad\u099f\u09c1\u0995\u09c1', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09af\u09be\u09df', '\u09b8\u09c7\u099f\u09be\u0987', '\u0986\u09ae\u09be\u09b0', '\u099c\u09a8\u09cd\u09af\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0964']\n\nAfter stemming: ['\u0986\u09ae\u09bf', '\u099c\u09be\u09a8\u09bf', '\u0986\u09ae\u09bf', '\u098f\u0987', '\u09b2\u09c7\u0996\u09be', '\u099c\u09a8\u09cd\u09af', '\u0986\u09ae\u09bf', '\u0985\u09a8\u09c7\u0995', '\u0997\u09be\u09b2\u09ae\u09a8\u09cd\u09a6', '\u09b6\u09c1\u09a8\u09a4\u09c7', '\u09b9\u09ac\u09c7', ',', '\u09a4\u09be\u09b0\u09aa\u09b0\u09c7', '\u09b2\u09bf\u0996\u099b\u09bf', '\u0964', '\u09b2\u09bf\u0996\u09c7', '\u0996\u09c1\u09ac', '\u0995\u09be\u099c', '\u09b9\u09df', '\u09b8\u09c7', '\u09b0\u0995\u09ae', '\u0989\u09a6\u09be\u09b9\u09b0\u09a3', '\u0986\u09ae\u09bf', '\u09b9\u09be\u09a4\u09c7', '\u0996\u09c1\u09ac', '\u09ac\u09c7\u09b6', '\u09a8\u09c7', '\u0995\u09bf\u09a8\u09cd\u09a4\u09c1', '\u0985\u09a8\u09cd\u09a4\u09a4', '\u09a8\u09bf\u099c\u09c7\u09b0', '\u09ad\u09c7\u09a4\u09b0', '\u0995\u09cd\u09b7\u09cb\u09ad', '\u09ac\u09c7\u09b0', '\u0995\u09b0\u09be', '\u09af\u09be\u09df', '\u09b8\u09c7\u099f\u09bf', '\u0986\u09ae\u09bf', '\u099c\u09a8\u09cd\u09af\u09c7', '\u0985\u09a8\u09c7\u0995', '\u0964']\n\n\n```\n\n## Contribution\nIf you want to contribute, please make a pull request and wait for PR confirmation. You can also send me a mail to saimoncse19@gmail.com with the subject **Contributing to BLTK** specifying a little bit about what you are interested to contribute.\n\nContribution can also be made by adding issues.",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "https://github.com/saimoncse19/bltk/archive/v1.2.tar.gz",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/saimoncse19/bltk",
            "keywords": "pos-tagger,pos tagger,phrase chunker,phrase-chunker,stemmer,bengali,natural language processing,Machine learning,NLP",
            "license": "MIT",
            "maintainer": "Saimon Hossain",
            "maintainer_email": "saimoncse19@gmail.com",
            "name": "bltk",
            "package_url": "https://pypi.org/project/bltk/",
            "platform": "",
            "project_url": "https://pypi.org/project/bltk/",
            "project_urls": {
                "Download": "https://github.com/saimoncse19/bltk/archive/v1.2.tar.gz",
                "Homepage": "https://github.com/saimoncse19/bltk"
            },
            "release_url": "https://pypi.org/project/bltk/1.2/",
            "requires_dist": null,
            "requires_python": ">=3.6",
            "summary": "A lightweight but robust toolkit for Bengali Natural Language Processing.",
            "version": "1.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 6798620,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f7039f1968012ed3adeee677c48f37ee",
                    "sha256": "8818d00aeaa0b2fe436299f969d2a09080b6f2a77f72333c15fc69c1cd963ffa"
                },
                "downloads": -1,
                "filename": "bltk-1.2.tar.gz",
                "has_sig": false,
                "md5_digest": "f7039f1968012ed3adeee677c48f37ee",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 17435239,
                "upload_time": "2020-03-12T10:40:01",
                "upload_time_iso_8601": "2020-03-12T10:40:01.508597Z",
                "url": "https://files.pythonhosted.org/packages/17/d9/d8000e8cf8f5df6d39615266b529174e95046b34da1c88636d855c1b16c3/bltk-1.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}