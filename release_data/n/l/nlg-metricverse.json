{
    "0.1.0": {
        "info": {
            "author": "DISI UniBo NLP",
            "author_email": "disi.unibo.nlp@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Education",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "<h1 align=\"center\">nlg-metricverse \ud83c\udf0c</h1>\r\n\r\n<table align=\"center\">\r\n    <tr>\r\n        <td align=\"left\">\ud83d\ude80 Spaceship</td>\r\n        <td align=\"left\">\r\n          <a href=\"https://pypi.org/project/jury\"><img src=\"https://img.shields.io/pypi/v/jury?color=blue\" alt=\"PyPI\"></a>\r\n          <a href=\"https://pypi.org/project/jury\"><img src=\"https://img.shields.io/pypi/pyversions/jury\" alt=\"Python versions\"></a>\r\n          <a href=\"https://www.python.org/\"><img src=\"https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple&logo=python&logoColor=FFF800\" alt=\"Made with Python\"></a>\r\n          <br>\r\n          <a href=\"https://github.com/disi-unibo-nlp/nlg-metricverse/actions\"><img alt=\"Build status\" src=\"https://github.com/disi-unibo-nlp/nlg-metricverse/actions/workflows/ci.yml/badge.svg\"></a>\r\n          <a href=\"https://libraries.io/pypi/jury\"><img alt=\"Dependencies\" src=\"https://img.shields.io/librariesio/github/obss/jury\"></a>\r\n          <a href=\"https://github.com/disi-unibo-nlp/nlg-metricverse/issues\"><img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/disi-unibo-nlp/nlg-metricverse.svg\"></a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td align=\"left\">\ud83d\udc68\u200d\ud83d\ude80 Astronauts</td>\r\n        <td align=\"left\">\r\n          <a href=\"https://github.com/disi-unibo-nlp/nlg-metricverse/\"><img src=\"https://badges.frapsoft.com/os/v1/open-source.svg?v=103\" alt=\"Open Source Love svg1\"></a>\r\n          <a href=\"https://github.com/obss/jury/blob/main/LICENSE\"><img alt=\"License: MIT\" src=\"https://img.shields.io/pypi/l/jury\"></a>\r\n          <a href=\"https://GitHub.com/Nthakur20/StrapDown.js/graphs/commit-activity\"><img src=\"https://img.shields.io/badge/Maintained%3F-yes-green.svg\" alt=\"Maintenance\"></a>\r\n          <a href=\"https://pepy.tech/badge/beir\"><img src=\"https://pepy.tech/badge/beir\" alt=\"Downloads\"></a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td align=\"left\">\ud83d\udef0\ufe0f Training Program</td>\r\n        <td align=\"left\">\r\n          <a href=\"https://colab.research.google.com/drive/1HfutiEhHMJLXiWGT8pcipxT5L2TpYEdt?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td align=\"left\">\ud83d\udcd5 Operating Manual</td>\r\n        <td align=\"left\">\r\n          <a href=\"https://doi.org/10.5281/zenodo.6109838\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.6109838.svg\" alt=\"DOI\"></a>\r\n        </td>\r\n    </tr>\r\n</table>\r\n\r\n<br>\r\n\r\n> One NLG evaluation library to rule them all\r\n\r\n<p align=\"center\">\r\n  <img src=\"./figures/nlgmetricverse_banner.png\" title=\"nlg-metricverse\" alt=\"\">\r\n</p>\r\n\r\n### Explore the universe of Natural Language Generation (NLG) evaluation metrics.\r\nNLG Metricverse is a living collection of NLG metrics in a unified and easy-to-use python environment.\r\n* Reduces the implementational burden, allowing users to easily move from papers to practical applications.\r\n* Increases comparability and replicability of NLG research.\r\n\r\n## Tables Of Contents\r\n- [Motivations](#-motivations)\r\n- [Available Metrics](#-available-metrics)\r\n    - [Hypothesis-Reference Supercluster](#hypothesis-reference-supercluster)\r\n        - [N-gram Overlap Galaxy](#n-gram-overlap-galaxy)\r\n        - [Embedding-based Galaxy](#embedding-based-galaxy)\r\n    - [Hypothesis-only Supercluster](#hypothesis-only-supercluster)\r\n- [Installation](#-installation)\r\n    - [Explore on Hugging Face Spaces](#explore-on-hugging-face-spaces)\r\n- [Quickstart](#-quickstart)\r\n    - [Metric Selection](#metric-selection)\r\n        - [Metric Documentation](#metric-documentation)\r\n        - [Metric Filtering](#metric-filtering)\r\n    - [Metric Usage](#metric-usage)\r\n        - [Prediction-Reference Cardinality](#prediction-reference-cardinality)\r\n        - [Scorer Application](#scorer-application)\r\n        - [Metric-specific Parameters](#metric-specific-parameters)\r\n        - [Outputs](#outputs)\r\n- [Tests](#-tests)\r\n    - [Code Style](#code-style)\r\n- [Custom Metrics](#-custom-metrics)\r\n- [Contributing](#-contributing)\r\n- [Contact](#-contact)\r\n- [License](#license)\r\n\r\n## \ud83d\udca1 Motivations\r\n* \ud83d\udccc Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for quality and are very cheap to compute.\r\n* \ud83d\udccc NLG metrics aims to summarize and quantify the extent to which a model has managed to reproduce or accurately match some gold standard token sequences. Task examples: machine translation, abstractive question answering, single/multi-document summarization, data-to-text, chatbots, image/video captioning, etc.\r\n* \u2620 Different evaluation metrics encode **different properties** and have **different biases and other weaknesses**. Thus, you should choose your metrics carefully depending on your goals and motivate those choices when writing up and presenting your work.\r\n* \u2620 **As NLG models have gotten better over time, evaluation metrics have become an important bottleneck for research in this field**. In fact, areas can stagnate due to poor metrics, so we must be vigilant! It is an increasingly pressing priority to develop and apply better evaluation metrics given the recent advances in neural text generation. You shouldn't feel confined to the most traditional overlap-based metrics. If you're working on an established problem, you'll feel pressure from readers to be conservative and use the metrics that have already been tested for the same task. However, this might be a compelling pressure. Our view is that NLP engineers should enrich their evaluation toolkits with multiple metrics capturing different textual properties, being free to argue against cultural norms and motivate new ones, also exploring the latest contributions focused on semantics.\r\n* \u2620 NLG evaluation is very challenging also because **the relationships between candidate and reference texts tend to be one-to-many or many-to-many**. An artificial text predicted by a model might have multiple human references (i.e., there is more than one effective way to say most things), as well as a model can generate multiple distinct outputs. Such cardinality is crucial, but official implementations tend to neglect it.\r\n* \u2620 New NLG metrics are constantly being proposed in top conferences, but their **implementations (and related features) remain disrupted**, significantly restricting their application. Existing libraries tend to support a very small number of metrics, which mistakenly receive less attention than generative models. The absence of a shared and continuously updated repository makes it difficult to discover alternative metrics and slows down their use on a practical side.\r\n* \ud83c\udfaf NLG Metricverse implements a large number of prominent evaluation metrics in NLG, seeking to articulate the textual properties they encode (e.g., fluency, grammatical correctness, informativeness), tasks, and limits. Understanding, using, and examining a metric has never been easier.\r\n\r\n## \ud83e\ude90 Available Metrics\r\nNLG Metricverse supports X diverse evaluation metrics overall (last update: May X, 2022).\r\n\r\n### Hypothesis-Reference Supercluster\r\n\r\n#### N-gram Overlap Galaxy\r\n\r\n| Metric | Publication Year | Conference | NLG Metricverse | [Jury](https://github.com/obss/jury) | [HF/datasets](https://github.com/huggingface/datasets/tree/master/metrics) | [NLG-eval](https://github.com/Maluuba/nlg-eval) | [TorchMetrics](https://github.com/PyTorchLightning/metrics/tree/master/torchmetrics/text)\r\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\r\n| BLEU | 2002 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\r\n| NIST | 2002 | HLT | :white_check_mark: | :x: | :x: | :x: | :x: |\r\n| ORANGE (SentBLEU) | 2004 | COLING | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| ROUGE | 2004 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\r\n| WER | 2004 | ICSLP | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| CER (TODO) | 2004 | ICSLP | | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| METEOR | 2005 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| CIDEr (TODO) | 2005 | | | :x: | :x: | :white_check_mark: | :x: |\r\n| TER | 2006 | AMTA | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| ChrF(++) | 2015 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| CharacTER (TODO) | 2016 | WMT | | :x: | :x: | :x: | :x: |\r\n| SacreBLEU | 2018 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| METEOR++ (TODO) | 2018 | WMT | | :x: | :x: | :x: | :x: |\r\n| Accuracy (TODO) | / | / | | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| Precision (TODO) | / | / | | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| F1 (TODO) | / | / | | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| MER (TODO) | / | / | | :x: | :x: | :x: | :white_check_mark: |\r\n\r\n#### Embedding-based Galaxy\r\n\r\n| Metric | Publication Year | Conference | NLG Metricverse | [Jury](https://github.com/obss/jury) | [HF/datasets](https://github.com/huggingface/datasets/tree/master/metrics) | [NLG-eval](https://github.com/Maluuba/nlg-eval) | [TorchMetrics](https://github.com/PyTorchLightning/metrics/tree/master/torchmetrics/text)\r\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\r\n| WMD (TODO) | 2015 | ICML | | :x: | :x: | :x: | :x: |\r\n| SMD (TODO) | 2015 | ICML | | :x: | :x: | :x: | :x: |\r\n| MOVERScore | 2019 | ACL | :white_check_mark: | :x: | :x: | :x: | :x: |\r\n| EED (TODO) | 2019 | WMT | | :x: | :x: | :x: | :white_check_mark: |\r\n| COMET | 2020 | EMNLP | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| FactCC(X) (TODO) | 2020 | EMNLP | | :x: | :x: | :x: | :x: |\r\n| BLEURT | 2020 | ACL | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :x: |\r\n| NUBIA (TODO) | 2020 | EvalNLGEval<br>NeurIPS talk | | :x: | :x: | :x: | :x: |\r\n| BERTScore | 2020 | ICLR | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: |\r\n| PRISM (TODO) | 2020 | EMNLP | | :white_check_mark: | :x: | :x: | :x: |\r\n| BARTScore | 2021 | NeurIPS | :white_check_mark: | :white_check_mark: | :x: | :x: | :x: |\r\n| RoMe (TODO) | 2022 | ACL | | :x: | :x: | :x: | :x: |\r\n| InfoLM (TODO) | 2022 | AAAI | | :x: | :x: | :x: | :x: |\r\n| Perplexity (TODO) | / | / | | :x: | :white_check_mark: | :x: | :x: |\r\n| Embedding Cosine Similarity (TODO) | / | / | | :x: | :x: | :white_check_mark: | :x: |\r\n| Vector Extrema (TODO) | / | / | | :x: | :x: | :white_check_mark: | :x: |\r\n| Greedy Matching (TODO) | / | / | | :x: | :x: | :white_check_mark: | :x: |\r\n| Coverage (TODO) | ... | ... | | :x: | :x: | :x: | :x: |\r\n| Density (TODO) | ... | ... | | :x: | :x: | :x: | :x: |\r\n| Compression (TODO) | ... | ... | | :x: | :x: | :x: | :x: |\r\n\r\n### Hypothesis-only Supercluster\r\n\r\n| Metric | Publication Year | Conference | NLG Metricverse | [Jury](https://github.com/obss/jury) | [HF/datasets](https://github.com/huggingface/datasets/tree/master/metrics) | [NLG-eval](https://github.com/Maluuba/nlg-eval) | [TorchMetrics](https://github.com/PyTorchLightning/metrics/tree/master/torchmetrics/text)\r\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\r\n| MAUVE (TODO) | 2021 | NeurIPS | | :x: | :white_check_mark: | :x: | :x: |\r\n| Flesch-Kincaid Readability | ... | ... | :white_check_mark: | :x: | :x: | :x: | :x: |\r\n| Average Unique N-gram Ratios | ... | ... | :white_check_mark: | :x: | :x: | :x: | :x: |\r\n\r\n\r\n## \ud83d\udd0c Installation\r\nInstall from PyPI repository\r\n```\r\npip install nlg-metricverse\r\n```\r\nor build from source\r\n```\r\ngit clone https://github.com/disi-unibo-nlp/nlg-metricverse.git\r\ncd nlg-metricverse\r\npip install -v .\r\n```\r\n\r\n#### Explore on Hugging Face Spaces\r\nThe **Spaces** edition of NLG Metricverse launched on May X, 2022. Check it out here:\r\n[![](./figures/spaces.png)](https://huggingface.co/spaces/disi-unibo-nlp/nlg-metricverse)\r\n\r\n## \ud83d\ude80 Quickstart\r\n\r\nIt is only <b>two lines of code</b> to evaluate generated outputs: <b>(i)</b> instantiate your scorer by selecting the desired metric(s) and <b>(ii)</b> apply it!\r\n\r\n### Metric Selection\r\nSpecify the metrics you want to use on instantiation,\r\n```python\r\nfrom nlgmetricverse import NLGMetricverse\r\n\r\n# If you specify more metrics, each of them will be applyied on your data (allowing for a fast prediction/efficiency comparison)\r\nscorer = NLGMetricverse(metrics=[\"bleu\", \"rouge\"])\r\n```\r\nor directly import metrics from `nlgmetricverse.metrics` as classes, then instantiate and use them as desired.\r\n```python\r\nfrom nlgmetricverse.metrics import BertScore\r\n\r\nscorer = BertScore.construct()\r\n```\r\nYou can seemlessly access both `nlgmetricverse` and HuggingFace `datasets` metrics through `nlgmetricverse.load_metric`.\r\nNLG Metricverse falls back to `datasets` implementation of metrics for the ones that are currently not supported; you can see the metrics available for `datasets` on [datasets/metrics](https://github.com/huggingface/datasets/tree/master/metrics). \r\n```python\r\nimport nlgmetricverse\r\nbleu = nlgmetricverse.load_metric(\"bleu\")\r\n# metrics not available in `nlgmetricverse` but in `datasets`\r\nwer = nlgmetricverse.load_metric(\"competition_math\") # It falls back to `datasets` package with a warning\r\n```\r\nNote: if a selected metric requires specific packages, you'll be invited to install them (e.g., \"bertscore\" \u2192 `pip install bertscore`).\r\n\r\n#### Metric Documentation\r\nTODO\r\n\r\n#### Metric Filtering\r\nTODO\r\n\r\n### Metric Usage\r\n\r\n#### Prediction-Reference Cardinality\r\n<i>1:1</i>. One prediction, one reference ([p<sub>1</sub>, ..., p<sub>n</sub>] and [r<sub>1</sub>, ..., r<sub>n</sub>] syntax).\r\n```python\r\npredictions = [\"Evaluating artificial text has never been so simple\", \"the cat is on the mat\"]\r\nreferences = [\"Evaluating artificial text is not difficult\", \"The cat is playing on the mat.\"]\r\n```\r\n<i>1:M</i>. One prediction, many references ([p<sub>1</sub>, ..., p<sub>n</sub>] and [[r<sub>11</sub>, ..., r<sub>1m</sub>], ..., [r<sub>n1</sub>, ..., r<sub>nm</sub>]] syntax)\r\n```python\r\npredictions = [\"Evaluating artificial text has never been so simple\", \"the cat is on the mat\"]\r\nreferences = [\r\n    [\"Evaluating artificial text is not difficult\", \"Evaluating artificial text is simple\"],\r\n    [\"The cat is playing on the mat.\", \"The cat plays on the mat.\"]\r\n]\r\n```\r\n<i>K:M</i>. Many predictions, many references ([[p<sub>11</sub>, ..., p<sub>1k</sub>], ..., [p<sub>n1</sub>, ..., p<sub>nk</sub>]] and [[r<sub>11</sub>, ..., r<sub>1m</sub>], ..., [r<sub>n1</sub>, ..., r<sub>nm</sub>]] syntax). This is helpful for language models with a decoding strategy focused on diversity (e.g., beam search, temperature sampling).\r\n```python\r\npredictions = [\r\n    [\"Evaluating artificial text has never been so simple\", \"The evaluation of automatically generated text is simple.\"],\r\n    [\"the cat is on the mat\", \"the cat likes playing on the mat\"]\r\n]\r\nreferences = [\r\n    [\"Evaluating artificial text is not difficult\", \"Evaluating artificial text is simple\"],\r\n    [\"The cat is playing on the mat.\", \"The cat plays on the mat.\"]\r\n]\r\n```\r\n\r\n#### Scorer Application\r\n```python\r\nscores = scorer(predictions, references)\r\n```\r\nThe `scorer` automatically selects the proper strategy for applying the selected metric(s) depending on the input format. In any case, if a prediction needs to be compared against multiple references, you can customize the reduction function to use (e.g., `reduce_fn=max` chooses the prediction-reference pair with the highest score for each of the N items in the dataset).\r\n```python\r\nscores = scorer.compute(predictions, references, reduce_fn=\"max\")\r\n```\r\n\r\n#### Metric-specific Parameters\r\nAdditional metric-specific parameters can be specified on `compute()`,\r\n```python\r\n# BertScore example for:\r\n# - specifying which pre-trained BERT model use\r\n# - asking to mount idf weighting\r\nscore = scorer.compute(predictions=predictions, references=references,\r\n                       model_type=\"microsoft/deberta-large-mnli\", idf=True)\r\n```\r\nor alternatively on instantiation.\r\n```python\r\nfrom nlgmetricverse.metrics import BertScore\r\nscorer = BertScore.construct(compute_kwargs={\r\n            \"model_type\": \"microsoft/deberta-large-mnli\",\r\n            \"idf\": True})\r\nscore = scorer.compute(predictions=predictions, references=references)\r\n```\r\n```python\r\nimport nlgmetricverse\r\nscorer = nlgmetricverse.load_metric(\r\n            \"bertscore\",\r\n            resulting_name=\"custom_bertscore\",\r\n            compute_kwargs={\r\n                \"model _type\": \"microsoft/deberta-large-mnli\",\r\n                \"idf\": True})\r\n```\r\n\r\n#### Outputs\r\nTODO\r\n\r\n## \ud83d\udd0e Tests\r\nTODO\r\n\r\n### Code Style\r\nTo check the code style,\r\n```\r\npython tests/run_code_style.py check\r\n```\r\nTo format the codebase,\r\n```\r\npython tests/run_code_style.py format\r\n```\r\n\r\n## \ud83c\udfa8 Custom Metrics\r\nYou can use custom metrics by inheriting `nlgmetricverse.metrics.Metric`.\r\nYou can see current metrics implemented on NLG Metricverse from [nlgmetricverse/metrics](https://github.com/disi-unibo-nlp/nlg-metricverse/tree/main/nlgmetricverse/metrics).\r\nNLG Metricverse itself uses `datasets.Metric` as a base class to drive its own base class as `nlgmetricverse.metrics.Metric`. The interface is similar; however, NLG Metricverse makes the metrics to take a unified input type by handling metric-specific inputs and allowing multiple cardinalities (1:1, 1:M, K:M).\r\nFor implementing custom metrics, both base classes can be used but we strongly recommend using `nlgmetricverse.metrics.Metric` for its advantages.\r\n```python\r\nfrom nlgmetricverse.metrics import MetricForLanguageGeneration\r\n\r\nclass CustomMetric(MetricForLanguageGeneration):\r\n    def _compute_single_pred_single_ref(\r\n        self, predictions, references, reduce_fn = None, **kwargs\r\n    ):\r\n        raise NotImplementedError\r\n\r\n    def _compute_single_pred_multi_ref(\r\n        self, predictions, references, reduce_fn = None, **kwargs\r\n    ):\r\n        raise NotImplementedError\r\n\r\n    def _compute_multi_pred_multi_ref(\r\n            self, predictions, references, reduce_fn = None, **kwargs\r\n    ):\r\n        raise NotImplementedError\r\n```\r\nFor more details, have a look at base metric implementation [nlgmetricverse.metrics.Metric](./nlgmetricverse/metrics/_core/base.py)\r\n\r\n## \ud83d\ude4c Contributing\r\nThanks go to all these wonderful collaborations for their contribution towards the NLG Metricverse library:\r\n\r\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\r\n<!-- prettier-ignore-start -->\r\n<!-- markdownlint-disable -->\r\n<table>\r\n  <tr>\r\n    <td align=\"center\"><a href=\"https://giacomofrisoni.github.io/\"><img src=\"https://github.com/giacomofrisoni.png\" width=\"100px;\" alt=\"\"/><br /><sub><b>Giacomo Frisoni</b></sub></a></td>\r\n    <td align=\"center\"><a href=\"https://andreazammarchi3.github.io/\"><img src=\"https://github.com/andreazammarchi3.png\" width=\"100px;\" alt=\"\"/><br /><sub><b>Andrea Zammarchi</b></sub></a></td>\r\n</table>\r\n\r\n> We are hoping that the open-source community will help us edit the code and make it better!\r\n> Don't hesitate to open issues and contribute the fix/improvement! We can guide you if you're not sure where to start but want to help us out \ud83e\udd47.\r\n> In order to contribute a change to our code base, please submit a pull request (PR) via GitHub and someone from our team will go over it and accept it.\r\n\r\n> If you have troubles, suggestions, or ideas, the [Discussion](https://github.com/disi-unibo-nlp/nlg-metricverse/discussions) board might have some relevant information. If not, you can post your questions there \ud83d\udcac\ud83d\udde8.\r\n\r\n## \u2709 Contact\r\nContact person: Giacomo Frisoni, [giacomo.frisoni@unibo.it](mailto:giacomo.frisoni@unibo.it).\r\n\r\n## License\r\n\r\nThe code is released under the [MIT License](LICENSE). It should not be used to promote or profit from violence, hate, and division, environmental destruction, abuse of human rights, or the destruction of people's physical and mental health.\r\n\r\n\r\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "https://github.com/disi-unibo-nlp/nlg-metricverse/archive/refs/tags/0.1.0.tar.gz",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/disi-unibo-nlp/nlg-metricverse",
            "keywords": "natural-language-processing natural-language-generation nlg-evaluation metrics language-models visualization python pytorch",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "nlg-metricverse",
            "package_url": "https://pypi.org/project/nlg-metricverse/",
            "platform": null,
            "project_url": "https://pypi.org/project/nlg-metricverse/",
            "project_urls": {
                "Download": "https://github.com/disi-unibo-nlp/nlg-metricverse/archive/refs/tags/0.1.0.tar.gz",
                "Homepage": "https://github.com/disi-unibo-nlp/nlg-metricverse"
            },
            "release_url": "https://pypi.org/project/nlg-metricverse/0.1.0/",
            "requires_dist": null,
            "requires_python": ">=3.7",
            "summary": "An End-to-End Library for Evaluating Natural Language Generation.",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 13831904,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f29935acce04859bfa58159ea5f3d432",
                    "sha256": "d074707ffcb7eea4e8d4df9784d86d678de96cfc29a7931a0724666f70533234"
                },
                "downloads": -1,
                "filename": "nlg-metricverse-0.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "f29935acce04859bfa58159ea5f3d432",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 81873,
                "upload_time": "2022-05-16T16:47:31",
                "upload_time_iso_8601": "2022-05-16T16:47:31.605471Z",
                "url": "https://files.pythonhosted.org/packages/20/09/3799d125f6c22cd639bfc11748e7084513f9f1851b363cfc27759cf3baa6/nlg-metricverse-0.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}