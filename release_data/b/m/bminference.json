{
    "0.0.2": {
        "info": {
            "author": "a710128",
            "author_email": "qbjooo@qq.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: C++",
                "Programming Language :: Python :: 3"
            ],
            "description": "# BMInference\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587]\n\n[\u7b80\u4f53\u4e2d\u6587]: ./README-ZH.md\n\nBMInference (Big Model Inference) is a low-resource inference package for large-scale pretrained language models (PLMs).\n\n- **Low Resource.** Instead of running on large-scale GPU clusters, the package enables the running of the inference process for large-scale pretrained language models on personal computers!\n- **Open.** Model parameters and configurations are all publicly released, you don't need to access a PLM via online APIs, just run it on your computer! \n- **Green.** Run pretrained language models with fewer machines and GPUs, also with less energy consumption.\n\n## Demo\nHere we provide an online demo based on the package with CPM2.\n\n## Install\n\n- From source: ``python setup.py install``\n\n- From docker: ``docker build . -f docker/base.Dockerfile``\n\nHere we list the minimum and recommended configurations for running BMInference. \n\n| | Minimum Configuration | Recommended Configuration |\n|-|-|-|\n| Memory | 16GB | 24GB\n| GPU | NVIDIA GeForce GTX 1060 6GB | NVIDIA Tesla V100 16GB\n| PCI-E |  PCI-E 3.0 x16 |  PCI-E 3.0 x16\n\n## Quick Start\n\nHere we provide a esay script for using BMInference. \n\nFirstly, import a model from the model base (e.g. CPM1, CPM2, EVA2).\n```python\nimport bigmodels\ncpm2 = bigmodels.models.CPM2()\n```\n\nThen define the text and use the ``<span>`` token to denote the blank to fill in.\n```python\ntext = \"\u5317\u4eac\u73af\u7403\u5ea6\u5047\u533a\u76f8\u5173\u8d1f\u8d23\u4eba\u4ecb\u7ecd\uff0c\u5317\u4eac\u73af\u7403\u5f71\u57ce\u6307\u5b9a\u5355\u65e5\u95e8\u7968\u5c06\u91c7\u7528<span>\u5236\u5ea6\uff0c\u5373\u63a8\u51fa\u6de1\u5b63\u65e5\u3001\u5e73\u5b63\u65e5\u3001\u65fa\u5b63\u65e5\u548c\u7279\u5b9a\u65e5\u95e8\u7968\u3002<span>\u4ef7\u683c\u4e3a418\u5143\uff0c<span>\u4ef7\u683c\u4e3a528\u5143\uff0c<span>\u4ef7\u683c\u4e3a638\u5143\uff0c<span>\u4ef7\u683c\u4e3a<span>\u5143\u3002\u5317\u4eac\u73af\u7403\u5ea6\u5047\u533a\u5c06\u63d0\u4f9b90\u5929\u6eda\u52a8\u4ef7\u683c\u65e5\u5386\uff0c\u4ee5\u65b9\u4fbf\u6e38\u5ba2\u63d0\u524d\u89c4\u5212\u884c\u7a0b\u3002\"\n```\n\nUse the ``generate`` function to obtain the results and replace ``<span>`` tokens with the results.\n\n```python\nfor result in cpm2.generate(text, \n    top_p=1.0,\n    top_n=10, \n    temperature=0.9,\n    frequency_penalty=0,\n    presence_penalty=0\n):\n    value = result[\"text\"]\n    text = text.replace(\"<span>\", \"\\033[0;32m\" + value + \"\\033[0m\", 1)\nprint(text)\n```\nFinally, you can get the predicted text. For more examples, go to the ``examples`` folder.\n\n## Performances\n\nHere we report the speeds of CPM2 encoder and decoder we have tested on different platforms. You can also run ``benchmark/cpm2/encoder.py`` and ``benchmark/cpm2/decoder.py`` to test the speed on your machine!\n\n| GPU | Encoder Speed (tokens/s) | Decoder Speed (tokens/s) |\n|-|-|-|\n| NVIDIA GeForce GTX 1060 | 533 | 1.6\n| NVIDIA GeForce GTX 1080Ti | 1200 | 12\n\n## Contributing\nLinks to the user community and contributing guidelines.\n\n## License\n\nThe package is released under the [Apache 2.0](./LICENSE) License.\n\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "bminference",
            "package_url": "https://pypi.org/project/bminference/",
            "platform": "",
            "project_url": "https://pypi.org/project/bminference/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/bminference/0.0.2/",
            "requires_dist": null,
            "requires_python": ">=3.6",
            "summary": "A toolkit for big model inference",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 11369551,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "65dad39b1b4cd68af1095b63e37ece2b",
                    "sha256": "d4c9e2f43acdcaa01c082211e8d9bccfca05f3667a1cc559c7156f8cae7057b2"
                },
                "downloads": -1,
                "filename": "bminference-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "65dad39b1b4cd68af1095b63e37ece2b",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 29504,
                "upload_time": "2021-09-05T13:10:06",
                "upload_time_iso_8601": "2021-09-05T13:10:06.457091Z",
                "url": "https://files.pythonhosted.org/packages/61/79/c79ac962f71a2607615146ee8ba8a197184421661e28f6b39a717a9c5150/bminference-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}