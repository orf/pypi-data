{
    "0.4.0": {
        "info": {
            "author": "Intel AIA/AIPC Team",
            "author_email": "feng.tian@intel.com, haihao.shen@intel.com,hanwen.chang@intel.com, penghui.cheng@intel.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "# NLP Toolkit: Optimization for Natural Language Processing (NLP) Models\nNLP Toolkit is a powerful toolkit for automatically applying model optimizations on Natural Language Processing Models. It leverages [Intel\u00ae Neural Compressor](https://intel.github.io/neural-compressor) to provide a variety of model compression techniques: quantization, pruning, distillation and so on.\n\n## What does NLP Toolkit offer?\nThis toolkit allows developers to improve the productivity through ease-of-use model compression APIs by extending HuggingFace transformer APIs for deep learning models in NLP (Natural Language Processing) domain and accelerate the inference performance using compressed models.\n\n- Model Compression\n\n    |Framework          |Quantization |Pruning/Sparsity |Distillation |AutoDistillation |\n    |-------------------|:-----------:|:---------------:|:-----------:|:--------------:|\n    |PyTorch            |&#10004;     |&#10004;         |&#10004;     |&#10004;        |\n    |TensorFlow         |&#10004;     |&#10004;         |Stay tuned :star:|Stay tuned :star:|\n\n- Data Augmentation for NLP Datasets\n- Neural Engine for Reference Deployment\n\n## Getting Started\n### Installation\n#### Install Dependency\n```bash\npip install -r requirements.txt\n```\n\n#### Install NLP Toolkit\n```bash\ngit clone https://github.com/intel-innersource/frameworks.ai.nlp-toolkit.intel-nlp-toolkit.git nlp_toolkit\ncd nlp_toolkit\ngit submodule update --init --recursive\npython setup.py install\n```\n\n### Quantization\n```python\nfrom nlp_toolkit import QuantizationConfig, metric, objectives\nfrom nlp_toolkit.optimization.trainer import NLPTrainer\n\n# Replace transformers.Trainer with NLPTrainer\n# trainer = transformers.Trainer(...)\ntrainer = NLPTrainer(...)\nmetric = metrics.Metric(name=\"eval_f1\", is_relative=True, criterion=0.01)\nq_config = QuantizationConfig(\n    approach=\"PostTrainingStatic\",\n    metrics=[metric],\n    objectives=[objectives.performance]\n)\nmodel = trainer.quantize(quant_config=q_config)\n```\n\nPlease refer to [quantization document](docs/quantization.md) for more details.\n\n### Pruning\n```python\nfrom nlp_toolkit import PrunerConfig, PruningConfig\nfrom nlp_toolkit.optimization.trainer import NLPTrainer\n\n# Replace transformers.Trainer with NLPTrainer\n# trainer = transformers.Trainer(...)\ntrainer = NLPTrainer(...)\nmetric = metrics.Metric(name=\"eval_accuracy\")\npruner_config = PrunerConfig(prune_type='BasicMagnitude', target_sparsity_ratio=0.9)\np_conf = PruningConfig(pruner_config=[pruner_config], metrics=metric)\nmodel = trainer.prune(pruning_config=p_conf)\n```\n\nPlease refer to [pruning document](docs/pruning.md) for more details.\n\n### Distillation\n```python\nfrom nlp_toolkit import DistillationConfig, Criterion\nfrom nlp_toolkit.optimization.trainer import NLPTrainer\n\n# Replace transformers.Trainer with NLPTrainer\n# trainer = transformers.Trainer(...)\nteacher_model = ... # exist model\ntrainer = NLPTrainer(...)\nmetric = metrics.Metric(name=\"eval_accuracy\")\nd_conf = DistillationConfig(metrics=metric)\nmodel = trainer.distill(distillation_config=d_conf, teacher_model=teacher_model)\n```\n\nPlease refer to [distillation document](docs/distillation.md) for more details.\n\n### Data Augmentation\nData augmentation provides the facilities to generate synthesized NLP dataset for further model optimization. The data augmentation supports text generation on popular fine-tuned models like GPT, GPT2, and other text synthesis approaches from [nlpaug](https://github.com/makcedward/nlpaug).\n\n```python\nfrom nlp_toolkit.preprocessing.data_augmentation import DataAugmentation\naug = DataAugmentation(augmenter_type=\"TextGenerationAug\")\naug.input_dataset = \"original_dataset.csv\" # example: https://huggingface.co/datasets/glue/viewer/sst2/train\naug.column_names = \"sentence\"\naug.output_path = os.path.join(self.result_path, \"test2.cvs\")\naug.augmenter_arguments = {'model_name_or_path': 'gpt2-medium'}\naug.data_augment()\nraw_datasets = load_dataset(\"csv\", data_files=aug.output_path, delimiter=\"\\t\", split=\"train\")\n```\n\nPlease refer to [data augmentation document](docs/data_augmentation.md) for more details.\n\n### Neural Engine\nNeural Engine is one of reference deployments that NLP toolkit provides. Neural Engine aims to demonstrate the optimal performance of extremely compressed NLP models by exploring the optimization opportunities from both HW and SW.\n\n```python\nfrom nlp_toolkit.backends.neural_engine.compile import compile\n# /path/to/your/model is a TensorFlow pb model or ONNX model\nmodel = compile('/path/to/your/model')\ninputs = ... # [input_ids, segment_ids, input_mask]\nmodel.inference(inputs)\n```\n\nPlease refer to [Neural Engine](examples/deployment/) for more details.\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/intel/intel-extension-for-transformers",
            "keywords": "quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "intel-extension-for-transformers",
            "package_url": "https://pypi.org/project/intel-extension-for-transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/intel-extension-for-transformers/",
            "project_urls": {
                "Homepage": "https://github.com/intel/intel-extension-for-transformers"
            },
            "release_url": "https://pypi.org/project/intel-extension-for-transformers/0.4.0/",
            "requires_dist": [
                "numpy",
                "transformers (>=4.12.0)",
                "packaging"
            ],
            "requires_python": ">=3.6.0",
            "summary": "Repository of Intel\u00ae Extension for Transformers",
            "version": "0.4.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15783762,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4c2e2fc89360aeac527bc7171462c4b3",
                    "sha256": "2771d1133b5cdf2aa210e12540bbdf30a894debdda2f15afa7b48c75d2359f59"
                },
                "downloads": -1,
                "filename": "intel_extension_for_transformers-0.4.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "4c2e2fc89360aeac527bc7171462c4b3",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6.0",
                "size": 260449,
                "upload_time": "2022-11-16T03:11:29",
                "upload_time_iso_8601": "2022-11-16T03:11:29.548515Z",
                "url": "https://files.pythonhosted.org/packages/63/06/4462514f013ff983259dd4f18035c1e491e8a503459bf18917bc145e5cc3/intel_extension_for_transformers-0.4.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "4223f57d91501ab3bbd81c089c9aadee",
                    "sha256": "e1cfa083e85d45575f83cc921938d1c3c92e2ca4a05772d776ad5bff40d2c076"
                },
                "downloads": -1,
                "filename": "intel-extension-for-transformers-0.4.0.tar.gz",
                "has_sig": false,
                "md5_digest": "4223f57d91501ab3bbd81c089c9aadee",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6.0",
                "size": 149951,
                "upload_time": "2022-11-16T03:11:31",
                "upload_time_iso_8601": "2022-11-16T03:11:31.849598Z",
                "url": "https://files.pythonhosted.org/packages/26/73/39ad81634af6ce29edecb17027b5165d8067d82686477696277e7c83ddf8/intel-extension-for-transformers-0.4.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}