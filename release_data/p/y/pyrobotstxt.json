{
    "0.0.1": {
        "info": {
            "author": "Faisal Shahzad",
            "author_email": "seowingsorg@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.seowings.org/",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pyrobotstxt",
            "package_url": "https://pypi.org/project/pyrobotstxt/",
            "platform": null,
            "project_url": "https://pypi.org/project/pyrobotstxt/",
            "project_urls": {
                "Bug Tracker": "https://github.com/seowings/pyrobotstxt/issues",
                "Documentation": "https://pyrobotstxt.seowings.org/",
                "Homepage": "https://www.seowings.org/"
            },
            "release_url": "https://pypi.org/project/pyrobotstxt/0.0.1/",
            "requires_dist": [
                "pillow (==9.3.0)"
            ],
            "requires_python": ">=3.9",
            "summary": "Python Package to Generate and Analyse Robots.txt files",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15982656,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f7c735fb9c856e2dcff62e39a4d03364",
                    "sha256": "b8fb34f3c4477d0278335df444783cbde004c351bc6a6c1084501b2e0666c73a"
                },
                "downloads": -1,
                "filename": "pyrobotstxt-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "f7c735fb9c856e2dcff62e39a4d03364",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9",
                "size": 3943,
                "upload_time": "2022-12-03T18:06:46",
                "upload_time_iso_8601": "2022-12-03T18:06:46.319029Z",
                "url": "https://files.pythonhosted.org/packages/bf/6b/085967b0528c2af30e3ecafb4bd20b7611411ce60b0e05ad070b597b22c7/pyrobotstxt-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "43ef35be2772ec7e015f309536145833",
                    "sha256": "6bf98b3fa96e95cd9b67d7afa7701987e21fe8693b4982a0b9d7ae4c20cc4f82"
                },
                "downloads": -1,
                "filename": "pyrobotstxt-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "43ef35be2772ec7e015f309536145833",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9",
                "size": 3703,
                "upload_time": "2022-12-03T18:06:48",
                "upload_time_iso_8601": "2022-12-03T18:06:48.057586Z",
                "url": "https://files.pythonhosted.org/packages/6c/cb/f7e38dd462e7951b854d3616c92ff7ec645ba962c3b8aff2478cda02047b/pyrobotstxt-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.2": {
        "info": {
            "author": "Faisal Shahzad",
            "author_email": "seowingsorg@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Topic :: Utilities"
            ],
            "description": "![pyrobotstx feature image](docs/img/feature-image.png)\n\n# pyrobotstxt: Python Package for **robots.txt** Files\n\n``pyrobotstxt`` package can be used to (systematically) generate **robots.txt** files. Moreover, this package comes in handy for creating and including ``ASCII`` images in **robots.txt** files.\n\nIn future releases, it would be possible to parse and analyze **robots.txt** file generated using any software (not limited to ``pyrobotstxt``)\n\n## Whats in PyRobotsTxt?\n\nWe believe in monolithic software development and created this tiny package that does its job without any bloat. It is useful for \n\n- Createing **robots.txt** File\n- Parsing a **robots.txt** File [in progress]\n- Analyzing **robots.txt** File [in progress]\n\n## Installation\n\nYou can install this package using ``pip`` with the following command. It should also work with other methods, e.g. ``pipenv`` or ``poetry``. If you encounter any installation problems, please create an ``issue``.\n\n```bash\npip install pyrobotstxt\n```\n\n## Usage\n\nThere are several use cases for ``pyrobotstxt``. \n\n### Basic\nYou can use it to check the details of a search bot. Just specify a keyword, e.g. ``Google``, and it will provide a list of Google bots in the ``pyrobotstxt`` database.\n\n```python\nfrom pyrobotstxt import RobotsTxt\nprint(RobotsTxt().robots_name(\"Google\"))\n```\nYou can also do a reverse search by providing a robot name. Again, you will get details about that bot.\n\n```python\nfrom pyrobotstxt import RobotsTxt\nprint(RobotsTxt().robots_details(\"Googlebot\"))\n```\n\n### Advance\n\nYou can create a robot file by creating an object of the ``RobotsTxt`` class. \n\n```python\nfrom pyrobotstxt import RobotsTxt\nrobots_file = RobotsTxt()\n```\n\nYou can add a header and footer section. These are comments for humans looking into this **robots.txt** file. In the header section, you can also include file creation ``date``. It is handy for archiving purposes.\n\n```python\nrobots_file.include_header(\"Welcome Crawlers\", append_date=True)\nrobots_file.include_footer(\"Good Bye Crawlers\")\n```\n\nIn **robots.txt** file, rules are specified as per user agent. ``pyrobotstxt`` offer a ``UserAgent`` class. You can use it to create multiple user agent. Default user agent is ``*``. \n\n```python\nua_general = UserAgent(name=\"*\")\n```\nAfter creating a user agent, you can add all those routes/pages/images you want to Allow or Disallow.\n\n```python\nua_general.add_allow(\n    allow_items=[\"/home\", \"/deep\", \"/home\"],\n    unique=True,\n    comments=\"This is a list of allowed items\",\n)\n\nua_general.add_disallow(\n    disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"],\n    unique=True,\n    comments=\"This is a list of allowed items\",\n)\n```\n\nHere is a complete example of a user agent. You can also include a sitemap of your website.\n\n```python\nua_general_google = UserAgent(name=\"Google\")\nua_general_google.add_allow(\n    allow_items=[\"/home\", \"/deep\", \"/home\"],\n    unique=True,\n    comments=\"This is a list of allowed items\",\n)\nua_general_google.add_disallow(\n    disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"],\n    unique=True,\n    comments=\"This is a list of allowed items\",\n)\nua_general_google.add_sitemap(\"https://seowings.org/sitemap.xml\")\n```\n\nAfter you have prepared user agents, you can add them to the ``RobotsTxt`` object. This object keeps a list of all the user agents.\n\n```python\nrobots_file.add_user_agent(ua_general)\nrobots_file.add_user_agent(ua_general_google)\n```\n\nYou can also include any image (``ASCII`` format) in your **robots.txt** file. For example, add the following command in your script/program to include an ``ASCII`` image in your **robots.txt** file.\n\n```python\nrobots_file.include_image(\"logo_dark.png\", 90)\n```\n\nIn the end, you can save this file to the desired location. The default name of the file is **robots.txt**.\n\n```python\nrobots_file.write(\"robots.txt\")\n```\n\n## Contribute\n\nPull Requests, Feature Suggestions, and collaborations are welcome.\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/seowings/pyrobotstxt/",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pyrobotstxt",
            "package_url": "https://pypi.org/project/pyrobotstxt/",
            "platform": null,
            "project_url": "https://pypi.org/project/pyrobotstxt/",
            "project_urls": {
                "Bug Tracker": "https://github.com/seowings/pyrobotstxt/issues",
                "Documentation": "https://pyrobotstxt.seowings.org/",
                "Homepage": "https://github.com/seowings/pyrobotstxt/"
            },
            "release_url": "https://pypi.org/project/pyrobotstxt/0.0.2/",
            "requires_dist": [
                "pillow (==9.3.0)"
            ],
            "requires_python": ">=3.9",
            "summary": "Python Package to Generate and Analyse Robots.txt files",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15982656,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "0a821b730859aec5d186754bdddf8a68",
                    "sha256": "f95e90333e856dfa3b22573ea2b502ff3611fd8b4006a060089f447cd23651af"
                },
                "downloads": -1,
                "filename": "pyrobotstxt-0.0.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "0a821b730859aec5d186754bdddf8a68",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9",
                "size": 7467,
                "upload_time": "2022-12-04T02:21:54",
                "upload_time_iso_8601": "2022-12-04T02:21:54.695954Z",
                "url": "https://files.pythonhosted.org/packages/24/cf/16e37bc242815c3548b80397482dec85b35af109f937769cf38318a0ec7d/pyrobotstxt-0.0.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "56120159b69fe809ba35828578f268b7",
                    "sha256": "bb0a86b06420127bba2a7433e45c4ec8f74cb8ed81c74f4cf8c67a45021f30ee"
                },
                "downloads": -1,
                "filename": "pyrobotstxt-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "56120159b69fe809ba35828578f268b7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9",
                "size": 6634,
                "upload_time": "2022-12-04T02:21:56",
                "upload_time_iso_8601": "2022-12-04T02:21:56.478093Z",
                "url": "https://files.pythonhosted.org/packages/50/22/6e71fee16b2572259d99faa9e5a220e8f09f537268f0c582487a31be7e0f/pyrobotstxt-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}