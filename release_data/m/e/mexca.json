{
    "1.0.0": {
        "info": {
            "author": "Malte Luken",
            "author_email": "m.luken@esciencecenter.nl",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 2 - Pre-Alpha",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description": "\n# Multimodal Emotion Expression Capture Amsterdam\n\n\n\n[![github license badge](https://img.shields.io/github/license/mexca/mexca)](https://github.com/mexca/mexca)\n[![RSD](https://img.shields.io/badge/rsd-mexca-00a3e3.svg)](https://www.research-software.nl/software/mexca)\n[![read the docs badge](https://readthedocs.org/projects/pip/badge/)](https://mexca.readthedocs.io/en/latest/index.html)\n[![fair-software badge](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu)\n[![workflow scq badge](https://sonarcloud.io/api/project_badges/measure?project=mexca_mexca&metric=alert_status)](https://sonarcloud.io/dashboard?id=mexca_mexca)\n[![workflow scc badge](https://sonarcloud.io/api/project_badges/measure?project=mexca_mexca&metric=coverage)](https://sonarcloud.io/dashboard?id=mexca_mexca)\n[![build](https://github.com/mexca/mexca/actions/workflows/build.yml/badge.svg)](https://github.com/mexca/mexca/actions/workflows/build.yml)\n[![cffconvert](https://github.com/mexca/mexca/actions/workflows/cffconvert.yml/badge.svg)](https://github.com/mexca/mexca/actions/workflows/cffconvert.yml)\n[![markdown-link-check](https://github.com/mexca/mexca/actions/workflows/markdown-link-check.yml/badge.svg)](https://github.com/mexca/mexca/actions/workflows/markdown-link-check.yml)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6962473.svg)](https://doi.org/10.5281/zenodo.6962473)\n\n<div align=\"center\">\n<img src=\"mexca_logo.png\">\n</div>\n\nMexca is an open-source Python package which aims to capture emotion expression cues in human faces and speech by combining visual and auditory modalities (video/audio). \n\n## How To Use Mexca\n\nMexca provides a customizable yet easy-to-use pipeline for extracting emotion expression features from videos. It contains building blocks that can be used to extract features for individual modalities (i.e., facial expressions, voice, and dialogue/spoken text). The blocks can also be integrated into a single pipeline to extract the features from all modalities at once. Next to extracting features, mexca can also identify the speakers shown in the video by clustering speaker and face representations. This allows users to compare emotion expressions across speakers, time, and contexts.  \n\nPlease cite mexca if you use it for scientific or commercial purposes.\n\n- L\u00fcken, Malte, & Viviani, Eva. (2022). mexca (1.0.0). Zenodo. https://doi.org/10.5281/zenodo.6962473\n\n\n## Installation\n\nMexca supports Python >=3.7 and Python <= 3.9.\nWe recommend to install mexca in a new virtual environment, e.g., using `venv`:\n\n```console\npython3 -m venv env\nenv/bin/activate\n```\n\nAlternatively, if you use conda:\n\n```console\nconda create -n env\nconda activate env\n```\n\nOnce you have activated your virtual environment you can then install mexca from PyPi:\n\n```console\npython3 -m pip install mexca\n```\n\nTo install mexca from the GitHub repository, do:\n\n```console\ngit clone https://github.com/mexca/mexca.git\ncd mexca\npython3 -m pip install .\n```\n\n## Getting Started\n\nIf you would like to learn how to use mexca, the best place to start is our [demo](https://github.com/mexca/mexca/tree/main/examples) tutorial. \n\nEmotion expression features can be extracted with mexca using the following lines of code:\n\n```python\nfrom mexca.core.pipeline import Pipeline\n\n# Path to video file (consider using os.path.join())\nfilename = 'path/to/video'\n\n# Create pipeline object from default constructor method\npipeline = Pipeline().from_default(language='english')\n\n# Apply pipeline to video file (may take a long time depending on video length)\noutput = pipeline.apply(filename)\n```\n\nMexca's pipeline returns a `Multimodal` object that contains the extracted emotion expression features in the `feature` attribute. We can convert the features into a `pandas.DataFrame` for further inspection and processing.\n\n```python\ndf = pd.DataFrame(output.features)\ndf\n```\n\nThis is what the output looks like:\n\n|      |   frame |   time | face_box                                          |   face_prob | face_landmarks                | face_aus                                                               |   face_id |   pitchF0 |   segment_id |   segment_start |   segment_end | track   | speaker_id   |   text_token_id | text_token               |   text_token_start |   text_token_end |   match_id |\n|-----:|--------:|-------:|:--------------------------------------------------|------------:|:------------------------------|:-----------------------------------------------------------------------|----------:|----------:|-------------:|----------------:|--------------:|:--------|:-------------|----------------:|:-------------------------|-------------------:|-----------------:|-----------:|\n|   0 |      0 |   0.52 | [254.80342   52.627777 339.73337  162.48317]     |    0.999263 | [253.81114993 106.13823438]   | [1.7722143e-01 9.6993530e-01 3.4657875e-03 5.7775569e-01 ...] |         7 |  114.05050 |            1 |        0.497812 |       21.0178 | 0       | SPEAKER_00   |               0 |       is                  |               0.52    |             0.60    |          1 |\n|   1 |      1 |   0.56 | [255.26508  52.85576 339.82748 162.45255]         |    0.999143 | [254.09605609 106.21201348]   | [1.7896292e-01 9.6784592e-01 3.4994783e-03 5.6765985e-01 ...] |         7 |  117.58867 |            1 |        0.497812 |       21.0178 | 0       | SPEAKER_00   |               0 |       is                  |               0.52    |             0.60    |          1 |\n\n## Structure and Performance\n\nCurrently, mexca includes three independent submodules (video, audio and text). The `core` module is responsible for running a single pipeline which calls in turn all the other submodules.\n\nThe video submodule supports the extraction of facial features (e.g., facial landmarks, action units). It relies on [pyfeat](https://py-feat.org/pages/intro.html)[^1] and [facenet-pytorch](https://github.com/timesler/facenet-pytorch)[^2]. It includes the following components:\n\n- Face detection with Multi-task Convolutional Neural Network (MTCNN; 0.95 on FDDB; 0.85 on WIDER FACE easy, 0.82 on medium, 0.61 on hard data subset; all AUC).\n- Face identification with Inception ResNet v1 (supervised: Acc = 0.9965 on LFW dataset when trained on VGGFace2).\n- Landmark detection (6.41$for Feat-PFLD; 6.00 for Feat-MobileFaceNet; 5.23 for Feat-MobileNet; all RMSE on 300W dataset).\n- Action unit detection (0.22 for Feat-JaaNET; 0.52 for Feat-Logistic; 0.57 for Feat-SVM; all average F1 on DisfaPlus dataset; Feat-RF is currently not available).\n\nThe audio module relies on [praat-parselmouth](https://github.com/YannickJadoul/Parselmouth)[^3] for voice pitch analysis, and on [pyannote.audio](https://github.com/pyannote/pyannote-audio) for speaker diarization[^4]. It includes the following components:\n\n- Voice pitch as fundamental frequency (F0).\n- Speaker diarization with ECAPA-TDNN model (supervised: 2.65 with known # of speakers; 3.01 with estimated # of speakers; unsupervised: 18.2 with estimated # of speakers; all DER on AMI Mix-Headset only words test dataset).\n\nThe text module supports text transcriptions for Dutch and English audio files. It relies on a pre-trained model made available by [HuggingSound](https://github.com/jonatasgrosman/huggingsound) that is the wav2vec-large model[^5] fine-tuned on Dutch (WER = 15.7; CER = 5.4 on Common Voice nl test set) and English (WER = 19.1; CER = 7.7 on Common Voice en test set).\n\n\n## Documentation\n\nThe documentation of mexca can be found on [Read the Docs](https://mexca.readthedocs.io/en/latest/index.html).\n\n## Contributing\n\nIf you want to contribute to the development of mexca,\nhave a look at the [contribution guidelines](CONTRIBUTING.md).\n\n## License\n\nThe code is licensed under the Apache 2.0 License. This means that mexca can be used, modified and redistributed for free, even for commercial purposes.\n\n## Credits\n\nMexca is being developed by the [Netherlands eScience Center](https://www.esciencecenter.nl/) in collaboration with the [Hot Politics Lab](http://www.hotpolitics.eu/) at the University of Amsterdam.\n\nThis package was created with [Cookiecutter](https://github.com/audreyr/cookiecutter) and the [NLeSC/python-template](https://github.com/NLeSC/python-template).\n\n## References\n[^1]: Cheong, J. H., Xie, T., Byrne, S., & Chang, L. J. (2021). Py-feat: Python facial expression analysis toolbox. *arXiv*. https://doi.org/10.48550/arXiv.2104.03509\n\n[^2]: Schroff, F., Kalenichenko, D., & Philbin, J. (2015). FaceNet: A unified embedding for face recognition and clustering. *arXiv*. https://doi.org/10.48550/arXiv.1503.03832\n\n[^3]: Jadoul, Y., Thompson, B., & de Boer, B. (2018). Introducing Parselmouth: A Python interface to Praat. Journal of Phonetics, 71, 1-15. https://doi.org/10.1016/j.wocn.2018.07.001\n\n[^4]: Bredin, H., & Laurent, A. (2021). End-to-end speaker segmentation for overlap-aware resegmentation. *arXiv*. https://doi.org/10.48550/arXiv.2104.04045\n\n[^5]: Schneider, S., Baevski, A., Collobert, R., & Auli, M. (2019). wav2vec: Unsupervised pre-training for speech recognition. *arXiv*. https://doi.org/10.48550/arXiv.1904.05862\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mexca/mexca",
            "keywords": "emotion,multimodal,expression",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "mexca",
            "package_url": "https://pypi.org/project/mexca/",
            "platform": null,
            "project_url": "https://pypi.org/project/mexca/",
            "project_urls": {
                "Bug Tracker": "https://github.com/mexca/mexca/issues",
                "Homepage": "https://github.com/mexca/mexca"
            },
            "release_url": "https://pypi.org/project/mexca/1.0.0/",
            "requires_dist": [
                "dataclasses",
                "facenet-pytorch",
                "moviepy",
                "numpy",
                "opencv-python-headless",
                "pandas",
                "praat-parselmouth",
                "py-feat",
                "pyannote.audio",
                "scipy",
                "scikit-learn",
                "spectralcluster",
                "speechbrain",
                "torchvision",
                "huggingsound",
                "bump2version ; extra == 'dev'",
                "coverage[toml] ; extra == 'dev'",
                "prospector[with_pyroma] ; extra == 'dev'",
                "isort ; extra == 'dev'",
                "pytest ; extra == 'dev'",
                "pytest-cov ; extra == 'dev'",
                "sphinx ; extra == 'dev'",
                "sphinx-rtd-theme ; extra == 'dev'",
                "sphinx-autoapi ; extra == 'dev'",
                "tox ; extra == 'dev'",
                "myst-parser ; extra == 'dev'",
                "twine ; extra == 'publishing'",
                "wheel ; extra == 'publishing'"
            ],
            "requires_python": "",
            "summary": "Emotion expression capture from multiple modalities.",
            "version": "1.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16019134,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "18be9ef88cb09f92e2722fa69e37153a",
                    "sha256": "59b7e1cc4378f370109d8b0ec0b03b9e73e1310cb83a6bbb05ab399b67abd047"
                },
                "downloads": -1,
                "filename": "mexca-1.0.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "18be9ef88cb09f92e2722fa69e37153a",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 24060,
                "upload_time": "2022-08-09T09:45:27",
                "upload_time_iso_8601": "2022-08-09T09:45:27.008568Z",
                "url": "https://files.pythonhosted.org/packages/7e/ae/979908ae0a706deb4cd23fc51e97e89fae913c1a7877084e20a4c52fe2e9/mexca-1.0.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "814107aafb434535a54fc1b8269e6cb2",
                    "sha256": "67c796e37114a0540f72c1a52ee4f66e69d878709af6441c6c4e9e988dc784a3"
                },
                "downloads": -1,
                "filename": "mexca-1.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "814107aafb434535a54fc1b8269e6cb2",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 23749,
                "upload_time": "2022-08-09T09:45:28",
                "upload_time_iso_8601": "2022-08-09T09:45:28.928919Z",
                "url": "https://files.pythonhosted.org/packages/d6/5d/c7ad4044bccb0f5ca44260bb573b3317b2ba28d2f259d3f3d07609989cbb/mexca-1.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}