{
    "2.0.0": {
        "info": {
            "author": "",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "",
            "version": "2.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "da6284e6406086b338fea30c41290f20",
                    "sha256": "8128557aebb16029e53801b9ed57800ee2cbcb102e160559d9a5016ab673eda0"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.0-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "da6284e6406086b338fea30c41290f20",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 131697,
                "upload_time": "2021-06-03T15:45:26",
                "upload_time_iso_8601": "2021-06-03T15:45:26.504667Z",
                "url": "https://files.pythonhosted.org/packages/e6/9f/a27bf3a14f49633b8873fba28da4cc7b2d5e601794f16c2a2724f6177d0a/polyglot_tokenizer-2.0.0-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5a5a4a60ae02601a9d3c001e14b39409",
                    "sha256": "90b8fae690ea30268541619e43307d2f33343cbcba94a3fce23755294c5477bc"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "5a5a4a60ae02601a9d3c001e14b39409",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 100807,
                "upload_time": "2018-12-16T17:08:31",
                "upload_time_iso_8601": "2018-12-16T17:08:31.786098Z",
                "url": "https://files.pythonhosted.org/packages/26/2e/3be7828e741f8b0434432b021c8ca561ed19e969f300a2e8416d6a06c393/polyglot-tokenizer-2.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description_content_type": "",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=5.1,<6.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "5b0ab3722c221d532cd41406add6e028",
                    "sha256": "0e95e57e50079db8c952288d8d5d5c5a8bb2655d43b8c25f5e8f2bbae7566636"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "5b0ab3722c221d532cd41406add6e028",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 319691,
                "upload_time": "2019-01-18T09:57:07",
                "upload_time_iso_8601": "2019-01-18T09:57:07.805642Z",
                "url": "https://files.pythonhosted.org/packages/71/65/8189f4ba5238f1072655408af4179205c0ed5a224d10da3af487786f1dbd/polyglot_tokenizer-2.0.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "ab4f17076f0c2e3ab35aafacd98b3a4b",
                    "sha256": "48e55f45cb2d22657b8fdff934492350d67ded44acfc38b0d619e39d0b3404be"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "ab4f17076f0c2e3ab35aafacd98b3a4b",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 96643,
                "upload_time": "2019-01-18T09:57:05",
                "upload_time_iso_8601": "2019-01-18T09:57:05.230212Z",
                "url": "https://files.pythonhosted.org/packages/2a/bd/04d80180c8271df82241507d9c8001af03379e725fb8e78a725fd3ca0d70/polyglot-tokenizer-2.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.1": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description_content_type": "text/x-rst",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.1/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=5.1,<6.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "56ee44a235561c836b7eb399d604ba2c",
                    "sha256": "754773355dbabd6f0d42cf5b62144585d3bffc83921304e0d3076873ad9b7574"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "56ee44a235561c836b7eb399d604ba2c",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 321574,
                "upload_time": "2019-01-18T10:14:48",
                "upload_time_iso_8601": "2019-01-18T10:14:48.705688Z",
                "url": "https://files.pythonhosted.org/packages/07/b2/f6aadcb2f08b4f9f92d51a5869a6f52ec552b44e7a29edde85255cfd63ca/polyglot_tokenizer-2.0.1.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "f30fea389792e48a6ede2e70dd2d659c",
                    "sha256": "b24ccff698adae30fffbc7eaa45007f13a6972be6bb5ab7584390dc65a29ad3f"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.1.tar.gz",
                "has_sig": false,
                "md5_digest": "f30fea389792e48a6ede2e70dd2d659c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 100837,
                "upload_time": "2019-01-18T10:14:45",
                "upload_time_iso_8601": "2019-01-18T10:14:45.870405Z",
                "url": "https://files.pythonhosted.org/packages/7b/7b/d25a8c3da0a313f14822698b6d83d19e316d8a11a2251bd07224dad1377d/polyglot-tokenizer-2.0.1.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.2": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description_content_type": "text/x-rst",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.2/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=5.1,<6.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "6a79b6ca38fab53dd26190a0e554082e",
                    "sha256": "f262a2747ac4f2fc736ddc3e684f97a40651b1119ca7bd1881d95599b39ff1ff"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.2-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "6a79b6ca38fab53dd26190a0e554082e",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 321577,
                "upload_time": "2019-01-18T10:41:30",
                "upload_time_iso_8601": "2019-01-18T10:41:30.530110Z",
                "url": "https://files.pythonhosted.org/packages/39/c6/a6d50d8ef3f6e0bb9e793b8eabc493f15347bd67459fcdf64dcbb53322d7/polyglot_tokenizer-2.0.1.2-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "e80581235a90382e74a11b15bb8b4cf9",
                    "sha256": "bcf65d0f26d05af231f9f2ab92d9891c2e42e3a98b0e12c2917977a04122c7a9"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.2.tar.gz",
                "has_sig": false,
                "md5_digest": "e80581235a90382e74a11b15bb8b4cf9",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 101291,
                "upload_time": "2019-01-18T10:41:27",
                "upload_time_iso_8601": "2019-01-18T10:41:27.716681Z",
                "url": "https://files.pythonhosted.org/packages/56/3f/4bbbf9d6b8be7b30f484bd75e4d4f6fa721caf052edbc8b8d45d441128cb/polyglot-tokenizer-2.0.1.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.3": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description_content_type": "text/x-rst",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/irshadbhat/polyglot-tokenizer",
            "keywords": "nlp,polyglot,tokenizer",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/irshadbhat/polyglot-tokenizer",
                "Repository": "https://github.com/irshadbhat/polyglot-tokenizer"
            },
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.3/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=2.0,<3.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f1ba3cba7c092cc0eac684658c5882c0",
                    "sha256": "9b611fc8fc3305b174916c65e034079741f804d434b4ba1149f6ce4820c665db"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.3-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "f1ba3cba7c092cc0eac684658c5882c0",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 321887,
                "upload_time": "2019-01-18T15:38:29",
                "upload_time_iso_8601": "2019-01-18T15:38:29.143316Z",
                "url": "https://files.pythonhosted.org/packages/cb/44/0786a527c83e772216e3b05970e89d8ec0c300b18fa24d834b276833d2c1/polyglot_tokenizer-2.0.1.3-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "bb622e51f73b32dc9459662b2309a366",
                    "sha256": "4bcf3f2bc3fe7852b221c31b163d743c4d04e1e12ab04f2dffbc97f30f537c40"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.3.tar.gz",
                "has_sig": false,
                "md5_digest": "bb622e51f73b32dc9459662b2309a366",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 101075,
                "upload_time": "2019-01-18T15:38:25",
                "upload_time_iso_8601": "2019-01-18T15:38:25.492964Z",
                "url": "https://files.pythonhosted.org/packages/58/02/123c1bc468777501162021a02a17d90362a7def63057b5f73909e1df987a/polyglot-tokenizer-2.0.1.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.4": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description_content_type": "text/x-rst",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/irshadbhat/polyglot-tokenizer",
            "keywords": "nlp,polyglot,tokenizer",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/irshadbhat/polyglot-tokenizer",
                "Repository": "https://github.com/irshadbhat/polyglot-tokenizer"
            },
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.4/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=2.0,<3.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.4",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "aa9222de23bf17ff0590b7f335e52671",
                    "sha256": "9f6f3de28fb52d0161acadb6c072229eabcbb3c0a3c46c278c93b1da3dccfa08"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.4-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "aa9222de23bf17ff0590b7f335e52671",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 321877,
                "upload_time": "2019-01-18T16:32:45",
                "upload_time_iso_8601": "2019-01-18T16:32:45.023674Z",
                "url": "https://files.pythonhosted.org/packages/aa/30/37ce725b650240b390bcf4c509c9edb80e754810e361153e93447dd2e703/polyglot_tokenizer-2.0.1.4-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "2297566db6440a51702299616395cc4c",
                    "sha256": "22db75fc256286d74dc4f9260bdfac5f81ab906b6db5f6128d6be05790f0eae3"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.4.tar.gz",
                "has_sig": false,
                "md5_digest": "2297566db6440a51702299616395cc4c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 101052,
                "upload_time": "2019-01-18T16:32:41",
                "upload_time_iso_8601": "2019-01-18T16:32:41.403933Z",
                "url": "https://files.pythonhosted.org/packages/a6/21/bc881d90d6f499c886412938a36391044585db10fea0f539af3d3d39588a/polyglot-tokenizer-2.0.1.4.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.6": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7"
            ],
            "description_content_type": "",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.6/",
            "requires_dist": null,
            "requires_python": ">=2.7,<3.0",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.6",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "3d1e3c9a0a8a792b9653b978f6b6a4d3",
                    "sha256": "1f7d58b29488507228bcf00269d7402b2b16c1a5a297a8d683be45baefb7c577"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.6-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "3d1e3c9a0a8a792b9653b978f6b6a4d3",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=2.7,<3.0",
                "size": 107015,
                "upload_time": "2021-06-04T06:51:48",
                "upload_time_iso_8601": "2021-06-04T06:51:48.349704Z",
                "url": "https://files.pythonhosted.org/packages/8a/1f/16623d6c6f72d87a0f4643f6277eaeff6902086ec51982e65d31a1096436/polyglot_tokenizer-2.0.1.6-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "3635348b5ad48cba8fa256666d66c4b8",
                    "sha256": "9cf84c3d81d597a8ad2837db510fa2d8250b7e89a0e0ebdc7cf64a491c333adb"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.6.tar.gz",
                "has_sig": false,
                "md5_digest": "3635348b5ad48cba8fa256666d66c4b8",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=2.7,<3.0",
                "size": 97302,
                "upload_time": "2021-06-04T06:51:44",
                "upload_time_iso_8601": "2021-06-04T06:51:44.762788Z",
                "url": "https://files.pythonhosted.org/packages/af/0d/ea9d4f80a84f443dcb313efbe953d013f90dd72747027ca679d66384c9a6/polyglot-tokenizer-2.0.1.6.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.1.7": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description_content_type": "",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.1.7/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.1.7",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "0ef32c94f2da31b30c9de60774cc86fb",
                    "sha256": "2905b89bc263a4717e2e33569211e7cf99f4e456a7c3aa707b141eba7d79d453"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.1.7-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "0ef32c94f2da31b30c9de60774cc86fb",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 107017,
                "upload_time": "2021-06-09T05:01:58",
                "upload_time_iso_8601": "2021-06-09T05:01:58.330071Z",
                "url": "https://files.pythonhosted.org/packages/e9/fa/e6eb12a916e0519722101a2f88a3449c53ed0137d481c3ca2186076e0e20/polyglot_tokenizer-2.0.1.7-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "f072dfdff96d1edc685f5815e1fb0936",
                    "sha256": "4a40e9405661fcc13da1ce95bec41cd51d536e06acb2ccf78a0d8ab4d4e2f372"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.1.7.tar.gz",
                "has_sig": false,
                "md5_digest": "f072dfdff96d1edc685f5815e1fb0936",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 97294,
                "upload_time": "2021-06-09T05:01:53",
                "upload_time_iso_8601": "2021-06-09T05:01:53.520465Z",
                "url": "https://files.pythonhosted.org/packages/6e/23/7a6961dabebbc8612c29ebb32e40ac0c9c2d90bd6b4850170bb235464507/polyglot-tokenizer-2.0.1.7.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "2.0.2": {
        "info": {
            "author": "irshadbhat",
            "author_email": "bhatirshad127@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 2",
                "Programming Language :: Python :: 2.7",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.4",
                "Programming Language :: Python :: 3.5",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7"
            ],
            "description": "Polyglot Tokenizer\n==================\n\n\nTokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.\n\n\nInstallation\n------------\n\n::\n\n    pip install polyglot-tokenizer\n\nExamples\n--------\n\nWithin Python\n^^^^^^^^^^^^^\n\n\n.. code:: python\n\n    >>> from __future__ import unicode_literals\n    >>> from polyglot_tokenizer import Tokenizer\n    >>> tk = Tokenizer(lang='en', smt=True) #smt is a flag for social-media-text\n    >>> text = \"RT @BJP_RSS Crack down on Black money.India slides to 75th slot on Swiss bank money list #ModiForeignAchievements @RituRathaur https://t.c\u2026\"\n    >>> tk.tokenize(text)\n    ['RT', '@BJP_RSS', 'Crack', 'down', 'on', 'Black', 'money', '.', 'India', 'slides', 'to', '75th', 'slot', 'on', 'Swiss', 'bank', 'money', 'list', '#ModiForeignAchievements', '@RituRathaur', 'https://t.c\u2026']\n    >>> tk = Tokenizer(lang='hi')\n    >>> tk.tokenize(\"22 \u0938\u093e\u0932 \u0915\u0947 \u0932\u0902\u092c\u0947 \u0907\u0902\u0924\u091c\u093e\u0930 \u0915\u0947 \u092c\u093e\u0926 \u0906\u0916\u093f\u0930\u0915\u093e\u0930 \u0939\u0949\u0932\u0940\u0935\u0941\u0921 \u0938\u094d\u091f\u093e\u0930 \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u094b \u0905\u092a\u0928\u0940 \u092a\u0939\u0932\u0940 \u0911\u0938\u094d\u0915\u0930 \u091f\u094d\u0930\u0949\u092b\u0940\"\n    ...             \" \u092e\u093f\u0932 \u091a\u0941\u0915\u0940 \u0939\u0948\u0964 \u0909\u0928\u094d\u0939\u0947\u0902 \u092f\u0947 \u0905\u0935\u0949\u0930\u094d\u0921 \u0905\u092a\u0928\u0940 \u092b\u093f\u0932\u094d\u092e \u2018\u0926 \u0930\u0947\u0935\u0947\u0928\u0947\u0902\u091f\u2019 \u092e\u0947\u0902 \u0939\u094d\u092f\u0942\u091c \u0917\u094d\u0932\u093e\u0938 \u0915\u0947 \u0915\u093f\u0930\u0926\u093e\u0930 \u0915\u0947 \u0932\u093f\u090f \u092e\u093f\u0932\u093e, \u0932\u0947\u0915\u093f\u0928 \u0909\u0928\u0915\u0947\"\n    ...             \" \u0915\u0947 \u0932\u093f\u090f \u0930\u094b\u0932 \u0928\u093f\u092d\u093e\u0928\u093e \u0906\u0938\u093e\u0928 \u0928\u0939\u0940\u0902 \u0925\u093e\u0964\")\n    ['22', '\u0938\u093e\u0932', '\u0915\u0947', '\u0932\u0902\u092c\u0947', '\u0907\u0902\u0924\u091c\u093e\u0930', '\u0915\u0947', '\u092c\u093e\u0926', '\u0906\u0916\u093f\u0930\u0915\u093e\u0930', '\u0939\u0949\u0932\u0940\u0935\u0941\u0921', '\u0938\u094d\u091f\u093e\u0930', '\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b', '\u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b', '\u0915\u094b', '\u0905\u092a\u0928\u0940', '\u092a\u0939\u0932\u0940', '\u0911\u0938\u094d\u0915\u0930', '\u091f\u094d\u0930\u0949\u092b\u0940', '\u092e\u093f\u0932', '\u091a\u0941\u0915\u0940', '\u0939\u0948', '\u0964', '\u0909\u0928\u094d\u0939\u0947\u0902', '\u092f\u0947', '\u0905\u0935\u0949\u0930\u094d\u0921', '\u0905\u092a\u0928\u0940', '\u092b\u093f\u0932\u094d\u092e', \"'\", '\u0926', '\u0930\u0947\u0935\u0947\u0928\u0947\u0902\u091f', \"'\", '\u092e\u0947\u0902', '\u0939\u094d\u092f\u0942\u091c', '\u0917\u094d\u0932\u093e\u0938', '\u0915\u0947', '\u0915\u093f\u0930\u0926\u093e\u0930', '\u0915\u0947', '\u0932\u093f\u090f', '\u092e\u093f\u0932\u093e', ',', '\u0932\u0947\u0915\u093f\u0928', '\u0909\u0928\u0915\u0947', '\u0915\u0947', '\u0932\u093f\u090f', '\u0930\u094b\u0932', '\u0928\u093f\u092d\u093e\u0928\u093e', '\u0906\u0938\u093e\u0928', '\u0928\u0939\u0940\u0902', '\u0925\u093e', '\u0964']\n    >>> tk = Tokenizer(lang='hi', split_sen=True)\n    >>> tk.tokenize(\"22 \u0938\u093e\u0932 \u0915\u0947 \u0932\u0902\u092c\u0947 \u0907\u0902\u0924\u091c\u093e\u0930 \u0915\u0947 \u092c\u093e\u0926 \u0906\u0916\u093f\u0930\u0915\u093e\u0930 \u0939\u0949\u0932\u0940\u0935\u0941\u0921 \u0938\u094d\u091f\u093e\u0930 \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u094b \u0905\u092a\u0928\u0940 \u092a\u0939\u0932\u0940 \u0911\u0938\u094d\u0915\u0930 \u091f\u094d\u0930\u0949\u092b\u0940\"\n    ...             \" \u092e\u093f\u0932 \u091a\u0941\u0915\u0940 \u0939\u0948\u0964 \u0909\u0928\u094d\u0939\u0947\u0902 \u092f\u0947 \u0905\u0935\u0949\u0930\u094d\u0921 \u0905\u092a\u0928\u0940 \u092b\u093f\u0932\u094d\u092e \u2018\u0926 \u0930\u0947\u0935\u0947\u0928\u0947\u0902\u091f\u2019 \u092e\u0947\u0902 \u0939\u094d\u092f\u0942\u091c \u0917\u094d\u0932\u093e\u0938 \u0915\u0947 \u0915\u093f\u0930\u0926\u093e\u0930 \u0915\u0947 \u0932\u093f\u090f \u092e\u093f\u0932\u093e, \u0932\u0947\u0915\u093f\u0928 \u0909\u0928\u0915\u0947\"\n    ...             \" \u0915\u0947 \u0932\u093f\u090f \u0930\u094b\u0932 \u0928\u093f\u092d\u093e\u0928\u093e \u0906\u0938\u093e\u0928 \u0928\u0939\u0940\u0902 \u0925\u093e\u0964 \u092b\u093f\u0932\u094d\u092e \u090f\u0915 \u0938\u0940\u0928 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0915\u094b \u092d\u0948\u0902\u0938 \u0915\u093e \u0915\u091a\u094d\u091a\u093e \u0932\u0940\u0935\u0930 \u0916\u093e\u0928\u093e\"\n    ...             \" \u092a\u0921\u093c\u093e \u0925\u093e\u0964 \u091c\u092c\u0915\u093f \u0905\u0938\u0932 \u091c\u093f\u0902\u0926\u0917\u0940 \u092e\u0947\u0902 \u0935\u094b \u092a\u0942\u0930\u0940 \u0924\u0930\u0939 \u0936\u093e\u0915\u093e\u0939\u093e\u0930\u0940 \u0939\u0948\u0902\u0964 \u0939\u093e\u0932\u093e\u0902\u0915\u093f \u0907\u0938 \u0938\u0940\u0928 \u0915\u0947 \u0932\u093f\u090f \u092a\u0939\u0932\u0947 \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0915\u094b\"\n    ...             \" \u092e\u093e\u0902\u0938 \u091c\u0948\u0938\u0947 \u0926\u093f\u0916\u0928\u0947 \u0935\u093e\u0932\u0940 \u091a\u0940\u091c \u0926\u0940 \u0917\u0908 \u0925\u0940, \u0932\u0947\u0915\u093f\u0928 \u0909\u0928\u094d\u0939\u0947\u0902 \u0932\u0917\u093e \u0915\u093f \u0910\u0938\u093e \u0915\u0930\u0928\u093e \u0917\u0932\u0924 \u0939\u094b\u0917\u093e\u0964 \u092b\u093f\u0932\u094d\u092e \u0915\u0947 \u0932\u093f\u090f \u0907\u092e\u094d\u092a\u094b\u0930\u094d\u091f\"\n    ...             \" \u0915\u0940 \u0917\u0908 \u091a\u0940\u091f\u093f\u092f\u093e\u0902...\")\n    [['22', '\u0938\u093e\u0932', '\u0915\u0947', '\u0932\u0902\u092c\u0947', '\u0907\u0902\u0924\u091c\u093e\u0930', '\u0915\u0947', '\u092c\u093e\u0926', '\u0906\u0916\u093f\u0930\u0915\u093e\u0930', '\u0939\u0949\u0932\u0940\u0935\u0941\u0921', '\u0938\u094d\u091f\u093e\u0930', '\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b', '\u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b', '\u0915\u094b', '\u0905\u092a\u0928\u0940', '\u092a\u0939\u0932\u0940', '\u0911\u0938\u094d\u0915\u0930', '\u091f\u094d\u0930\u0949\u092b\u0940', '\u092e\u093f\u0932', '\u091a\u0941\u0915\u0940', '\u0939\u0948', '\u0964'], ['\u0909\u0928\u094d\u0939\u0947\u0902', '\u092f\u0947', '\u0905\u0935\u0949\u0930\u094d\u0921', '\u0905\u092a\u0928\u0940', '\u092b\u093f\u0932\u094d\u092e', \"'\", '\u0926', '\u0930\u0947\u0935\u0947\u0928\u0947\u0902\u091f', \"'\", '\u092e\u0947\u0902', '\u0939\u094d\u092f\u0942\u091c', '\u0917\u094d\u0932\u093e\u0938', '\u0915\u0947', '\u0915\u093f\u0930\u0926\u093e\u0930', '\u0915\u0947', '\u0932\u093f\u090f', '\u092e\u093f\u0932\u093e', ',', '\u0932\u0947\u0915\u093f\u0928', '\u0909\u0928\u0915\u0947', '\u0915\u0947', '\u0932\u093f\u090f', '\u0930\u094b\u0932', '\u0928\u093f\u092d\u093e\u0928\u093e', '\u0906\u0938\u093e\u0928', '\u0928\u0939\u0940\u0902', '\u0925\u093e', '\u0964'], ['\u092b\u093f\u0932\u094d\u092e', '\u090f\u0915', '\u0938\u0940\u0928', '\u0915\u0947', '\u0932\u093f\u090f', '\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b', '\u0915\u094b', '\u092d\u0948\u0902\u0938', '\u0915\u093e', '\u0915\u091a\u094d\u091a\u093e', '\u0932\u0940\u0935\u0930', '\u0916\u093e\u0928\u093e', '\u092a\u0921\u093c\u093e', '\u0925\u093e', '\u0964'], ['\u091c\u092c\u0915\u093f', '\u0905\u0938\u0932', '\u091c\u093f\u0902\u0926\u0917\u0940', '\u092e\u0947\u0902', '\u0935\u094b', '\u092a\u0942\u0930\u0940', '\u0924\u0930\u0939', '\u0936\u093e\u0915\u093e\u0939\u093e\u0930\u0940', '\u0939\u0948\u0902', '\u0964'], ['\u0939\u093e\u0932\u093e\u0902\u0915\u093f', '\u0907\u0938', '\u0938\u0940\u0928', '\u0915\u0947', '\u0932\u093f\u090f', '\u092a\u0939\u0932\u0947', '\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b', '\u0915\u094b', '\u092e\u093e\u0902\u0938', '\u091c\u0948\u0938\u0947', '\u0926\u093f\u0916\u0928\u0947', '\u0935\u093e\u0932\u0940', '\u091a\u0940\u091c', '\u0926\u0940', '\u0917\u0908', '\u0925\u0940', ',', '\u0932\u0947\u0915\u093f\u0928', '\u0909\u0928\u094d\u0939\u0947\u0902', '\u0932\u0917\u093e', '\u0915\u093f', '\u0910\u0938\u093e', '\u0915\u0930\u0928\u093e', '\u0917\u0932\u0924', '\u0939\u094b\u0917\u093e', '\u0964'], ['\u092b\u093f\u0932\u094d\u092e', '\u0915\u0947', '\u0932\u093f\u090f', '\u0907\u092e\u094d\u092a\u094b\u0930\u094d\u091f', '\u0915\u0940', '\u0917\u0908', '\u091a\u0940\u091f\u093f\u092f\u093e\u0902', '...']]\n\n\nFrom Console\n^^^^^^^^^^^^\n\n.. parsed-literal::\n\n    polyglot-tokenizer --h\n\n    usage: polyglot-tokenizer [-h] [-v] [-i] [-s] [-t] [-o] [-l]\n    \n    Tokenizer for world's most spoken languages\n\n    \n    optional arguments:\n      -h, --help            show this help message and exit\n      -v, --version         show program's version number and exit\n      -i , --input          <input-file>\n      -s, --split-sentences\n                            set this flag to apply sentence segmentation\n      -t, --social-media-test\n                            set this flag if the input file contains social media\n                            text like twitter, facebook and whatsapp\n      -o , --output         <output-file>\n      -l , --language       select language (2 letter ISO-639 code) {hi, ur, bn,\n                            as, gu, ml, pa, te, ta, kn, or, mr, cu, myv, nn, yi,\n                            ne, bo, br, ks, en, es, ca, cs, de, el, en, fi, da,\n                            eu, kok, nb, uz, fr, ga, hu, is, it, lt, lv, nl, pl,\n                            pt, ro, ru, sk, bm, yue, mk, ku, sl, sv, zh, et, fo,\n                            gl, hsb, af, ar, be, hy, bg, ka, ug, hr, mn, tk, kk,\n                            ky, la, no, fa, uk, tl, tr, vi, yo, ko, got, ckb, he,\n                            id, sr}\n\n    Example ::\n\n    polyglot-tokenizer < raw_file.txt -l en -s > tokenized.txt\n\n\n\n\n",
            "description_content_type": "text/x-rst",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/irshadbhat/polyglot-tokenizer",
            "keywords": "nlp,polyglot,tokenizer",
            "license": "MIT",
            "maintainer": "irshadbhat",
            "maintainer_email": "bhatirshad127@gmail.com",
            "name": "polyglot-tokenizer",
            "package_url": "https://pypi.org/project/polyglot-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/polyglot-tokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/irshadbhat/polyglot-tokenizer",
                "Repository": "https://github.com/irshadbhat/polyglot-tokenizer"
            },
            "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.2/",
            "requires_dist": [
                "six (>=1.12,<2.0)",
                "pbr (>=2.0,<3.0)"
            ],
            "requires_python": "",
            "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
            "version": "2.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10596923,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "944fefbadb4c7afdd67e45ad8684451b",
                    "sha256": "0ddb1101fa212c35795ca916adc02436e4ea7723182449fab5185f8f94caf873"
                },
                "downloads": -1,
                "filename": "polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "944fefbadb4c7afdd67e45ad8684451b",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 323283,
                "upload_time": "2021-06-09T05:31:30",
                "upload_time_iso_8601": "2021-06-09T05:31:30.196447Z",
                "url": "https://files.pythonhosted.org/packages/e4/d2/1338a204f5035015d54caa765478a0ceb8d806dceed150f45263f9d3ff98/polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5e1f8a2b33dfb120fef68752adab103c",
                    "sha256": "8094a9a4b271cb0ac0ae81ace892d804b6806030bb7c2327810251e33ddafd95"
                },
                "downloads": -1,
                "filename": "polyglot-tokenizer-2.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "5e1f8a2b33dfb120fef68752adab103c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 101320,
                "upload_time": "2021-06-09T05:31:25",
                "upload_time_iso_8601": "2021-06-09T05:31:25.069288Z",
                "url": "https://files.pythonhosted.org/packages/a2/9e/94e64554280a3636d4e9ad1185d7aed3d1919981885b9f3756578dee19d9/polyglot-tokenizer-2.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}