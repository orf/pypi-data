{
    "0.3.0": {
        "info": {
            "author": "Arm Ltd",
            "author_email": "mlia@arm.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: POSIX :: Linux",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://git.mlplatform.org/ml/mlia.git",
            "keywords": "ml,arm,ethos-u,tflite",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "mlia",
            "package_url": "https://pypi.org/project/mlia/",
            "platform": null,
            "project_url": "https://pypi.org/project/mlia/",
            "project_urls": {
                "Homepage": "https://git.mlplatform.org/ml/mlia.git"
            },
            "release_url": "https://pypi.org/project/mlia/0.3.0/",
            "requires_dist": [
                "tensorflow (~=2.7.1)",
                "tensorflow-model-optimization (~=0.7.2)",
                "ethos-u-vela (~=3.3.0)",
                "requests",
                "rich",
                "click",
                "sh",
                "paramiko",
                "filelock",
                "psutil",
                "cloup (>=0.12.0)",
                "pytest (==7.1.1) ; extra == 'dev'",
                "pytest-cov (==3.0.0) ; extra == 'dev'",
                "mypy (==0.942) ; extra == 'dev'",
                "pylint (==2.13.7) ; extra == 'dev'"
            ],
            "requires_python": ">=3.8",
            "summary": "ML Inference Advisor",
            "version": "0.3.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15914221,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "051843f8b0b9685d73121dce26da3c6e",
                    "sha256": "875bd4bda886c32f463d1992452905cd0263653a3c8bc972733e406eacf45d25"
                },
                "downloads": -1,
                "filename": "mlia-0.3.0-py3-none-manylinux1_x86_64.whl",
                "has_sig": false,
                "md5_digest": "051843f8b0b9685d73121dce26da3c6e",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8",
                "size": 1269474,
                "upload_time": "2022-06-02T07:52:00",
                "upload_time_iso_8601": "2022-06-02T07:52:00.953715Z",
                "url": "https://files.pythonhosted.org/packages/56/e2/6ea8a95bd986a849a15ede6d04fc9fe3bf2b569b06e13dfa34f21e5c9c79/mlia-0.3.0-py3-none-manylinux1_x86_64.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.4.0": {
        "info": {
            "author": "Arm Ltd",
            "author_email": "mlia@arm.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: POSIX :: Linux",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://git.mlplatform.org/ml/mlia.git",
            "keywords": "ml,arm,ethos-u,tflite",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "mlia",
            "package_url": "https://pypi.org/project/mlia/",
            "platform": null,
            "project_url": "https://pypi.org/project/mlia/",
            "project_urls": {
                "Homepage": "https://git.mlplatform.org/ml/mlia.git"
            },
            "release_url": "https://pypi.org/project/mlia/0.4.0/",
            "requires_dist": [
                "tensorflow (~=2.8.2)",
                "tensorflow-model-optimization (~=0.7.2)",
                "ethos-u-vela (~=3.4.0)",
                "requests",
                "rich",
                "sh",
                "pytest (==7.1.1) ; extra == 'dev'",
                "pytest-cov (==3.0.0) ; extra == 'dev'",
                "mypy (==0.942) ; extra == 'dev'",
                "pylint (==2.13.7) ; extra == 'dev'",
                "pre-commit ; extra == 'dev'",
                "tosa-checker (==0.1.0) ; extra == 'tosa'"
            ],
            "requires_python": ">=3.8",
            "summary": "ML Inference Advisor",
            "version": "0.4.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15914221,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "b797f2dae81f978446da645bfc77b3cf",
                    "sha256": "27face0a218b4e424496c0932d081365388a13652296c9017e86fe09b0947452"
                },
                "downloads": -1,
                "filename": "mlia-0.4.0-py3-none-manylinux2014_x86_64.whl",
                "has_sig": false,
                "md5_digest": "b797f2dae81f978446da645bfc77b3cf",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8",
                "size": 1248900,
                "upload_time": "2022-09-01T16:46:49",
                "upload_time_iso_8601": "2022-09-01T16:46:49.151035Z",
                "url": "https://files.pythonhosted.org/packages/77/5d/84f06faa8836c1d406c87ac83ef45bcc649f3da426aabbf84fb8bd47a787/mlia-0.4.0-py3-none-manylinux2014_x86_64.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.5.0": {
        "info": {
            "author": "Arm Ltd",
            "author_email": "mlia@arm.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: POSIX :: Linux",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "<!---\nSPDX-FileCopyrightText: Copyright 2022, Arm Limited and/or its affiliates.\nSPDX-License-Identifier: Apache-2.0\n--->\n# ML Inference Advisor - Introduction\n\nThe ML Inference Advisor (MLIA) is used to help AI developers design and\noptimize neural network models for efficient inference on Arm\u00ae targets (see\n[supported targets](#target-profiles)) by enabling performance analysis and\nproviding actionable advice early in the model development cycle. The final\nadvice can cover supported operators, performance analysis and suggestions for\nmodel optimization (e.g. pruning, clustering, etc.).\n\n## Inclusive language commitment\n\nThis product conforms to Arm's inclusive language policy and, to the best of\nour knowledge, does not contain any non-inclusive language.\n\nIf you find something that concerns you, email terms@arm.com.\n\n## Releases\n\nRelease notes can be found in [MLIA releases](https://review.mlplatform.org/plugins/gitiles/ml/mlia/+/refs/tags/0.5.0/RELEASES.md).\n\n## Getting support\n\nIn case you need support or want to report an issue, give us feedback or\nsimply ask a question about MLIA, please send an email to mlia@arm.com.\n\nAlternatively, use the\n[AI and ML forum](https://community.arm.com/support-forums/f/ai-and-ml-forum)\nto get support by marking your post with the **MLIA** tag.\n\n## Reporting vulnerabilities\n\nInformation on reporting security issues can be found in\n[Reporting vulnerabilities](https://review.mlplatform.org/plugins/gitiles/ml/mlia/+/refs/tags/0.5.0/SECURITY.md).\n\n## License\n\nML Inference Advisor is licensed under [Apache License 2.0](https://review.mlplatform.org/plugins/gitiles/ml/mlia/+/refs/tags/0.5.0/LICENSES/Apache-2.0.txt).\n\n## Trademarks and copyrights\n\n* Arm\u00ae, Arm\u00ae Ethos\u2122-U, Arm\u00ae Cortex\u00ae-A, Arm\u00ae Cortex\u00ae-M, Arm\u00ae Corstone\u2122 are\n  registered trademarks or trademarks of Arm\u00ae Limited (or its subsidiaries) in\n  the U.S. and/or elsewhere.\n* TensorFlow\u2122 is a trademark of Google\u00ae LLC.\n* Keras\u2122 is a trademark by Fran\u00e7ois Chollet.\n* Linux\u00ae is the registered trademark of Linus Torvalds in the U.S. and\n  elsewhere.\n* Python\u00ae is a registered trademark of the PSF.\n* Ubuntu\u00ae is a registered trademark of Canonical.\n* Microsoft and Windows are trademarks of the Microsoft group of companies.\n\n# General usage\n\n## Prerequisites and dependencies\n\nIt is recommended to use a virtual environment for MLIA installation, and a\ntypical setup for MLIA requires:\n\n* Ubuntu\u00ae 20.04.03 LTS (other OSs may work, the ML Inference Advisor has been\n  tested on this one specifically)\n* Python\u00ae >= 3.8\n* Ethos\u2122-U Vela dependencies (Linux\u00ae only)\n  * For more details, please refer to the\n    [prerequisites of Vela](https://pypi.org/project/ethos-u-vela/)\n\n## Installation\n\nMLIA can be installed with `pip` using the following command:\n\n```bash\npip install mlia\n```\n\nIt is highly recommended to create a new virtual environment to install MLIA.\n\n## First steps\n\nAfter the installation, you can check that MLIA is installed correctly by\nopening your terminal, activating the virtual environment and typing the\nfollowing command that should print the help text:\n\n```bash\nmlia --help\n```\n\nThe ML Inference Advisor works with sub-commands, i.e. in general a MLIA command\nwould look like this:\n\n```bash\nmlia [sub-command] [arguments]\n```\n\nWhere the following sub-commands are available:\n\n* [\"operators\"](#operators-ops): show the model's operator list\n* [\"optimization\"](#model-optimization-opt): run the specified optimizations\n* [\"performance\"](#performance-perf): measure the performance of inference on hardware\n* [\"all_tests\"](#all-tests-all): have a full report\n\nDetailed help about the different sub-commands can be shown like this:\n\n```bash\nmlia [sub-command] --help\n```\n\nThe following sections go into further detail regarding the usage of MLIA.\n\n# Sub-commands\n\nThis section gives an overview of the available sub-commands for MLIA.\n\n## **operators** (ops)\n\nLists the model's operators with information about their compatibility with the\nspecified target.\n\n*Examples:*\n\n```bash\n# List operator compatibility with Ethos-U55 with 256 MAC\nmlia operators --target-profile ethos-u55-256 ~/models/mobilenet_v1_1.0_224_quant.tflite\n\n# List operator compatibility with Cortex-A\nmlia ops --target-profile cortex-a ~/models/mobilenet_v1_1.0_224_quant.tflite\n\n# Get help and further information\nmlia ops --help\n```\n\n## **performance** (perf)\n\nEstimate the model's performance on the specified target and print out\nstatistics.\n\n*Examples:*\n\n```bash\n# Use default parameters\nmlia performance ~/models/mobilenet_v1_1.0_224_quant.tflite\n\n# Explicitly specify the target profile and backend(s) to use with --evaluate-on\nmlia perf ~/models/ds_cnn_large_fully_quantized_int8.tflite \\\n    --evaluate-on \"Vela\" \"Corstone-310\" \\\n    --target-profile ethos-u65-512\n\n# Get help and further information\nmlia perf --help\n```\n\n## **optimization** (opt)\n\nThis sub-command applies optimizations to a Keras model (.h5 or SavedModel) and\nshows the performance improvements compared to the original unoptimized model.\n\nThere are currently two optimization techniques available to apply:\n\n* **pruning**: Sets insignificant model weights to zero until the specified\n    sparsity is reached.\n* **clustering**: Groups the weights into the specified number of clusters and\n    then replaces the weight values with the cluster centroids.\n\nMore information about these techniques can be found online in the TensorFlow\ndocumentation, e.g. in the\n[TensorFlow model optimization guides](https://www.tensorflow.org/model_optimization/guide).\n\n**Note:** A ***Keras model*** (.h5 or SavedModel) is required as input to\nperform the optimizations. Models in the TensorFlow Lite format are **not**\nsupported.\n\n*Examples:*\n\n```bash\n# Custom optimization parameters: pruning=0.6, clustering=16\nmlia optimization \\\n    --optimization-type pruning,clustering \\\n    --optimization-target 0.6,16 \\\n    ~/models/ds_cnn_l.h5\n\n# Get help and further information\nmlia opt --help\n```\n\n## **all_tests** (all)\n\nCombine sub-commands described above to generate a full report of the input\nmodel with all information available for the specified target. E.g. for Ethos-U\nthis combines sub-commands *operators* and *optimization*. Therefore most\ncommand line arguments are shared with other sub-commands.\n\n*Examples:*\n\n```bash\n# Create full report and save it as JSON file\nmlia all_tests --output ./report.json ~/models/ds_cnn_l.h5\n\n# Get help and further information\nmlia all --help\n```\n\n# Target profiles\n\nMost sub-commands accept the name of a target profile as input parameter. The\nprofiles currently available are described in the following sections.\n\nThe support of the above sub-commands for different targets is provided via\nbackends that need to be installed separately, see\n[Backend installation](#backend-installation) section.\n\n## Ethos-U\n\nThere are a number of predefined profiles for Ethos-U with the following\nattributes:\n\n```\n+--------------------------------------------------------------------+\n| Profile name  | MAC | System config               | Memory mode    |\n+=====================================================================\n| ethos-u55-256 | 256 | Ethos_U55_High_End_Embedded | Shared_Sram    |\n+---------------------------------------------------------------------\n| ethos-u55-128 | 128 | Ethos_U55_High_End_Embedded | Shared_Sram    |\n+---------------------------------------------------------------------\n| ethos-u65-512 | 512 | Ethos_U65_High_End          | Dedicated_Sram |\n+---------------------------------------------------------------------\n| ethos-u65-256 | 256 | Ethos_U65_High_End          | Dedicated_Sram |\n+--------------------------------------------------------------------+\n```\n\nExample:\n\n```bash\nmlia perf --target-profile ethos-u65-512 ~/model.tflite\n```\n\nEthos-U is supported by these backends:\n\n* [Corstone-300](#corstone-300)\n* [Corstone-310](#corstone-310)\n* [Vela](#vela)\n\n## Cortex-A\n\nThe profile *cortex-a* can be used to get the information about supported\noperators for Cortex-A CPUs when using the Arm NN TensorFlow Lite delegate.\nPlease, find more details in the section for the\n[corresponding backend](#arm-nn-tensorflow-lite-delegate).\n\n## TOSA\n\nThe target profile *tosa* can be used for TOSA compatibility checks of your\nmodel. It requires the [TOSA Checker](#tosa-checker) backend.\n\nFor more information, see TOSA Checker's:\n\n* [repository](https://review.mlplatform.org/plugins/gitiles/tosa/tosa_checker/+/refs/heads/main)\n* [pypi.org page](https://pypi.org/project/tosa-checker/)\n\n# Backend installation\n\nThe ML Inference Advisor is designed to use backends to provide different\nmetrics for different target hardware. Some backends come pre-installed with\nMLIA, but others can be added and managed using the command `mlia-backend`, that\nprovides the following functionality:\n\n* **install**\n* **uninstall**\n* **list**\n\n *Examples:*\n\n```bash\n# List backends installed and available for installation\nmlia-backend list\n\n# Install Corstone-300 backend for Ethos-U\nmlia-backend install Corstone-300 --path ~/FVP_Corstone_SSE-300/\n\n# Uninstall the Corstone-300 backend\nmlia-backend uninstall Corstone-300\n\n# Get help and further information\nmlia-backend --help\n```\n\n**Note:** Some, but not all, backends can be automatically downloaded, if no\npath is provided.\n\n## Available backends\n\nThis section lists available backends. As not all backends work on any platform\nthe following table shows some compatibility information:\n\n```\n+----------------------------------------------------------------------------+\n| Backend       | Linux                  | Windows        | Python           |\n+=============================================================================\n| Arm NN        |                        |                |                  |\n| TensorFlow    | x86_64                 | Windows 10     | Python>=3.8      |\n| Lite delegate |                        |                |                  |\n+-----------------------------------------------------------------------------\n| Corstone-300  | x86_64                 | Not compatible | Python>=3.8      |\n+-----------------------------------------------------------------------------\n| Corstone-310  | x86_64                 | Not compatible | Python>=3.8      |\n+-----------------------------------------------------------------------------\n| TOSA checker  | x86_64 (manylinux2014) | Not compatible | 3.7<=Python<=3.9 |\n+-----------------------------------------------------------------------------\n| Vela          | x86_64                 | Windows 10     | Python~=3.7      |\n+----------------------------------------------------------------------------+\n```\n\n### Arm NN TensorFlow Lite delegate\n\nThis backend provides general information about the compatibility of operators\nwith the Arm NN TensorFlow Lite delegate for Cortex-A. It comes pre-installed\nwith MLIA.\n\nFor more information see:\n\n* [Arm NN TensorFlow Lite delegate documentation](https://arm-software.github.io/armnn/latest/delegate.xhtml)\n\n### Corstone-300\n\nCorstone-300 is a backend that provides performance metrics for systems based\non Cortex-M55 and Ethos-U. It is only available on the Linux platform.\n\n*Examples:*\n\n```bash\n# Download and install Corstone-300 automatically\nmlia-backend install Corstone-300\n# Point to a local version of Corstone-300 installed using its installation script\nmlia-backend install Corstone-300 --path YOUR_LOCAL_PATH_TO_CORSTONE_300\n```\n\nFor further information about Corstone-300 please refer to:\n<https://developer.arm.com/Processors/Corstone-300>\n\n### Corstone-310\n\nCorstone-310 is a backend that provides performance metrics for systems based\non Cortex-M85 and Ethos-U. It is available as Arm Virtual Hardware (AVH) only,\ni.e. it can not be downloaded automatically.\n\n* For access to AVH for Corstone-310 please refer to:\n  <https://developer.arm.com/Processors/Corstone-310>\n* Please use the examples of MLIA using Corstone-310 here to get started:\n  <https://github.com/ARM-software/open-iot-sdk>\n\n### TOSA Checker\n\nThe TOSA Checker backend provides operator compatibility checks against the\nTOSA specification.\n\nPlease, install it into the same environment as MLIA using this command:\n\n```bash\nmlia-backend install tosa-checker\n```\n\nAdditional resources:\n\n* Source code: <https://review.mlplatform.org/admin/repos/tosa/tosa_checker>\n* PyPi package <https://pypi.org/project/tosa-checker/>\n\n### Vela\n\nThe Vela backend provides performance metrics for Ethos-U based systems. It\ncomes pre-installed with MLIA.\n\nAdditional resources:\n\n* <https://pypi.org/project/ethos-u-vela/>\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://git.mlplatform.org/ml/mlia.git",
            "keywords": "ml,arm,ethos-u,tflite",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "mlia",
            "package_url": "https://pypi.org/project/mlia/",
            "platform": null,
            "project_url": "https://pypi.org/project/mlia/",
            "project_urls": {
                "Homepage": "https://git.mlplatform.org/ml/mlia.git"
            },
            "release_url": "https://pypi.org/project/mlia/0.5.0/",
            "requires_dist": [
                "tensorflow-model-optimization (~=0.7.3)",
                "requests",
                "rich",
                "sh",
                "tensorflow-aarch64 (~=2.7.3) ; platform_machine == \"aarch64\"",
                "ethos-u-vela (~=3.3.0) ; platform_machine == \"aarch64\"",
                "tensorflow (~=2.9.2) ; platform_machine == \"x86_64\"",
                "ethos-u-vela (~=3.5.0) ; platform_machine == \"x86_64\"",
                "pytest (==7.2.0) ; extra == 'dev'",
                "pytest-cov (==4.0.0) ; extra == 'dev'",
                "mypy (==0.982) ; extra == 'dev'",
                "pylint (==2.15.5) ; extra == 'dev'",
                "pre-commit ; extra == 'dev'",
                "tosa-checker (==0.1.0) ; extra == 'tosa'"
            ],
            "requires_python": ">=3.8",
            "summary": "ML Inference Advisor",
            "version": "0.5.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15914221,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "621cb0a295c4209773bbb8e17db8c437",
                    "sha256": "7df35e85cc20d1a81122414ad319b9c62711334e8553fadb5cf79c1a61c976e5"
                },
                "downloads": -1,
                "filename": "mlia-0.5.0-py3-none-manylinux2014_x86_64.whl",
                "has_sig": false,
                "md5_digest": "621cb0a295c4209773bbb8e17db8c437",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8",
                "size": 1502843,
                "upload_time": "2022-11-28T13:43:39",
                "upload_time_iso_8601": "2022-11-28T13:43:39.929669Z",
                "url": "https://files.pythonhosted.org/packages/a8/45/a93d0fad76e81b84e5549ec583c09601c6e3917da147e3d972247236b4ab/mlia-0.5.0-py3-none-manylinux2014_x86_64.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}