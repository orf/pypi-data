{
    "0.0.22": {
        "info": {
            "author": "Robel Equbasilassie",
            "author_email": "robiki4life@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description": "# Ethiopian Natural Language Toolkit (etnltk)\n\n- The Ethiopian Natural Language Toolkit (etnltk) project aimed to develop a suite of open source Natural Language Processing modules for the Ethiopian languages.\n- The Ethiopian Natural Language Toolkit (etnltk) is built using python language and takes inspiration from `spacy` and `nltk` libraries.\n\n## Installation\n\n### pip\n\n- **etnltk** supports Python 3.6 or later. We recommend that you install etnltk via `pip`, the Python package manager. To install, simply run:\n\n  ```python\n    pip install etnltk\n  ```\n\n### From Source\n\n- Alternatively, you can also install from source via `etnltk` git repository, which will give you more flexibility in developing on top of etltk. For this option, run\n\n  ```python\n    git clone https://github.com/robikieq/etnltk.git\n    \n    cd etnltk\n    \n    pip install -e .\n  ```\n\n## Documentation\n\n<https://etnltk.netlify.app/>\n\n## Usage\n\n1. Amharic text preprocessing with Amharic document\n    - Preprocessing amharic text is very simple: you can simply pass the text to the `Amharic` document and access all annotations from the returned Amharic document object:\n\n    ```python\n      from etnltk import Amharic\n\n      sample_text = \"\"\"\n        \u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12d3.\u121d \ud83e\udd17 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 /Artificial Intelligence/ \u12a0\u1201\u1295 \u12ab\u1208\u1260\u1275 \u12dd\u1245\u1270\u129b \u12f0\u1228\u1303 \u12c8\u12f0 \u120b\u1240 \u12f0\u1228\u1303 \u1208\u121b\u12f5\u1228\u1235\u1363 \u1203\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u12d3\u1208\u121d \u1270\u12f0\u122b\u123d \u1208\u121b\u12f5\u1228\u130d\u1363 \u12a0\u1308\u122b\u12ca \u12a0\u1245\u121d\u1295 \u1208\u121b\u1233\u12f0\u130d \u12a5\u1293 \u1270\u1320\u1243\u121a \u1208\u1218\u1206\u1295 \u1260\u130b\u122b \u12a0\u1265\u122e \u1218\u1235\u122b\u1271 \u12a5\u1305\u130d \u1320\u1243\u121a \u1290\u12cd\u1361\u1361\n\n        \u1260\u121b\u123d\u1295 \u12d3\u1235\u1270\u121d\u122e (Machine Learning) \u12a0\u121b\u12ab\u129d\u1290\u1275 \u12e8\u133d\u1201\u134d \u1293\u1219\u1293\u12ce\u127d \u1260\u12a0\u122d\u1272\u134a\u123b\u120d \u12a2\u1295\u1270\u1208\u1300\u1295\u1235 \u1225\u122d\u12d3\u1275 \u1208\u121b\u1230\u120d\u1320\u1295\u1363 \u12e8\u133d\u1201\u134d \u12f3\u1273\u1295 \u1218\u1230\u1265\u1230\u1265 \u12a5\u1293 \u121b\u12f0\u122b\u1300\u1275\u1364 \u12e8\u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d\u1295 /Natural Language Processing Tools/ \u1260\u1218\u1320\u1240\u121d \u12e8\u133d\u1201\u134d \u12f3\u1273\u1295 \u1355\u122e\u1230\u1235 \u121b\u12f5\u1228\u130d \u1270\u1240\u12f3\u121a \u12a5\u1293 \u1218\u1230\u1228\u1273\u12ca \u1309\u12f3\u12ed \u1290\u12cd\u1362\n      \"\"\"\n  \n      # Annotating Amharic document\n      doc = Amharic(sample_text)\n\n      # print the `clean` text:\n      print(doc)\n      \n      # output: Amharic(\"\u121a\u12eb\u12dd\u12eb \u12d3\u1218\u1270 \u121d\u1205\u1228\u1275 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12a0\u1201\u1295 \u12ab\u1208\u1260\u1275 \u12dd\u1245\u1270\u129b \u12f0\u1228\u1303 \u12c8\u12f0 \u120b\u1240 \u12f0\u1228\u1303 \u1208\u121b\u12f5\u1228\u1235 \u1200\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u12a0\u1208\u121d \u1270\u12f0\u122b\u123d \u1208\u121b\u12f5\u1228\u130d \u12a0\u1308\u122b\u12ca \u12a0\u1245\u121d\u1295 \u1208\u121b\u1233\u12f0\u130d \u12a5\u1293 \u1270\u1320\u1243\u121a \u1208\u1218\u1206\u1295 \u1260\u130b\u122b \u12a0\u1265\u122e \u1218\u1235\u122b\u1271 \u12a5\u1305\u130d \u1320\u1243\u121a \u1290\u12cd \u1260\u121b\u123d\u1295 \u12a0\u1235\u1270\u121d\u122e \u12a0\u121b\u12ab\u129d\u1290\u1275 \u12e8\u1345\u1201\u134d \u1293\u1219\u1293\u12ce\u127d \u1260\u12a0\u122d\u1272\u134a\u123b\u120d \u12a2\u1295\u1270\u1208\u1300\u1295\u1235 \u1235\u122d\u12a0\u1275 \u1208\u121b\u1230\u120d\u1320\u1295 \u12e8\u1345\u1201\u134d \u12f3\u1273\u1295 \u1218\u1230\u1265\u1230\u1265 \u12a5\u1293 \u121b\u12f0\u122b\u1300\u1275 \u12e8\u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d\u1295 \u1260\u1218\u1320\u1240\u121d \u12e8\u1345\u1201\u134d \u12f3\u1273\u1295 \u1355\u122e\u1230\u1235 \u121b\u12f5\u1228\u130d \u1270\u1240\u12f3\u121a \u12a5\u1293 \u1218\u1230\u1228\u1273\u12ca \u1309\u12f3\u12ed \u1290\u12cd\")\n    ```\n\n     - Here is a another example of performing text cleaning on a piece of plaintext using `clean_amharic` function:\n\n    ```python\n    from etnltk.lang.am import (\n      preprocessing,\n      clean_amharic\n    )\n\n    sample_text = \"\"\"\n      \u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12d3.\u121d \ud83e\udd17 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 /Artificial Intelligence/ \u12a0\u1201\u1295 \u12ab\u1208\u1260\u1275 \u12dd\u1245\u1270\u129b \u12f0\u1228\u1303 \u12c8\u12f0 \u120b\u1240 \u12f0\u1228\u1303 \u1208\u121b\u12f5\u1228\u1235\u1363 \u1203\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u12d3\u1208\u121d \u1270\u12f0\u122b\u123d \u1208\u121b\u12f5\u1228\u130d\u1363 \u12a0\u1308\u122b\u12ca \u12a0\u1245\u121d\u1295 \u1208\u121b\u1233\u12f0\u130d \u12a5\u1293 \u1270\u1320\u1243\u121a \u1208\u1218\u1206\u1295 \u1260\u130b\u122b \u12a0\u1265\u122e \u1218\u1235\u122b\u1271 \u12a5\u1305\u130d \u1320\u1243\u121a \u1290\u12cd\u1361\u1361\n\n      \u1260\u121b\u123d\u1295 \u12d3\u1235\u1270\u121d\u122e (Machine Learning) \u12a0\u121b\u12ab\u129d\u1290\u1275 \u12e8\u133d\u1201\u134d \u1293\u1219\u1293\u12ce\u127d \u1260\u12a0\u122d\u1272\u134a\u123b\u120d \u12a2\u1295\u1270\u1208\u1300\u1295\u1235 \u1225\u122d\u12d3\u1275 \u1208\u121b\u1230\u120d\u1320\u1295\u1363 \u12e8\u133d\u1201\u134d \u12f3\u1273\u1295 \u1218\u1230\u1265\u1230\u1265 \u12a5\u1293 \u121b\u12f0\u122b\u1300\u1275\u1364 \u12e8\u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d\u1295 /Natural Language Processing Tools/ \u1260\u1218\u1320\u1240\u121d \u12e8\u133d\u1201\u134d \u12f3\u1273\u1295 \u1355\u122e\u1230\u1235 \u121b\u12f5\u1228\u130d \u1270\u1240\u12f3\u121a \u12a5\u1293 \u1218\u1230\u1228\u1273\u12ca \u1309\u12f3\u12ed \u1290\u12cd\u1362\n    \"\"\"\n\n    # Define a custom preprocessor pipeline\n    custom_pipeline = [\n      preprocessing.remove_emojis, \n      preprocessing.remove_digits,\n      preprocessing.remove_ethiopic_punct,\n      preprocessing.remove_english_chars, \n      preprocessing.remove_punct\n    ]\n    \n    # `clean_amharic` function takes a custom pipeline, if not uses the default pipeline\n    cleaned = clean_amharic(input_text, abbrev=False, pipeline=custom_pipeline)\n\n    # print the `clean` text:\n    print(cleaned)\n    # output: \u121a\u12eb\u12dd\u12eb \u12d3\u1218\u1270 \u121d\u1205\u1228\u1275 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12a0\u1201\u1295 \u12ab\u1208\u1260\u1275 \u12dd\u1245\u1270\u129b \u12f0\u1228\u1303 \u12c8\u12f0 \u120b\u1240 \u12f0\u1228\u1303 \u1208\u121b\u12f5\u1228\u1235 \u1200\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u12a0\u1208\u121d \u1270\u12f0\u122b\u123d \u1208\u121b\u12f5\u1228\u130d \u12a0\u1308\u122b\u12ca \u12a0\u1245\u121d\u1295 \u1208\u121b\u1233\u12f0\u130d \u12a5\u1293 \u1270\u1320\u1243\u121a \u1208\u1218\u1206\u1295 \u1260\u130b\u122b \u12a0\u1265\u122e \u1218\u1235\u122b\u1271 \u12a5\u1305\u130d \u1320\u1243\u121a \u1290\u12cd \u1260\u121b\u123d\u1295 \u12a0\u1235\u1270\u121d\u122e \u12a0\u121b\u12ab\u129d\u1290\u1275 \u12e8\u1345\u1201\u134d \u1293\u1219\u1293\u12ce\u127d \u1260\u12a0\u122d\u1272\u134a\u123b\u120d \u12a2\u1295\u1270\u1208\u1300\u1295\u1235 \u1235\u122d\u12a0\u1275 \u1208\u121b\u1230\u120d\u1320\u1295 \u12e8\u1345\u1201\u134d \u12f3\u1273\u1295 \u1218\u1230\u1265\u1230\u1265 \u12a5\u1293 \u121b\u12f0\u122b\u1300\u1275 \u12e8\u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d\u1295 \u1260\u1218\u1320\u1240\u121d \u12e8\u1345\u1201\u134d \u12f3\u1273\u1295 \u1355\u122e\u1230\u1235 \u121b\u12f5\u1228\u130d \u1270\u1240\u12f3\u121a \u12a5\u1293 \u1218\u1230\u1228\u1273\u12ca \u1309\u12f3\u12ed \u1290\u12cd\n    ```\n\n2. Tokenization - Sentence\n    - Here is a simple example of performing sentence tokenization on a piece of plaintext using Amharic document:\n    - Within Amharic document, annotations are further stored in `Sentences`\n\n    ```python\n    from etnltk import Amharic\n\n    sample_text = \"\"\"\n      \u12e8\u121b\u123d\u1295 \u1208\u122d\u1292\u1295\u130d \u1235\u120d\u1270-\u1240\u1218\u122e\u127d  (Algorithms) \u1260\u1218\u1320\u1240\u121d \u124b\u1295\u124b\u12ce\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12a5\u1293 \u1218\u1228\u12f3\u1275\u1363 \u12e8\u133d\u1201\u134d \u12ed\u12d8\u1276\u127d\u1295 \u1218\u1208\u12e8\u1275\u1363 \u12e8\u124b\u1295\u124b\u1295 \u1218\u12cb\u1245\u122d \u1218\u1270\u1295\u1270\u1295 \u12e8\u121a\u12eb\u1235\u127d\u1209 \u12e8\u1203\u1308\u122a\u129b \u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d (NLP tools) \u1363 \u1235\u120d\u1270-\u1240\u1218\u122e\u127d \u12a5\u1293 \u121e\u12f4\u120e\u127d\u1295 \u121b\u12d8\u130b\u1300\u1275 \u1270\u1308\u1262 \u1290\u12cd\u1362 \u1260\u12da\u1205\u121d \u1218\u1230\u1228\u1275 \u12a0\u121b\u122d\u129b\u1363 \u12a0\u134b\u1295 \u12a6\u122e\u121e\u1363 \u1236\u121b\u120a\u129b \u12a5\u1293 \u1275\u130d\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u121b\u123d\u1295 \u12e8\u121b\u1235\u1270\u121b\u122d \u1202\u12f0\u1275\u1295 \u1240\u120b\u120d\u1293 \u12e8\u1270\u1240\u120b\u1270\u134d \u12a5\u1295\u12f2\u1206\u1295 \u12eb\u1235\u127d\u120b\u120d\u1361\u1361\n    \"\"\"\n\n    # Annotating Amharic Text\n    doc = Amharic(sample_text)\n\n    # print all list of `Sentence` in a document:\n    print(doc.sentences)\n    # output: [Sentence(\"\u12e8\u121b\u123d\u1295 \u1208\u122d\u1292\u1295\u130d \u1235\u120d\u1270\u1240\u1218\u122e\u127d \u1260\u1218\u1320\u1240\u121d \u124b\u1295\u124b\u12ce\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12a5\u1293 \u1218\u1228\u12f3\u1275 \u12e8\u1345\u1201\u134d \u12ed\u12d8\u1276\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12e8\u124b\u1295\u124b\u1295 \u1218\u12cb\u1245\u122d \u1218\u1270\u1295\u1270\u1295 \u12e8\u121a\u12eb\u1235\u127d\u1209 \u12e8\u1200\u1308\u122a\u129b \u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d \u1235\u120d\u1270\u1240\u1218\u122e\u127d \u12a5\u1293 \u121e\u12f4\u120e\u127d\u1295 \u121b\u12d8\u130b\u1300\u1275 \u1270\u1308\u1262 \u1290\u12cd\"), Sentence(\"\u1260\u12da\u1205\u121d \u1218\u1230\u1228\u1275 \u12a0\u121b\u122d\u129b \u12a0\u134b\u1295 \u12a6\u122e\u121e \u1236\u121b\u120a\u129b \u12a5\u1293 \u1275\u130d\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u121b\u123d\u1295 \u12e8\u121b\u1235\u1270\u121b\u122d \u1202\u12f0\u1275\u1295 \u1240\u120b\u120d\u1293 \u12e8\u1270\u1240\u120b\u1270\u134d \u12a5\u1295\u12f2\u1206\u1295 \u12eb\u1235\u127d\u120b\u120d\")]\n    ```\n\n    - Here is another example of performing sentence tokenization on a piece of plaintext using `sentence_tokenize` function:\n\n    ```python\n    from etnltk.tokenize.am import sent_tokenize\n\n    sample_text = \"\"\"\n      \u12e8\u121b\u123d\u1295 \u1208\u122d\u1292\u1295\u130d \u1235\u120d\u1270-\u1240\u1218\u122e\u127d  (Algorithms) \u1260\u1218\u1320\u1240\u121d \u124b\u1295\u124b\u12ce\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12a5\u1293 \u1218\u1228\u12f3\u1275\u1363 \u12e8\u133d\u1201\u134d \u12ed\u12d8\u1276\u127d\u1295 \u1218\u1208\u12e8\u1275\u1363 \u12e8\u124b\u1295\u124b\u1295 \u1218\u12cb\u1245\u122d \u1218\u1270\u1295\u1270\u1295 \u12e8\u121a\u12eb\u1235\u127d\u1209 \u12e8\u1203\u1308\u122a\u129b \u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d (NLP tools) \u1363 \u1235\u120d\u1270-\u1240\u1218\u122e\u127d \u12a5\u1293 \u121e\u12f4\u120e\u127d\u1295 \u121b\u12d8\u130b\u1300\u1275 \u1270\u1308\u1262 \u1290\u12cd\u1362 \u1260\u12da\u1205\u121d \u1218\u1230\u1228\u1275 \u12a0\u121b\u122d\u129b\u1363 \u12a0\u134b\u1295 \u12a6\u122e\u121e\u1363 \u1236\u121b\u120a\u129b \u12a5\u1293 \u1275\u130d\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u121b\u123d\u1295 \u12e8\u121b\u1235\u1270\u121b\u122d \u1202\u12f0\u1275\u1295 \u1240\u120b\u120d\u1293 \u12e8\u1270\u1240\u120b\u1270\u134d \u12a5\u1295\u12f2\u1206\u1295 \u12eb\u1235\u127d\u120b\u120d\u1361\u1361\n    \"\"\"\n\n    # Annotating a Document\n    sentences = sent_tokenize(sample_text)\n\n    # print all list of sentence:\n    print(sentences)\n    # output: ['\u12e8\u121b\u123d\u1295 \u1208\u122d\u1292\u1295\u130d \u1235\u120d\u1270\u1240\u1218\u122e\u127d \u1260\u1218\u1320\u1240\u121d \u124b\u1295\u124b\u12ce\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12a5\u1293 \u1218\u1228\u12f3\u1275 \u12e8\u1345\u1201\u134d \u12ed\u12d8\u1276\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12e8\u124b\u1295\u124b\u1295 \u1218\u12cb\u1245\u122d \u1218\u1270\u1295\u1270\u1295 \u12e8\u121a\u12eb\u1235\u127d\u1209 \u12e8\u1200\u1308\u122a\u129b \u1293\u1279\u122b\u120d \u120b\u1295\u1309\u12cc\u1305 \u1355\u122e\u1230\u1232\u1295\u130d \u1271\u120e\u127d \u1235\u120d\u1270\u1240\u1218\u122e\u127d \u12a5\u1293 \u121e\u12f4\u120e\u127d\u1295 \u121b\u12d8\u130b\u1300\u1275 \u1270\u1308\u1262 \u1290\u12cd', '\u1260\u12da\u1205\u121d \u1218\u1230\u1228\u1275 \u12a0\u121b\u122d\u129b \u12a0\u134b\u1295 \u12a6\u122e\u121e \u1236\u121b\u120a\u129b \u12a5\u1293 \u1275\u130d\u122d\u129b \u124b\u1295\u124b\u12ce\u127d\u1295 \u1208\u121b\u123d\u1295 \u12e8\u121b\u1235\u1270\u121b\u122d \u1202\u12f0\u1275\u1295 \u1240\u120b\u120d\u1293 \u12e8\u1270\u1240\u120b\u1270\u134d \u12a5\u1295\u12f2\u1206\u1295 \u12eb\u1235\u127d\u120b\u120d']\n\n3. Tokenization - Word\n    - Here is a simple example of performing word tokenization on a piece of plaintext using AmharicDocument:\n    - Within Amharic focument, annotations are further stored in `Words`.\n\n    ```python\n    from etnltk import AmharicDocument\n\n    sample_text = \"\"\"\n      \u201c\u1270\u1228\u129b\u1363 \u1270\u1228\u129b!\u201d \u12a0\u1208 \u1290\u122d\u1231\u1362 \u12c8\u12ed\u12d8\u122e\n      \u1273\u122a\u12b3\u1363 \u201c\u12a0\u1264\u1275!\u201d \u1265\u1208\u12cd \u12e8\u1201\u1208\u1275\n      \u12d3\u1218\u1275 \u120d\u1303\u1278\u12cd\u1295 \u12ed\u12d8\u12cd \u1308\u1261\u1362\n      \u201c\u121d\u1291\u1295 \u1290\u12cd \u12eb\u1218\u1218\u12cd?\u201d \u12f6\u12ad\u1270\u122f\n      \u1320\u12e8\u1241\u1362 \u201c\u12a0\u12eb\u12e9\u1275\u121d! \u1340\u1309\u1229 \u1233\u1235\u1277\u120d\u1364\n      \u1206\u12f1 \u1270\u1290\u134d\u1277\u120d\u1364 \u12f5\u12f1\u121d \u12ed\u12f0\u121b\u120d\u201d\n      \u12a0\u1209 \u12c8\u12ed\u12d8\u122e \u1273\u122a\u12b3\u1362 \u12f6\u12ad\u1270\u122f\u121d\u1363\n      \u201c\u1260\u1323\u121d \u12eb\u1233\u12dd\u1293\u120d\u1364 \u12a5\u1295\u12f0\u12da\u1205\n      \u12eb\u12f0\u1228\u1308\u12cd \u12e8\u1270\u1218\u1323\u1320\u1290 \u121d\u130d\u1265 \u12a0\u1208\u121b\u130d\u1298\u1271 \u1290\u12cd\u1362 \u12a0\u1201\u1295\u121d \u12c8\u1270\u1275\u1363\n      \u12a5\u1295\u1241\u120b\u120d\u1363 \u121b\u122d\u1363 \u12a0\u1275\u12ad\u120d\u1275\u1293 \u134d\u122b\u134d\u122c \u12ed\u1218\u130d\u1261\u1275\u1364 \u1276\u120e \u12ed\u123b\u1208\u12cb\u120d\u1364\n      \u1208\u12a0\u1201\u1291 \u130d\u1295 \u1218\u12f5\u1283\u1292\u1275 \u12a0\u12dd\u1208\u1273\u1208\u1201\u201d \u1260\u121b\u1208\u1275 \u12a0\u1235\u1228\u12f7\u1278\u12cd\u1362 \u12c8\u12ed\u12d8\u122e\n      \u1273\u122a\u12b3\u121d \u201c\u12c8\u12ed \u12a0\u1208\u121b\u12c8\u1245! \u120d\u1304\u1295 \u1260\u121d\u130d\u1265 \u12a5\u1325\u1228\u1275 \u1308\u12f5\u12ec\u12cd \u1290\u1260\u122d\"\n      \u1260\u121b\u1208\u1275 \u12a0\u1208\u1240\u1231\u1362\n\n      \"\"\"\n    \n    # Annotating Amharic Text\n    doc = Amharic(sample_text)\n\n    # print all list of `AmharicWord` in a document:\n    print(doc.words)\n    # output: ['\u1270\u1228\u129b', '\u1270\u1228\u129b', '\u12a0\u1208', '\u1290\u122d\u1231', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3', '\u12a0\u1264\u1275', '\u1265\u1208\u12cd', '\u12e8\u1201\u1208\u1275', '\u12a0\u1218\u1275', '\u120d\u1303\u1278\u12cd\u1295', '\u12ed\u12d8\u12cd', '\u1308\u1261', '\u121d\u1291\u1295', '\u1290\u12cd', '\u12eb\u1218\u1218\u12cd', '\u12f6\u12ad\u1270\u122f', '\u1320\u12e8\u1241', '\u12a0\u12eb\u12e9\u1275\u121d', '\u1340\u1309\u1229', '\u1233\u1235\u1277\u120d', '\u1206\u12f1', '\u1270\u1290\u134d\u1277\u120d', '\u12f5\u12f1\u121d', '\u12ed\u12f0\u121b\u120d', '\u12a0\u1209', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3', '\u12f6\u12ad\u1270\u122f\u121d', '\u1260\u1323\u121d', '\u12eb\u1233\u12dd\u1293\u120d', '\u12a5\u1295\u12f0\u12da\u1205', '\u12eb\u12f0\u1228\u1308\u12cd', '\u12e8\u1270\u1218\u1323\u1320\u1290', '\u121d\u130d\u1265', '\u12a0\u1208\u121b\u130d\u1298\u1271', '\u1290\u12cd', '\u12a0\u1201\u1295\u121d', '\u12c8\u1270\u1275', '\u12a5\u1295\u1241\u120b\u120d', '\u121b\u122d', '\u12a0\u1275\u12ad\u120d\u1275\u1293', '\u134d\u122b\u134d\u122c', '\u12ed\u1218\u130d\u1261\u1275', '\u1276\u120e', '\u12ed\u123b\u1208\u12cb\u120d', '\u1208\u12a0\u1201\u1291', '\u130d\u1295', '\u1218\u12f5\u1200\u1292\u1275', '\u12a0\u12dd\u1208\u1273\u1208\u1201', '\u1260\u121b\u1208\u1275', '\u12a0\u1235\u1228\u12f7\u1278\u12cd', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3\u121d', '\u12c8\u12ed', '\u12a0\u1208\u121b\u12c8\u1245', '\u120d\u1304\u1295', '\u1260\u121d\u130d\u1265', '\u12a5\u1325\u1228\u1275', '\u1308\u12f5\u12ec\u12cd', '\u1290\u1260\u122d', '\u1260\u121b\u1208\u1275', '\u12a0\u1208\u1240\u1231']\n    ```\n\n    - Here is another example of performing word tokenization on a piece of plaintext using `word_tokenize` function:\n\n    ```python\n    from etnltk.tokenize.am import word_tokenize\n\n    sample_text = \"\"\"\n      \u201c\u1270\u1228\u129b\u1363 \u1270\u1228\u129b!\u201d \u12a0\u1208 \u1290\u122d\u1231\u1362 \u12c8\u12ed\u12d8\u122e\n      \u1273\u122a\u12b3\u1363 \u201c\u12a0\u1264\u1275!\u201d \u1265\u1208\u12cd \u12e8\u1201\u1208\u1275\n      \u12d3\u1218\u1275 \u120d\u1303\u1278\u12cd\u1295 \u12ed\u12d8\u12cd \u1308\u1261\u1362\n      \u201c\u121d\u1291\u1295 \u1290\u12cd \u12eb\u1218\u1218\u12cd?\u201d \u12f6\u12ad\u1270\u122f\n      \u1320\u12e8\u1241\u1362 \u201c\u12a0\u12eb\u12e9\u1275\u121d! \u1340\u1309\u1229 \u1233\u1235\u1277\u120d\u1364\n      \u1206\u12f1 \u1270\u1290\u134d\u1277\u120d\u1364 \u12f5\u12f1\u121d \u12ed\u12f0\u121b\u120d\u201d\n      \u12a0\u1209 \u12c8\u12ed\u12d8\u122e \u1273\u122a\u12b3\u1362 \u12f6\u12ad\u1270\u122f\u121d\u1363\n      \u201c\u1260\u1323\u121d \u12eb\u1233\u12dd\u1293\u120d\u1364 \u12a5\u1295\u12f0\u12da\u1205\n      \u12eb\u12f0\u1228\u1308\u12cd \u12e8\u1270\u1218\u1323\u1320\u1290 \u121d\u130d\u1265 \u12a0\u1208\u121b\u130d\u1298\u1271 \u1290\u12cd\u1362 \u12a0\u1201\u1295\u121d \u12c8\u1270\u1275\u1363\n      \u12a5\u1295\u1241\u120b\u120d\u1363 \u121b\u122d\u1363 \u12a0\u1275\u12ad\u120d\u1275\u1293 \u134d\u122b\u134d\u122c \u12ed\u1218\u130d\u1261\u1275\u1364 \u1276\u120e \u12ed\u123b\u1208\u12cb\u120d\u1364\n      \u1208\u12a0\u1201\u1291 \u130d\u1295 \u1218\u12f5\u1283\u1292\u1275 \u12a0\u12dd\u1208\u1273\u1208\u1201\u201d \u1260\u121b\u1208\u1275 \u12a0\u1235\u1228\u12f7\u1278\u12cd\u1362 \u12c8\u12ed\u12d8\u122e\n      \u1273\u122a\u12b3\u121d \u201c\u12c8\u12ed \u12a0\u1208\u121b\u12c8\u1245! \u120d\u1304\u1295 \u1260\u121d\u130d\u1265 \u12a5\u1325\u1228\u1275 \u1308\u12f5\u12ec\u12cd \u1290\u1260\u122d\"\n      \u1260\u121b\u1208\u1275 \u12a0\u1208\u1240\u1231\u1362\n\n    \"\"\"\n      \n    # word tokenization\n    words = word_tokenize(sample_text)\n\n    # print all list of word:\n    print(words)\n    # output: ['\u1270\u1228\u129b', '\u1270\u1228\u129b', '\u12a0\u1208', '\u1290\u122d\u1231', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3', '\u12a0\u1264\u1275', '\u1265\u1208\u12cd', '\u12e8\u1201\u1208\u1275', '\u12a0\u1218\u1275', '\u120d\u1303\u1278\u12cd\u1295', '\u12ed\u12d8\u12cd', '\u1308\u1261', '\u121d\u1291\u1295', '\u1290\u12cd', '\u12eb\u1218\u1218\u12cd', '\u12f6\u12ad\u1270\u122f', '\u1320\u12e8\u1241', '\u12a0\u12eb\u12e9\u1275\u121d', '\u1340\u1309\u1229', '\u1233\u1235\u1277\u120d', '\u1206\u12f1', '\u1270\u1290\u134d\u1277\u120d', '\u12f5\u12f1\u121d', '\u12ed\u12f0\u121b\u120d', '\u12a0\u1209', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3', '\u12f6\u12ad\u1270\u122f\u121d', '\u1260\u1323\u121d', '\u12eb\u1233\u12dd\u1293\u120d', '\u12a5\u1295\u12f0\u12da\u1205', '\u12eb\u12f0\u1228\u1308\u12cd', '\u12e8\u1270\u1218\u1323\u1320\u1290', '\u121d\u130d\u1265', '\u12a0\u1208\u121b\u130d\u1298\u1271', '\u1290\u12cd', '\u12a0\u1201\u1295\u121d', '\u12c8\u1270\u1275', '\u12a5\u1295\u1241\u120b\u120d', '\u121b\u122d', '\u12a0\u1275\u12ad\u120d\u1275\u1293', '\u134d\u122b\u134d\u122c', '\u12ed\u1218\u130d\u1261\u1275', '\u1276\u120e', '\u12ed\u123b\u1208\u12cb\u120d', '\u1208\u12a0\u1201\u1291', '\u130d\u1295', '\u1218\u12f5\u1200\u1292\u1275', '\u12a0\u12dd\u1208\u1273\u1208\u1201', '\u1260\u121b\u1208\u1275', '\u12a0\u1235\u1228\u12f7\u1278\u12cd', '\u12c8\u12ed\u12d8\u122e', '\u1273\u122a\u12b3\u121d', '\u12c8\u12ed', '\u12a0\u1208\u121b\u12c8\u1245', '\u120d\u1304\u1295', '\u1260\u121d\u130d\u1265', '\u12a5\u1325\u1228\u1275', '\u1308\u12f5\u12ec\u12cd', '\u1290\u1260\u122d', '\u1260\u121b\u1208\u1275', '\u12a0\u1208\u1240\u1231']\n\n4. Normalization\n    1. Character Level Normalization such as \"`\u1338`\u1200\u12ed\" and \"`\u1340`\u1210\u12ed\"\n    2. Labialized Character Normalzation such as \"\u121e\u120d`\u1271\u12cb`\u120d\" to \"\u121e\u120d`\u1277`\u120d\"\n    3. Short Form Expansion such as \"`\u12a0.\u12a0`\" to \"`\u12a0\u12f2\u1235 \u12a0\u1260\u1263`\"\n    4. Punctuation Normalization such as `::` to `\u1362`\n\n    - Here is a simple example of performing normalization on a piece of plaintext using `normalize` function:\n\n    ```python\n    from etnltk.lang.am import normalize\n\n    sample_text = \"\"\"\n      \u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12d3.\u121d \u1260\u12d3\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u12f5\u1228\u12ad \u120b\u12ed\n      \u12e8\u1203\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d \u1275\u122d\u1309\u121d \u12a0\u1308\u120d\u130d\u120e\u1275\u1363 \n      \u127b\u1275\u1266\u1275 (\u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u1208\u12cb\u12c8\u132b \u122e\u1266\u1275): \n      \u12e8\u1345\u1201\u134d \u1230\u1290\u12f6\u127d \u1208\u1218\u1208\u12e8\u1275\u1363 \u12e8\u1243\u120b\u1275 \u1275\u12ad\u12ad\u1208\u129b\u1290\u1275\u1295 \u1208\u121b\u1228\u130b\u1308\u1325\u1363 \n      \u1260\u124b\u1295\u124b\u1295 \u1215\u130d\u130b\u1275 \u1218\u1220\u1228\u1275 \u133d\u1211\u134e\u127d\u1295 \u1208\u121b\u12cb\u1240\u122d \u12a5\u1293 \u1208\u1218\u1218\u1235\u1228\u1275\u1363 \n      \u1228\u1305\u121d \u133d\u1201\u134e\u127d\u1295 \u1208\u121b\u1233\u1320\u122d\u1363 \u12a0\u1295\u12b3\u122d \u1309\u12f3\u12ee\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12c8\u12ed\u121d \u1325\u1245\u120d \u1203\u1233\u1265 \u1208\u121b\u12cd\u1323\u1275\u1363 \n      \u1295\u130d\u130d\u122d\u1295 \u12c8\u12f0 \u133d\u1201\u134d \u1208\u1218\u1240\u12e8\u122d \u12e8\u121a\u12eb\u1235\u127d\u1209 \u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u1339\u12cb\u120d::\n    \"\"\"\n\n    # normalization\n    normalized_text = normalize(sample_text)\n\n    # The following example shows how to print all normalized in a document:\n    print(normalized_text)\n    # output: \u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12a0\u1218\u1270 \u121d\u1205\u1228\u1275 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u12f5\u1228\u12ad \u120b\u12ed\n    # \u12e8\u1200\u1308\u122d\u129b \u124b\u1295\u124b\u12ce\u127d \u1275\u122d\u1309\u121d \u12a0\u1308\u120d\u130d\u120e\u1275\u1363 \n    # \u127b\u1275\u1266\u1275 (\u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u1208\u12cb\u12c8\u132b \u122e\u1266\u1275)\u1361 \n    # \u12e8\u1345\u1201\u134d \u1230\u1290\u12f6\u127d \u1208\u1218\u1208\u12e8\u1275\u1363 \u12e8\u1243\u120b\u1275 \u1275\u12ad\u12ad\u1208\u129b\u1290\u1275\u1295 \u1208\u121b\u1228\u130b\u1308\u1325\u1363 \n    # \u1260\u124b\u1295\u124b\u1295 \u1205\u130d\u130b\u1275 \u1218\u1230\u1228\u1275 \u1345\u1201\u134e\u127d\u1295 \u1208\u121b\u12cb\u1240\u122d \u12a5\u1293 \u1208\u1218\u1218\u1235\u1228\u1275\u1363 \n    # \u1228\u1305\u121d \u1345\u1201\u134e\u127d\u1295 \u1208\u121b\u1233\u1320\u122d\u1363 \u12a0\u1295\u12b3\u122d \u1309\u12f3\u12ee\u127d\u1295 \u1218\u1208\u12e8\u1275 \u12c8\u12ed\u121d \u1325\u1245\u120d \u1200\u1233\u1265 \u1208\u121b\u12cd\u1323\u1275\u1363 \n    # \u1295\u130d\u130d\u122d\u1295 \u12c8\u12f0 \u1345\u1201\u134d \u1208\u1218\u1240\u12e8\u122d \u12e8\u121a\u12eb\u1235\u127d\u1209 \u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u133f\u120d\u1362 \"\"\"\n    ```\n\n    - Here is another example of performing normalization on a piece of plaintext using `normalize_char`, `normalize_punct`, `normalize_labialized`, `normalize_shortened` function:\n\n    ```python\n    from etnltk.lang.am.normalizer import ( \n      normalize_labialized, \n      normalize_shortened,\n      normalize_punct,\n      normalize_char\n    )\n\n    # normalize labialized \n    normalized_text = normalize_labialized(\"\u1295\u130d\u130d\u122d\u1295 \u12c8\u12f0 \u133d\u1201\u134d \u1208\u1218\u1240\u12e8\u122d \u12e8\u121a\u12eb\u1235\u127d\u1209 \u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u1339\u12cb\u120d\")\n    print(normalized_text)\n    # output: \u1295\u130d\u130d\u122d\u1295 \u12c8\u12f0 \u1345\u1201\u134d \u1208\u1218\u1240\u12e8\u122d \u12e8\u121a\u12eb\u1235\u127d\u1209 \u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u133f\u120d\n\n    # normalize short forms\n    normalized_text = normalize_shortened(\"\u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12d3.\u121d \u1260\u12d3\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u12f5\u1228\u12ad\")\n    print(normalized_text)\n    # output: \u121a\u12eb\u12dd\u12eb 14\u1363 2014 \u12d3\u1218\u1270 \u121d\u1205\u1228\u1275 \u1260\u12a0\u1308\u122d \u12f0\u1228\u1303 \u12e8\u1230\u12cd \u1230\u122b\u123d \u12a0\u1235\u1270\u12cd\u120e\u1275 \u12e8\u12cd\u12ed\u12ed\u1275 \u1218\u12f5\u1228\u12ad\n\n    # normalize punctuation\n    normalized_text = normalize_punct(\"\u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u1339\u12cb\u120d::\")\n    print(normalized_text)\n    # output: \u1218\u1270\u130d\u1260\u122a\u12eb\u12ce\u127d\u1295 \u121b\u120d\u121b\u1275 \u12a0\u1235\u1228\u120b\u130a\u1290\u1271 \u1270\u1308\u120d\u133f\u120d\u1362\n\n    # normalize characters\n    normalized_text = normalize_char(\"\u1260\u124b\u1295\u124b\u12c9 \u1215\u130d\u130b\u1275 \u1218\u1220\u1228\u1275 \u133d\u1211\u134e\u127d\u1295 \u121b\u12cb\u1240\u122d \u12a5\u1293 \u1218\u1218\u1225\u1228\u1275\")\n    print(normalized_text)\n    # output: \u1260\u124b\u1295\u124b\u12c9 \u1205\u130d\u130b\u1275 \u1218\u1230\u1228\u1275 \u1345\u1201\u134e\u127d\u1295 \u121b\u12cb\u1240\u122d \u12a5\u1293 \u1218\u1218\u1235\u1228\u1275\n\n## Features\n\n- Text preprocessing functions.\n\n    ``` python\n    from etnltk.lang.am import preprocessing\n    ```\n\n    | Function | Description |\n    -----------|-------------|\n    | remove_whitespaces | Remove extra spaces, tabs, and new lines from a text string\n    | remove_links | Remove URLs from a text string\n    | remove_tags | Remove HTML tags from a text string\n    | remove_emojis | Remove emojis from a text string\n    | remove_email | Remove email adresses from a text string\n    | remove_digits | Remove all digits from a text string\n    | remove_english_chars | Remove ascii characters from a text string\n    | remove_arabic_chars | Remove arabic characters and numerals from a text string\n    | remove_chinese_chars | Remove chinese characters from a text string\n    | remove_ethiopic_digits | Remove all ethiopic digits from a text string\n    | remove_ethiopic_punct | Remove ethiopic punctuations from a text string\n    | remove_non_ethiopic | Remove non ethioipc characters from a text string\n    | remove_stopwords | Remove stop words\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/robikieq/etnltk.git",
            "keywords": "['nlp','ethiopic','amharic','tokenization','preprocessing','text-analytics']",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "etnltk",
            "package_url": "https://pypi.org/project/etnltk/",
            "platform": null,
            "project_url": "https://pypi.org/project/etnltk/",
            "project_urls": {
                "Homepage": "https://github.com/robikieq/etnltk.git"
            },
            "release_url": "https://pypi.org/project/etnltk/0.0.22/",
            "requires_dist": [
                "textsearch (>=0.0.21)",
                "emoji (>=1.7.0)"
            ],
            "requires_python": ">=3.6",
            "summary": "Ethiopian Natural Language Toolkit",
            "version": "0.0.22",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 13837594,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "3105d3a4de55b09e09dc9d860e370397",
                    "sha256": "c19dbe624e48c91f15bc7b4e4cd343105ef31f34dc03a8e433ee4c152893e788"
                },
                "downloads": -1,
                "filename": "etnltk-0.0.22-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "3105d3a4de55b09e09dc9d860e370397",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6",
                "size": 20609,
                "upload_time": "2022-05-17T06:36:15",
                "upload_time_iso_8601": "2022-05-17T06:36:15.933157Z",
                "url": "https://files.pythonhosted.org/packages/7e/75/e4080c75afdfc6b435f2ad0fab9d25eebcdaefdbed6d1875ea218c6255fd/etnltk-0.0.22-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "ac2c018d36f36bca06d4f66e1163f2c9",
                    "sha256": "c231a918631cb4c1a5cba76bc3b493d1afc84ba6a5092457c71f3fe8596c4ce9"
                },
                "downloads": -1,
                "filename": "etnltk-0.0.22.tar.gz",
                "has_sig": false,
                "md5_digest": "ac2c018d36f36bca06d4f66e1163f2c9",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 20583,
                "upload_time": "2022-05-17T06:36:18",
                "upload_time_iso_8601": "2022-05-17T06:36:18.190222Z",
                "url": "https://files.pythonhosted.org/packages/f6/f8/7c58e6525b8764e886ff1b5f2a3e58b7812da0f8af8567010cd30fb721a7/etnltk-0.0.22.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}