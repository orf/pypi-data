{
    "1.0.1": {
        "info": {
            "author": "Dmtry Barsukov",
            "author_email": "riZZZhik@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# \u0412\u0441\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0442 )\n\u042d\u0442\u043e\u0442 \u043a\u043e\u0434 \u043d\u0443\u0436\u0435\u043d \u0447\u0442\u043e-\u0431\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 TensorRT \u0438 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0438\u0442\u044c \u0432 \u043b\u0430\u0439\u0432\u0435.\n\n\u0412\u0430\u0436\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442: engine \u0444\u0430\u0439\u043b\u044b \u043d\u0435 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438, \u043d\u0443\u0436\u043d\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0434 \u043a\u0430\u0436\u0434\u0443\u044e \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e.\n\n\n# Recommended pipeline\n- \u0417\u0430\u0440\u0430\u043d\u0435\u0435 \u043f\u0435\u0440\u0435\u0433\u043d\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0437 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430 \u0432 ONNX. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0432 \u044d\u0442\u043e\u043c \u043c\u043e\u0434\u0443\u043b\u0435. `python -c \"import onnx_converter; help(onnx_converter)\"`\n- \u0421\u043a\u0430\u0447\u0430\u0442\u044c \u044d\u0442\u0443 \u043b\u0438\u0431\u0443 \u0438 onnx \u0444\u0430\u0439\u043b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0435 \u0436\u0435\u043b\u0435\u0437\u043e.\n- \u041f\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438\n- \u0423\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u043f\u0443\u0442\u0438 \u0434\u043b\u044f \u044d\u043d\u0432\u043e\u0432 \u0438\u0437 \u043f\u0443\u043d\u043a\u0442\u0430 _Installation_\n- \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 onnx2trt: `python onnx2trt.py --help`\n  - \u0414\u043b\u044f INT8 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0441\u043e\u0437\u0434\u0430\u0442\u044c calibration \u0444\u0430\u0439\u043b \u0447\u0435\u0440\u0435\u0437 onnx2trt_handmade. `python -c \"from onnx2trt_handmade import onnx2trt_handmade; help(onnx2trt_handmade)\"`\n- \u0417\u0430\u043f\u043e\u043c\u043d\u0438\u0442\u044c \u043a\u0443\u0434\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0441\u044f \u0438\u0442\u043e\u0433\u043e\u0432\u044b\u0439 engine \u0444\u0430\u0439\u043b.\n- \u0414\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441 InferenceTRT. `python -c \"from inference import InferenceTRT; help(InferenceTRT)\"`\n\n\n# Installation\n\n## Envs\n- `CUDA_HOME`: \u041f\u0443\u0442\u044c \u0434\u043e \u043f\u0430\u043f\u043a\u0438 \u0441 \u043a\u0443\u0434\u043e\u0439.\n- `LD_LIBRARY_PATH`:\n  - \u041f\u0443\u0442\u044c \u0434\u043e `lib64` \u0432 \u043f\u0430\u043f\u043a\u0435 \u0441 \u043a\u0443\u0434\u043e\u0439.\n  - \u041f\u0443\u0442\u044c \u0434\u043e `targets/x86_64-linux/lib/` \u0432 \u043f\u0430\u043f\u043a\u0435 \u0441 \u043a\u0443\u0434\u043e\u0439.\n  - \u041f\u0443\u0442\u044c \u0434\u043e `lib` \u0432 \u043f\u0430\u043f\u043a\u0435 \u0441 TensorRT.\n\n## `trtexec`\n\u0414\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 onnx2trt \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043f\u0443\u0442\u044c \u0434\u043e trtexec. \u041a\u0430\u043a \u043f\u0440\u0430\u0432\u0438\u043b\u043e, \u043e\u043d \u0445\u0440\u0430\u043d\u0438\u0442\u0441\u044f \u043f\u043e \u043f\u0443\u0442\u0438 `/targets/x86_64-linux-gnu/bin/trtexec` \u0432 \u043f\u0430\u043f\u043a\u0435 \u0441 TensorRT.\n\n## `TensorRT`\nTested on version `8.2.2`\n\n## `CUDA`\n\n## `PyCUDA`\n\n\n# Convert to ONNX\n\n## `fastai2onnx`\n\u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0443 \u0438\u0437 FastAI \u0432 ONNX.\n\n### Args:\n- `fastai_path` (str): Path to fastai model file.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n- `input_names` (tuple or list): Model's input names.\n- `output_names` (tuple or list): Model's output names.\n\n## `torch2onnx`\n\u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0443 \u0438\u0437 PyTorch \u0432 ONNX.\n\n### Args:\n- `model` (torch.nn.Module): PyTorch model.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n- `input_names` (tuple or list): Model's input names.\n- `output_names` (tuple or list): Model's output names.\n\n## `keras2onnx`\n\u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0443 \u0438\u0437 Keras \u0432 ONNX.\n\n### Args:\n- `model` (): Keras model.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n\n\n# Optimize\n\n## `onnx2trt`\n\u0421\u0430\u043c\u044b\u0439 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438 \u043f\u043e\u0434 TensorRT. \u0412 \u043a\u043e\u043d\u0446\u0435 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0430\u0446\u0438\u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0431\u0435\u043d\u0447\u043c\u0430\u0440\u043a\u0438. \u041d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0443 \u0434\u043b\u044f INT8. \n\n_NB! \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043c\u0430\u0448\u0438\u043d\u044b \u043d\u0443\u0436\u043d\u043e \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e, \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0435\u0440\u0435\u0442\u0430\u0449\u0438\u0442\u044c \u0444\u0430\u0439\u043b\u0438\u043a engine \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442._\n\n### Args:\n- `onnx_path` (str): Path to onnx model (opset9).\n- `engine_path` (str): Path to save engine.\n- `dtype` (str or np.dtype): Model data type.\n- `use_sparsity` (bool): Use sparsity boolean.\n- `trtexec_path` (str): Path to trtexec binary.\n- `workspace_size` (int): Size of workspace in MB.\n\n## `onnx2trt_handmade`\n\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c TensorRT Python API.\n\u041b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f calibration \u0444\u0430\u0439\u043b\u0438\u043a\u0430, \u0435\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u0442\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438. \u0418 \u043f\u043e\u0434 \u043a\u0430\u0436\u0434\u0443\u044e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0447\u0435\u0440\u0435\u0437 trtexec. \n\n### Args:\n- `onnx_path` (str): Path to ONNX model.\n- `engine_path` (str): Path to save TensorRT engine.\n- `dtype` (str): Model data type. (fp32, fp16 or int8).\n- `calib` (trt.IInt8EntropyCalibrator2): Calibration class. Only for INT8.\n\n## `Calibration`\n\u041a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0438 INT8.\n\n### `__init__`\n#### Args:\n- `calibration_files` (list or tuple): List of paths to images for calibration.\n- `preprocessor` (function): Image preprocessing function.\n- `shape` (list or tuple): Input image shape. With batch_size.\n- `cache_file` (str): Path to save calibration.\n\n\n# Inference\n\n## InferenceTRT\n\u041a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0432 engine.\n\n### `__init__`\nInitialize class variables.\n#### Args:\n- `engine_path` (str): Path to TensorRT engine file.\n- `device` (cuda.Device): Cuda device to run inference on.\n\n### `load_engine`\nLoad TensorRT engine from file.\n#### Args:\n- `engine_path` (str): Path to TensorRT engine file.\n\n### `predict`\nGet model prediction.\n#### Args:\n- `inputs` (np.ndarray): Model inputs array.\n#### Returns:\n- Model prediction as np.ndarray.\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/riZZZhik/optimize_tensorrt",
            "keywords": "",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimize-tensorrt",
            "package_url": "https://pypi.org/project/optimize-tensorrt/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimize-tensorrt/",
            "project_urls": {
                "Homepage": "https://github.com/riZZZhik/optimize_tensorrt"
            },
            "release_url": "https://pypi.org/project/optimize-tensorrt/1.0.1/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Convert and inference TensorRT models",
            "version": "1.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 14300230,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "bff0bdb1e20e3c38a5a70ec1a77df68c",
                    "sha256": "0ff9404a026c9684448726a989cb9a50ceb7dff8988737b6298c4156b365f1d1"
                },
                "downloads": -1,
                "filename": "optimize_tensorrt-1.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "bff0bdb1e20e3c38a5a70ec1a77df68c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 13200,
                "upload_time": "2022-06-30T20:45:27",
                "upload_time_iso_8601": "2022-06-30T20:45:27.628378Z",
                "url": "https://files.pythonhosted.org/packages/9a/c1/d9b04dd20eca47ab471ff9aa524288c92a69ed7b88e7ac8937a3e3e42a8b/optimize_tensorrt-1.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}