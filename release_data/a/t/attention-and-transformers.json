{
    "0.0.1": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "## Attention mechanisms and Transformers\r\n\r\n\r\n\r\n[![Python 3.10.4](https://img.shields.io/badge/Python-3.10.4-3776AB)](https://www.python.org/downloads/release/python-3104/) [![TensorFlow 2.10.0](https://img.shields.io/badge/TensorFlow-2.10.0-FF6F00?logo=tensorflow)](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0) [![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)\r\n\r\n\r\n\r\n* This goal of this repository is to host basic architecture and model traning code associated with the different attention mechanisms and transformer architecture.\r\n\r\n* At the moment, I more interested in learning and recreating these new architectures from scratch than full-fledged training. For now, I'll just be training these models on small datasets.\r\n\r\n\r\n\r\n**Attention Mechanisms**\r\n\r\n\r\n\r\n<table>\r\n\r\n<thead>\r\n\r\n<tr>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong># No.</strong>\r\n\r\n</th>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong>Mechanism</strong>\r\n\r\n</th>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong>Paper</strong>\r\n\r\n</th>\r\n\r\n</tr>\r\n\r\n</thead>\r\n\r\n<tbody>\r\n\r\n<tr>\r\n\r\n<td style=\"text-align:center\">1</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/VisionTransformers/multihead_self_attention.py\">Multi-head Self Attention</a>\r\n\r\n</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>\r\n\r\n</td>\r\n\r\n</tr>\r\n\r\n<tr>\r\n\r\n<td style=\"text-align:center\">2</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/multihead_self_attention_2D.py\">Multi-head Self Attention 2D</a>\r\n\r\n</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT V1</a>\r\n\r\n</td>\r\n\r\n</tr>\r\n\r\n</tbody>\r\n\r\n</table>\r\n\r\n\r\n\r\n**Transformer Models**\r\n\r\n\r\n\r\n<table>\r\n\r\n<thead>\r\n\r\n<tr>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong># No.</strong>\r\n\r\n</th>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong>Models</strong>\r\n\r\n</th>\r\n\r\n<th style=\"text-align:center\">\r\n\r\n<strong>Paper</strong>\r\n\r\n</th>\r\n\r\n</tr>\r\n\r\n</thead>\r\n\r\n<tbody>\r\n\r\n<tr>\r\n\r\n<td style=\"text-align:center\">1</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/VisionTransformers/vision_transformer.py\">Vision Transformer</a>\r\n\r\n</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words:</a>\r\n\r\n</td>\r\n\r\n</tr>\r\n\r\n<tr>\r\n\r\n<td style=\"text-align:center\">2</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/mobile_vit.py\">MobileViT-V1</a>\r\n\r\n</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a>\r\n\r\n</td>\r\n\r\n</tr>\r\n\r\n<tr>\r\n\r\n<td style=\"text-align:center\">3</td>\r\n\r\n<td style=\"text-align:center\">MobileViT-V2 (under development)</td>\r\n\r\n<td style=\"text-align:center\">\r\n\r\n<a href=\"https://arxiv.org/abs/2206.02680\">Separable Self-attention for Mobile Vision Transformers</a>\r\n\r\n</td>\r\n\r\n</tr>\r\n\r\n</tbody>\r\n\r\n</table>\r\n\r\n\r\n\r\n<!-- **Attention Mechanisms**\r\n\r\n\r\n\r\n|:---------:|:----------------------------:|:-------------------------------------------------------------:|\r\n\r\n| 1         | [Multi-head Self Attention](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT-v1/multihead_self_attention_2D.py)    | [Attention is all you need](https://arxiv.org/abs/1706.03762) |\r\n\r\n| 2         | [Multi-head Self Attention 2D](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT_v1/multihead_self_attention_2D.py) | [MobileViT V1](https://arxiv.org/abs/2110.02178)              |\r\n\r\n\r\n\r\n**Transformer Models**\r\n\r\n\r\n\r\n| **# No.** | **Models**         | **Paper**                                                          |\r\n\r\n|:---------:|:------------------:|:------------------------------------------------------------------:|\r\n\r\n| 1         | [Vision Transformer](https://github.com/veb-101/Attention-and-Transformers/blob/main/VisionTransformers/vision_transformer.py) | [An Image is Worth 16x16 Words:](https://arxiv.org/abs/2010.11929) |\r\n\r\n| 2         | [MobileViT-V1](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT_v1/mobile_vit.py)     | [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)                   |\r\n\r\n| 3         | MobileViT-V2 (under development)| [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680)                   | -->\r\n\r\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.1/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow (==2.10.0) ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF. https://github.com/veb-101/Attention-and-Transformers",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15819240,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "c3613d59a4186f4f13fa17dfbb08999c",
                    "sha256": "3035de547414eb7fc292a4e4e3dcb4bc4b57aa8bb544a1cd4e625eb647f23069"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "c3613d59a4186f4f13fa17dfbb08999c",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 8379,
                "upload_time": "2022-11-18T21:02:35",
                "upload_time_iso_8601": "2022-11-18T21:02:35.618551Z",
                "url": "https://files.pythonhosted.org/packages/8f/f0/8046683eb8b6738c8fafa41c8f6ab317d2331000fa5303fd713134b87ee2/Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "c8511ddbbafc8f9bd4819debf82c1bcf",
                    "sha256": "34559a79d900db5e40f3056e34a1d733d95b249bbf4f2c3d9df8c4db3e259a56"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "c8511ddbbafc8f9bd4819debf82c1bcf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 7613,
                "upload_time": "2022-11-18T21:02:37",
                "upload_time_iso_8601": "2022-11-18T21:02:37.262685Z",
                "url": "https://files.pythonhosted.org/packages/5f/11/abb921146cdd95edd31e8b51a35354a246b9f3e141ea614c8c71ffdc2a7d/Attention_and_Transformers-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}