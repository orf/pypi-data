{
    "0.0.1": {
        "info": {
            "author": "ncoop57",
            "author_email": "nacooper01@wm.edu",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/ncoop57/code_tokenizers",
            "keywords": "nbdev jupyter notebook python tokenizer bpe ast",
            "license": "Apache Software License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "code-tokenizers",
            "package_url": "https://pypi.org/project/code-tokenizers/",
            "platform": null,
            "project_url": "https://pypi.org/project/code-tokenizers/",
            "project_urls": {
                "Homepage": "https://github.com/ncoop57/code_tokenizers"
            },
            "release_url": "https://pypi.org/project/code-tokenizers/0.0.1/",
            "requires_dist": [
                "fastcore",
                "gitpython",
                "pandas",
                "transformers",
                "tree-sitter (==0.20.1)",
                "nbdev ; extra == 'dev'"
            ],
            "requires_python": ">=3.7",
            "summary": "Aligning BPE and AST",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15848971,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "200f7be89ebe212fa28b72497bd92d2c",
                    "sha256": "1129c32e6f83872e5f9eec4256b5b5cf6ae3ab3deba1bd8e5d3dd8c39be7f1f5"
                },
                "downloads": -1,
                "filename": "code_tokenizers-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "200f7be89ebe212fa28b72497bd92d2c",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 110849,
                "upload_time": "2022-11-17T21:23:19",
                "upload_time_iso_8601": "2022-11-17T21:23:19.950202Z",
                "url": "https://files.pythonhosted.org/packages/2f/7f/8d927746897e59c054cea0244e29bfa0454f1c0286d47abbda98e4206c47/code_tokenizers-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5e2efaeebce8897081d5a7bf4e79d39d",
                    "sha256": "87088d95d9a57397e1858370f8c6da25ffd7d888e50be5d49bf4ce28e6b94c58"
                },
                "downloads": -1,
                "filename": "code_tokenizers-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "5e2efaeebce8897081d5a7bf4e79d39d",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 99051,
                "upload_time": "2022-11-17T21:23:22",
                "upload_time_iso_8601": "2022-11-17T21:23:22.447872Z",
                "url": "https://files.pythonhosted.org/packages/c8/cd/1fe052687b36a0882bbfa7772499d43d0f6fae9c6aeca24ce2980ad7b367/code_tokenizers-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.2": {
        "info": {
            "author": "ncoop57",
            "author_email": "nacooper01@wm.edu",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description": "code_tokenizers\n================\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\nThis library is built on top of the awesome\n[transformers](https://github.com/huggingface/transformers) and\n[tree-sitter](https://github.com/tree-sitter/py-tree-sitter) libraries.\nIt provides a simple interface to align the tokens produced by a BPE\ntokenizer with the tokens produced by a tree-sitter parser.\n\n## Install\n\n``` sh\npip install code_tokenizers\n```\n\n## How to use\n\nFirst you need to make sure you have the tree-sitter grammars for the\nlanguages you want to use. To simplify this process, we\u2019ve built a CLI\ntool that will download the grammars for you that comes with this\nlibrary:\n\n``` python\n!download_grammars --help\n```\n\n    usage: download_grammars [-h] [--languages LANGUAGES [LANGUAGES ...]]\n\n    Download Tree-sitter grammars\n\n    options:\n      -h, --help                            show this help message and exit\n      --languages LANGUAGES [LANGUAGES ...]\n                                            Languages to download (default: all)\n\nThis will download the grammars to the `grammars` directory in the\ndirectory where this library is installed. Let\u2019s continue this example\nwith the Python grammar:\n\n``` python\n!download_grammars --languages python\n```\n\nNow, we can create a\n[`CodeTokenizer`](https://ncoop57.github.io/code_tokenizers/core.html#codetokenizer)\nobject:\n\n``` python\nfrom code_tokenizers.core import CodeTokenizer\n\npy_tokenizer = CodeTokenizer.from_pretrained(\"gpt2\", \"python\")\n```\n\n    /home/nathan/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n      from .autonotebook import tqdm as notebook_tqdm\n    None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n\nYou can specify any pretrained BPE tokenizer from the [huggingface\nhub](hf.co/models) or a local directory and the language to parse the\nAST for.\n\nNow, we can tokenize some code:\n\n``` python\nfrom pprint import pprint\n\ncode = \"\"\"\ndef foo():\n    print(\"Hello world!\")\n\"\"\"\n\nencoding = py_tokenizer(code)\npprint(encoding, depth=1)\n```\n\n    {'ast_ids': [...],\n     'attention_mask': [...],\n     'input_ids': [...],\n     'offset_mapping': [...],\n     'parent_ast_ids': [...]}\n\nAnd we can print out the associated AST types:\n\n<div>\n\n> **Note**\n>\n> Note: Here the N/As are the tokens that are not part of the AST, such\n> as the spaces and the newline characters. Their IDs are set to -1.\n\n</div>\n\n``` python\nfor ast_id, parent_ast_id in zip(encoding[\"ast_ids\"], encoding[\"parent_ast_ids\"]):\n    if ast_id != -1:\n        print(py_tokenizer.node_types[parent_ast_id], py_tokenizer.node_types[ast_id])\n    else:\n        print(\"N/A\")\n```\n\n    N/A\n    function_definition def\n    function_definition identifier\n    parameters (\n    N/A\n    N/A\n    N/A\n    N/A\n    call identifier\n    argument_list (\n    argument_list string\n    argument_list string\n    argument_list string\n    argument_list )\n    N/A\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/ncoop57/code_tokenizers",
            "keywords": "nbdev jupyter notebook python tokenizer bpe ast",
            "license": "Apache Software License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "code-tokenizers",
            "package_url": "https://pypi.org/project/code-tokenizers/",
            "platform": null,
            "project_url": "https://pypi.org/project/code-tokenizers/",
            "project_urls": {
                "Homepage": "https://github.com/ncoop57/code_tokenizers"
            },
            "release_url": "https://pypi.org/project/code-tokenizers/0.0.2/",
            "requires_dist": [
                "fastcore",
                "gitpython",
                "pandas",
                "transformers",
                "tree-sitter (==0.20.1)",
                "nbdev ; extra == 'dev'"
            ],
            "requires_python": ">=3.7",
            "summary": "Aligning BPE and AST",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15848971,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "162373d3ccd1a05e297c6400f29f9845",
                    "sha256": "6adfb8278aaf20c5b51e511dbb643730979f2bd01a12e42b6bb58c201cedc93e"
                },
                "downloads": -1,
                "filename": "code_tokenizers-0.0.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "162373d3ccd1a05e297c6400f29f9845",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 61581,
                "upload_time": "2022-11-22T03:02:13",
                "upload_time_iso_8601": "2022-11-22T03:02:13.698236Z",
                "url": "https://files.pythonhosted.org/packages/4f/99/399a79f08efb322c9002f9a0a09aba5816510b7e080b0288d69ea2f5a36e/code_tokenizers-0.0.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "dd5d725b742484638b225bec795c5e10",
                    "sha256": "bbb9696e31989a0470a73f72a1329ea0bf4adc822f87e00ece19bc917ff5996c"
                },
                "downloads": -1,
                "filename": "code_tokenizers-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "dd5d725b742484638b225bec795c5e10",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 55884,
                "upload_time": "2022-11-22T03:02:15",
                "upload_time_iso_8601": "2022-11-22T03:02:15.676795Z",
                "url": "https://files.pythonhosted.org/packages/36/ee/fa7a9cd214598e1cfaf642b9517b235a5250bf8f745193762cefe2dce275/code_tokenizers-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}