{
    "0.3.0": {
        "info": {
            "author": null,
            "author_email": "jestimator authors <jestimator@google.com>",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3 :: Only"
            ],
            "description": "# Amos and JEstimator\n\n*This is not an officially supported Google product.*\n\nThis is the source code for the paper \"[Amos: An Adam-style Optimizer with\nAdaptive Weight Decay towards Model-Oriented\nScale](https://arxiv.org/abs/2210.11693)\".\n\nIt implements **Amos**, an optimizer compatible with the\n[optax](https://github.com/deepmind/optax) library, and **JEstimator**, a\nlight-weight library with a `tf.Estimator`-like interface to manage\n[T5X](https://github.com/google-research/t5x)-compatible checkpoints for machine\nlearning programs in [Jax](https://github.com/google/jax), which we use to run\nexperiments in the paper.\n\n## Quick start\n\n```\npip install jestimator\n```\n\nIt will install the jestimator lib with the Amos optimizer. We can test the Amos\noptimizer by:\n\n```\npython3 -m jestimator.amos_test\npython3 -m jestimator.amos_helper_test\n```\n\n## Usage of Amos\n\nThis implementation of Amos is used with [Jax](https://github.com/google/jax), a\nhigh-performance numerical computing library with automatic differentiation, for\nmachine learning research. The API of Amos is compatible with\n[optax](https://github.com/deepmind/optax), a library of Jax optimizers\n(hopefully Amos will be integrated into optax in the near future).\n\nIn order to demonstrate the usage, we will apply Amos to MNIST. It is based on\nFlax's official\n[MNIST Example](https://github.com/google/flax/tree/main/examples/mnist), and\nyou can find the code in a jupyter notebook\n[here](https://github.com/google-research/jestimator/tree/main/jestimator/models/mnist).\n\n### 1. Imports\n\n```\nimport jax\nimport jax.numpy as jnp                # JAX NumPy\nfrom jestimator import amos            # The Amos optimizer implementation\nfrom jestimator import amos_helper     # Helper module for Amos\n\nfrom flax import linen as nn           # The Linen API\nfrom flax.training import train_state  # Useful dataclass to keep train state\n\nimport math\nimport tensorflow_datasets as tfds     # TFDS for MNIST\nfrom sklearn.metrics import accuracy_score\n```\n\n### 2. Load data\n\n```\ndef get_datasets():\n  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n\n  ds_builder = tfds.builder('mnist')\n  ds_builder.download_and_prepare()\n  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n  return train_ds, test_ds\n```\n\n### 3. Build model\n\n```\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x\n\n  def classify_xe_loss(self, x, labels):\n    # Labels read from the tfds MNIST are integers from 0 to 9.\n    # Logits are arrays of size 10.\n    logits = self(x)\n    logits = jax.nn.log_softmax(logits)\n    labels_ = jnp.expand_dims(labels, -1)\n    llh_ = jnp.take_along_axis(logits, labels_, axis=-1)\n    loss = -jnp.sum(llh_)\n    return loss\n```\n\n### 4. Create train state\n\nA `TrainState` object keeps the model parameters and optimizer states, and can\nbe checkpointed into files.\n\nWe create the model and optimizer in this function.\n\n**For the optimizer, we use Amos here.** The following hyper-parameters are set:\n\n * *learning_rate*:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The global learning rate.\n * *eta_fn*:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The model-specific 'eta'.\n * *shape_fn*:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Memory reduction setting.\n * *beta*:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Rate for running average of gradient squares.\n * *clip_value*:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Gradient clipping for stable training.\n\nThe global learning rate is usually set to the 1/sqrt(N), where N is the number\nof batches in the training data. For MNIST, we have 60k training examples and\nbatch size is 32. So learning_rate=1/sqrt(60000/32).\n\nThe model-specific 'eta_fn' requires a function that, given a variable name and\nshape, returns a float indicating the expected scale of that variable. Hopefully\nin the near future we will have libraries that can automatically calculate this\n'eta_fn' from the modeling code; but for now we have to specify it manually.\n\nOne can use the amos_helper.params_fn_from_assign_map() helper function to\ncreate 'eta_fn' from an assign_map. An assign_map is a dict which maps regex\nrules to a value or simple Python expressions. It will find the first regex rule\nwhich matches the name of a variable, and evaluate the Python expression if\nnecessary to return the value. See our example below.\n\nThe 'shape_fn' similarly requires a function that, given a variable name and\nshape, returns a reduced shape for the corresponding slot variables. We can use\nthe amos_helper.params_fn_from_assign_map() helper function to create 'shape_fn'\nfrom an assign_map as well.\n\n'beta' is the exponential decay rate for running average of gradient squares. We\nset it to 0.98 here.\n\n'clip_value' is the gradient clipping value, which should match the magnitude of\nthe loss function. If the loss function is a sum of cross-entropy, then we\nshould set 'clip_value' to the sqrt of the number of labels.\n\nPlease refer to our [paper](https://arxiv.org/abs/2210.11693) for more details\nof the hyper-parameters.\n\n```\ndef get_train_state(rng):\n  model = CNN()\n  dummy_x = jnp.ones([1, 28, 28, 1])\n  params = model.init(rng, dummy_x)\n\n  eta_fn = amos_helper.params_fn_from_assign_map(\n      {\n          '.*/bias': 0.5,\n          '.*Conv_0/kernel': 'sqrt(8/prod(SHAPE[:-1]))',\n          '.*Conv_1/kernel': 'sqrt(2/prod(SHAPE[:-1]))',\n          '.*Dense_0/kernel': 'sqrt(2/SHAPE[0])',\n          '.*Dense_1/kernel': 'sqrt(1/SHAPE[0])',\n      },\n      eval_str_value=True,\n  )\n  shape_fn = amos_helper.params_fn_from_assign_map(\n      {\n          '.*Conv_[01]/kernel': '(1, 1, 1, SHAPE[-1])',\n          '.*Dense_0/kernel': '(1, SHAPE[1])',\n          '.*': (),\n      },\n      eval_str_value=True,\n  )\n  optimizer = amos.amos(\n      learning_rate=1/math.sqrt(60000/32),\n      eta_fn=eta_fn,\n      shape_fn=shape_fn,\n      beta=0.98,\n      clip_value=math.sqrt(32),\n  )\n  return train_state.TrainState.create(\n      apply_fn=model.apply, params=params, tx=optimizer)\n```\n\n### 5. Train step\n\nUse JAX\u2019s @jit decorator to just-in-time compile the function for better\nperformance.\n\n```\n@jax.jit\ndef train_step(batch, state):\n  grad_fn = jax.grad(state.apply_fn)\n  grads = grad_fn(\n      state.params,\n      batch['image'],\n      batch['label'],\n      method=CNN.classify_xe_loss)\n  return state.apply_gradients(grads=grads)\n```\n\n### 6. Infer step\n\nUse JAX\u2019s @jit decorator to just-in-time compile the function for better\nperformance.\n\n```\n@jax.jit\ndef infer_step(batch, state):\n  logits = state.apply_fn(state.params, batch['image'])\n  return jnp.argmax(logits, -1)\n```\n\n### 7. Main\n\nRun the training loop and evaluate on test set.\n\n```\ntrain_ds, test_ds = get_datasets()\n\nrng = jax.random.PRNGKey(0)\nrng, init_rng = jax.random.split(rng)\nstate = get_train_state(init_rng)\ndel init_rng  # Must not be used anymore.\n\nnum_epochs = 9\nfor epoch in range(1, num_epochs + 1):\n  # Use a separate PRNG key to permute image data during shuffling\n  rng, input_rng = jax.random.split(rng)\n  perms = jax.random.permutation(input_rng, 60000)\n  del input_rng\n  perms = perms.reshape((60000 // 32, 32))\n  for perm in perms:\n    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n    state = train_step(batch, state)\n\n  pred = jax.device_get(infer_step(test_ds, state))\n  accuracy = accuracy_score(test_ds['label'], pred)\n  print('epoch: %d, test accuracy: %.2f' % (epoch, accuracy * 100))\n```\n\nAfter 9 epochs, we should get 99.26 test accuracy. If you made it, congrats!\n\n## JEstimator\n\nWith JEstimator, you can build your model mostly similar to the MNIST example\nabove, but without writing code for the \"Main\" section; JEstimator will serve as\nthe entry point for your model, automatically handle checkpointing in a\ntrain/eval-once/eval-while-training-and-save-the-best/predict mode, and set up\nprofiling, tensorboard, and logging.\n\nIn addition, JEstimator supports model partitioning which is required for\ntraining very large models across multiple TPU pods. It supports a\n[T5X](https://github.com/google-research/t5x)-compatible checkpoint format that\nsaves and restores checkpoints in a distributed manner, which is suitable for\nlarge multi-pod models.\n\nClone this repo to get JEstimator models:\n\n```\ngit clone --branch=main https://github.com/google-research/jestimator\n```\n\n## Run models with JEstimator\n\nWe need [T5X](https://github.com/google-research/t5x#installation) and\n[FlaxFormer](https://github.com/google/flaxformer) for running the models.\n\n```\ngit clone --branch=main https://github.com/google-research/t5x\ncd t5x  # Install T5X with TPU support, so we can pre-train on Google Cloud:\npython3 -m pip install -e '.[tpu]' -f \\\n  https://storage.googleapis.com/jax-releases/libtpu_releases.html\ncd ..\n\ngit clone --branch=main https://github.com/google/flaxformer\ncd flaxformer  # Install FlaxFormer:\npip3 install '.[testing]'\ncd ..\n```\n\nThen, we can test a toy linear regression model with JEstimator:\n\n```\nJAX_PLATFORMS=cpu PYTHONPATH=. python3 jestimator/models/linear_regression/linear_regression_test.py\n```\n\nAnd we can train a single layer LSTM model on PTB:\n\n```\nJAX_PLATFORMS=cpu PYTHONPATH=. python3 jestimator/estimator.py \\\n  --module_imp=\"jestimator.models.lstm.lm\" \\\n  --module_config=\"jestimator/models/lstm/lm.py\" \\\n  --module_config.vocab_path=\"jestimator/models/lstm/ptb/vocab.txt\" \\\n  --train_pattern=\"jestimator/models/lstm/ptb/ptb.train.txt\" \\\n  --model_dir=\"$HOME/models/ptb_lstm\" \\\n  --train_batch_size=64 \\\n  --train_consecutive=113 \\\n  --train_shuffle_buf=4096 \\\n  --max_train_steps=200000 \\\n  --check_every_steps=1000 \\\n  --max_ckpt=20 \\\n  --module_config.opt_config.optimizer=\"amos\" \\\n  --module_config.opt_config.learning_rate=0.01 \\\n  --module_config.opt_config.beta=0.98 \\\n  --module_config.opt_config.momentum=0.0 \\\n  --logtostderr\n```\n\nAfter the training completes, we can evaluate the model on validation set:\n\n```\nJAX_PLATFORMS=cpu PYTHONPATH=. python3 jestimator/estimator.py \\\n  --module_imp=\"jestimator.models.lstm.lm\" \\\n  --module_config=\"jestimator/models/lstm/lm.py\" \\\n  --module_config.vocab_path=\"jestimator/models/lstm/ptb/vocab.txt\" \\\n  --eval_pattern=\"jestimator/models/lstm/ptb/ptb.valid.txt\" \\\n  --model_dir=\"$HOME/models/ptb_lstm\" \\\n  --eval_batch_size=1 \\\n  --logtostderr\n```\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": null,
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": null,
            "keywords": null,
            "license": null,
            "maintainer": null,
            "maintainer_email": null,
            "name": "jestimator",
            "package_url": "https://pypi.org/project/jestimator/",
            "platform": null,
            "project_url": "https://pypi.org/project/jestimator/",
            "project_urls": {
                "homepage": "https://github.com/google-research/jestimator",
                "repository": "https://github.com/google-research/jestimator"
            },
            "release_url": "https://pypi.org/project/jestimator/0.3.0/",
            "requires_dist": [
                "tensorflow-cpu",
                "jax",
                "flax",
                "pytest ; extra == \"dev\"",
                "pytest-xdist ; extra == \"dev\"",
                "pylint>=2.6.0 ; extra == \"dev\"",
                "yapf ; extra == \"dev\""
            ],
            "requires_python": ">=3.7",
            "summary": "Estimator-like checkpointing in Jax, with the Amos optimizer.",
            "version": "0.3.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15862527,
        "urls": [
            {
                "comment_text": null,
                "digests": {
                    "md5": "a8c62505560eb95dfb99ba2bbf962fbc",
                    "sha256": "de32277c84fd5800d388632183582593029558e3a230b1235a85dad00777e4f0"
                },
                "downloads": -1,
                "filename": "jestimator-0.3.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "a8c62505560eb95dfb99ba2bbf962fbc",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 40805,
                "upload_time": "2022-11-23T02:46:40",
                "upload_time_iso_8601": "2022-11-23T02:46:40.937019Z",
                "url": "https://files.pythonhosted.org/packages/8e/13/5ad212e691aafb99187dde0462dabc7c5369f9fd45b17d43760002828de2/jestimator-0.3.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": null,
                "digests": {
                    "md5": "03b03035d3b204baaf3ed229a1d4a14e",
                    "sha256": "08098c2525d10cd852b06ff5e43b35d17d8ac84e3f6ac1e1d688aa4b86996b8d"
                },
                "downloads": -1,
                "filename": "jestimator-0.3.0.tar.gz",
                "has_sig": false,
                "md5_digest": "03b03035d3b204baaf3ed229a1d4a14e",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 33496,
                "upload_time": "2022-11-23T02:46:48",
                "upload_time_iso_8601": "2022-11-23T02:46:48.077462Z",
                "url": "https://files.pythonhosted.org/packages/48/19/a0933fa0117232a4453beba580268039d836fc329e116488f1e9af2eae6e/jestimator-0.3.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}