{
    "1.0.1": {
        "info": {
            "author": "Mehdi Cherti",
            "author_email": "mehdicherti@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 2 - Pre-Alpha",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mehdidc/clip_benchmark",
            "keywords": "clip_benchmark",
            "license": "MIT license",
            "maintainer": "",
            "maintainer_email": "",
            "name": "clip-benchmark",
            "package_url": "https://pypi.org/project/clip-benchmark/",
            "platform": null,
            "project_url": "https://pypi.org/project/clip-benchmark/",
            "project_urls": {
                "Homepage": "https://github.com/mehdidc/clip_benchmark"
            },
            "release_url": "https://pypi.org/project/clip-benchmark/1.0.1/",
            "requires_dist": [
                "torch (>=1.8.1<2)",
                "torchvision (>=0.8.9<2)",
                "tqdm (>=4.62.3<2)",
                "scikit-learn (>=1.0<2)",
                "open-clip-torch (>=0.2.1)"
            ],
            "requires_python": ">=3.6",
            "summary": "CLIP-like models benchmarks on various datasets",
            "version": "1.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16090966,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "9f21c83f403a4102a0473d12a9678162",
                    "sha256": "34c263699ab3e9a2c57cd961eb3f4cf6de264860e86186076ba4cca6c20a29d3"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.0.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "9f21c83f403a4102a0473d12a9678162",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=3.6",
                "size": 67031,
                "upload_time": "2022-11-17T21:19:49",
                "upload_time_iso_8601": "2022-11-17T21:19:49.685969Z",
                "url": "https://files.pythonhosted.org/packages/56/57/8325bbd0b139f4e5aab54f61c555a837c87cd81c82c917c2f6590572b092/clip_benchmark-1.0.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "19b1cb6b92ae7c902d95b3b437e00b27",
                    "sha256": "cbe00bb6d4ccc36b777b56bbfd04edf2ae7f0494d9a33be8ba99d2d1d021d796"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "19b1cb6b92ae7c902d95b3b437e00b27",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 66282,
                "upload_time": "2022-11-17T21:19:51",
                "upload_time_iso_8601": "2022-11-17T21:19:51.379320Z",
                "url": "https://files.pythonhosted.org/packages/52/64/618506de24693974c32e7532e14a49946c51da32a8a953ecf28912df4cfe/clip_benchmark-1.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.1.0": {
        "info": {
            "author": "Mehdi Cherti",
            "author_email": "mehdicherti@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 2 - Pre-Alpha",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mehdidc/clip_benchmark",
            "keywords": "clip_benchmark",
            "license": "MIT license",
            "maintainer": "",
            "maintainer_email": "",
            "name": "clip-benchmark",
            "package_url": "https://pypi.org/project/clip-benchmark/",
            "platform": null,
            "project_url": "https://pypi.org/project/clip-benchmark/",
            "project_urls": {
                "Homepage": "https://github.com/mehdidc/clip_benchmark"
            },
            "release_url": "https://pypi.org/project/clip-benchmark/1.1.0/",
            "requires_dist": [
                "torch (>=1.8.1<2)",
                "torchvision (>=0.8.9<2)",
                "tqdm (>=4.62.3<2)",
                "scikit-learn (>=1.0<2)",
                "open-clip-torch (>=0.2.1)"
            ],
            "requires_python": ">=3.6",
            "summary": "CLIP-like models benchmarks on various datasets",
            "version": "1.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16090966,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "5a8744fe529416af17ead7511f400917",
                    "sha256": "0ffb60c09ae4c19bf47574cd967f7db6c7533425abdaec20041b4797f50d9873"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.1.0-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "5a8744fe529416af17ead7511f400917",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=3.6",
                "size": 881159,
                "upload_time": "2022-11-23T20:09:02",
                "upload_time_iso_8601": "2022-11-23T20:09:02.905000Z",
                "url": "https://files.pythonhosted.org/packages/cd/b0/8ec94682eacede24bc22470051942a8ebe7e77e3c65c59143955a73f89c3/clip_benchmark-1.1.0-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "b890d522d4632269133f1a63821620a7",
                    "sha256": "cda53e873481bc989e8688fd3a45d18cef3d7ab6925cd6ea93f83b723bd0e702"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "b890d522d4632269133f1a63821620a7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 1004138,
                "upload_time": "2022-11-23T20:09:04",
                "upload_time_iso_8601": "2022-11-23T20:09:04.924199Z",
                "url": "https://files.pythonhosted.org/packages/da/8a/c1e1e8db6df415338edd6f04d190be1c609a7a51d297ebaa06379c70f176/clip_benchmark-1.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.0": {
        "info": {
            "author": "Mehdi Cherti",
            "author_email": "mehdicherti@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 2 - Pre-Alpha",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8"
            ],
            "description": "# CLIP Benchmark\n\n\nThe goal of this repo is to evaluate CLIP-like models on a standard set\nof datasets on different tasks such as zero-shot classification and zero-shot\nretrieval.\n\nBelow we show the average rank (1 is the best, lower is better) of different CLIP models, evaluated\non different datasets.\n\n![benchmark.png](benchmark.png)\n\nThe current detailed results of the benchmark can be seen [here](benchmark/README.md)\nor directly in the [notebook](benchmark/results.ipynb).\n\n## Features\n\n* Support for zero-shot classification and zero-shot retrieval\n* Support for [OpenCLIP](https://github.com/mlfoundations/open_clip) pre-trained models\n* Support various datasets from [torchvision](https://pytorch.org/vision/stable/datasets.html), [tensorflow datasets](https://www.tensorflow.org/datasets), and [VTAB](https://github.com/google-research/task_adaptation), and datasets in [webdataset](https://github.com/webdataset/webdataset) format.\n\n\n## How to install?\n\n`pip install clip-benchmark`\n\nFor development, you can also do this:\n\n```bash\ngit clone https://github.com/LAION-AI/CLIP_benchmark\ncd CLIP_benchmark\npython setup.py install\n```\n\n## How to use?\n\n### Command line interface (CLI)\n\nThe easiest way to benchmark the models is using the CLI, `clip_benchmark`.\nYou can specify the model to use, the dataset and the task to evaluate on. Once it is done, evaluation is performed and\nthe results are written into a JSON file.\n\n### CIFAR-10 example\n\n Here is an example for CIFAR-10 zero-shot classification using OpenCLIP's pre-trained model on LAION-400m:\n\n `clip_benchmark --dataset=cifar10 --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\nHere is the content of `result.json` after the evaluation is done:\n\n```json\n{\n    \"dataset\": \"cifar10\", \"model\": \"ViT-B-32-quickgelu\", \n    \"pretrained\": \"laion400m_e32\", \"task\": \"zeroshot_classification\",\n    \"metrics\": {\"acc1\": 0.9074, \"acc5\": 0.998}\n}\n```\n\n### VOC2007 example\n\nHere is another example with VOC2007, which is a multi-label classification dataset.\n\n `clip_benchmark --dataset=voc2007_multilabel --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\nHere is the content of `result.json` after the evaluation is done:\n\n```json\n{\"dataset\": \"voc2007_multilabel\", \"model\": \"ViT-B-32-quickgelu\", \"pretrained\": \"laion400m_e32\", \"task\": \"zeroshot_classification\", \"metrics\": {\"mean_average_precision\": 0.7627869844436646}}\n```\n\nHere, we compute the mean average precision or mAP, more details about that metric [here](https://fangdahan.medium.com/calculate-mean-average-precision-map-for-multi-label-classification-b082679d31be) in the context of multi-label classification.\n\n### VTAB example\n\nHere is an example on how to run it on [VTAB](https://github.com/google-research/task_adaptation) classification tasks.\nFirst, you need to install VTAB's dedicated package.\n\n`pip install task_adaptation==0.1`\n\nThe name of the dataset follows the template `vtab/<TASK_NAME>`.\nTo have the list of the 19 classification tasks using in VTAB, you can use:\n\n`python -c 'from clip_benchmark.datasets.builder import VTAB_19TASKS;print(\"\\n\".join(VTAB_19TASKS))'`\n\n\nThen, you can run it by providing the full dataset name.\nExample with `eurosat`:\n\n `clip_benchmark --dataset=vtab/eurosat --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\n\n### TensorFlow dataset example\n\n\n\nHere is an example on how to run it on [Tensorflow datasets](https://www.tensorflow.org/datasets).\nFirst, you need to install `tfds-nightly` and `timm`.\n\n`pip install timm tfds-nightly`\n\n\nThe name of the dataset follows the template `tfds/<DATASET_NAME>`.\n\nExample with `cifar10`:\n\n `clip_benchmark --dataset=tfds/cifar10 --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\n\n### COCO captions example\n\n Here is an example for COCO captions zero-shot retrieval:\n\n `clip_benchmark --dataset=mscoco_captions --task=zeroshot_retrieval --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --dataset_root=<PATH_TO_IMAGE_FOLDER> --annotation_file=<PATH_TO_ANNOTATION_FILE> --batch_size=64` \n\n (see <https://cocodataset.org/#home> for instructions on how to download)\n\n Note that for using COCO, you also need to install `pycocotools`, using:\n\n `pip install pycocotools`\n\n ### Webdataset example\n\nHere is an example on how to run it on [webdatasets](https://github.com/webdataset/webdataset).\nFirst, you need to install `webdataset`.\n\n`pip install webdataset`\n\n#### Creating a webdataset\n\nYou can either convert an already supported CLIP_benchmark dataset to webdataset format, or manually create your own with the same file structure. For already supported datasets use the CLI command `clip_benchmark_export_wds` as in this example:\n\n```\n$ clip_benchmark_export_wds --dataset cifar10 --split train --dataset_root DATA_DIR/ --output wds_cifar10/\n$ clip_benchmark_export_wds --dataset cifar10 --split test --dataset_root DATA_DIR/ --output wds_cifar10/\n```\n\nwhich will convert the train and test splits for CIFAR-10 (downloaded to `DATA_DIR/`) and save the webdataset to `wds_cifar10/` (upload to Huggingface Hub must be done manually for now).\n\nFor other datasets, data must be stored with the following file structure:\n\n```\nroot_dir/\n    train/\n        nshards.txt\n        0.tar\n        1.tar\n        ...\n    test/\n        nshards.txt\n        0.tar\n        ...\n    classnames.txt\n    zeroshot_classification_templates.txt\n```\n\nEach split should be contained in its own folder and `nshards.txt` should contain a single integer corresponding to the number of TAR files. The TAR files should follow webdataset format, with an image file (.webp, .png, or .jpg) and a label (.cls) for each example. Classnames and templates are required for zeroshot classification evaluation, with each classname or template on its own line.\n\n#### Evaluating on a webdataset\n\nThe name of the dataset follows the template `wds/<DATASET_NAME>`. Note that the dataset name currently only affects the name in the results output - classnames and templates are loaded directly from the included files. The dataset root directory can be either a local path to the `root_dir` as specified above, or an HTTP URL pointing to a Huggingface Hub dataset file tree.\n\nExample with `cifar10`:\n\n```\n$ clip_benchmark --dataset wds/cifar10 --dataset_root ROOT_DIR/wds_cifar10/\n$ clip_benchmark --dataset wds/cifar10 --dataset_root https://huggingface.co/datasets/djghosh/wds_cifar10_test/tree/main\n```\n\nAll other arguments remain the same as in the other examples.\n\n### API\n\nYou can also use the API directly. This is especially useful if your model\ndoes not belong to currently supported models.\n(TODO)\n\n## Credits\n\n- Thanks to [OpenCLIP](https://github.com/mlfoundations/open_clip) authors, zero-shot accuracy code is adapted from there and pre-trained models are used in the command line interface.\n- Thanks to [SLIP](https://github.com/facebookresearch/SLIP) authors, some zero-shot templates and classnames are from there.\n- Thanks to [Wise-ft](https://github.com/mlfoundations/wise-ft) authors, Imagenet robustness datasets code is adapted from there\n- Thanks to [LiT](https://arxiv.org/abs/2111.07991.pdf) authors, some zero-shot templates and classnames of VTAB datasets are from there.\n- This package was created with [Cookiecutter]( https://github.com/audreyr/cookiecutter) and the [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template. Thanks to the author.\n\n\n## History\n\n### 1.2.0\n\n* Added support for loading webdatasets\n\n### 1.1.0\n\n* Added better support for multilingual eval\n* Added better support for linear probing\n* Added support for CuPL prompts\n\n### 1.0.1\n\n* pypi description as markdown\n\n### 1.0.0\n\n* Actual first release on PyPI.\n\n\n### 0.1.0\n\n* First release on PyPI.\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mehdidc/clip_benchmark",
            "keywords": "clip_benchmark",
            "license": "MIT license",
            "maintainer": "",
            "maintainer_email": "",
            "name": "clip-benchmark",
            "package_url": "https://pypi.org/project/clip-benchmark/",
            "platform": null,
            "project_url": "https://pypi.org/project/clip-benchmark/",
            "project_urls": {
                "Homepage": "https://github.com/mehdidc/clip_benchmark"
            },
            "release_url": "https://pypi.org/project/clip-benchmark/1.2.0/",
            "requires_dist": [
                "torch (>=1.8.1<2)",
                "torchvision (>=0.8.9<2)",
                "tqdm (>=4.62.3<2)",
                "scikit-learn (>=1.0<2)",
                "open-clip-torch (>=0.2.1)"
            ],
            "requires_python": ">=3.6",
            "summary": "CLIP-like models benchmarks on various datasets",
            "version": "1.2.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16090966,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4b54f739a9f55c8fe185bc0c59998d22",
                    "sha256": "c3219adb1470ea59b5e3d5692c4c35adde9983d4df3cb453f33493fd91e51311"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.2.0-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "4b54f739a9f55c8fe185bc0c59998d22",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=3.6",
                "size": 885146,
                "upload_time": "2022-12-13T20:00:52",
                "upload_time_iso_8601": "2022-12-13T20:00:52.089826Z",
                "url": "https://files.pythonhosted.org/packages/6d/3d/094f29a4a284b372765d1322b9d2c7ae9e6abceb2c84e15a78df9bd7cf07/clip_benchmark-1.2.0-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "4cdeef6e0d10ed05face878b208337a1",
                    "sha256": "78d58d56d8a9816f5d30198d7df9870f7dadf8d86a5b9a653a86424cd4202e63"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.2.0.tar.gz",
                "has_sig": false,
                "md5_digest": "4cdeef6e0d10ed05face878b208337a1",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 1009390,
                "upload_time": "2022-12-13T20:00:53",
                "upload_time_iso_8601": "2022-12-13T20:00:53.659592Z",
                "url": "https://files.pythonhosted.org/packages/18/4f/17a79373b44a7f6437b33bb99dd45a9e53277c3fe629220d1140ca03fbd4/clip_benchmark-1.2.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}