{
    "0.0.1": {
        "info": {
            "author": "Saurav Maheshkar",
            "author_email": "sauravvmaheshkar@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# Flax implementation of gMLP from \"Pay Attention to MLPs\"\n\n![](https://github.com/SauravMaheshkar/gMLP/blob/main/assets/gMLP%20Banner.png?raw=true)\n\nIt's no news that transformers have dominated the field of deep learning ever since 2017. But, Hanxiao Liu, Zihang Dai, David R. So and Quoc V. Le in their recent work titled [\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050) propose a new architecture **gMLP** (essentially MLPs with gating) that performs as well as Transformers in key language and vision applications. Based on the comparisons showen in the paper the authors show that self-attention is **not** critical for Vision Transformers !!, as gMLP can achieve the same accuracy, thus bringing into question the validity of Attention.\n\nThis repository includes an implementation of gMLP written in [Flax](https://github.com/google/flax). Most of the codebase is inspired from [Phil Wang](https://github.com/lucidrains)'s implementations in [Pytorch](https://github.com/lucidrains/g-mlp-pytorch) and [Haiku](https://github.com/lucidrains/mlp-gpt-jax).\n\n**NOTE: Causal Nature of Spatial Gating Unit hasn't been implemented yet**\n\n## Usage\n\n```python\nimport jax\nfrom gmlp_flax import gMLP\n\nrandom_key = jax.random.PRNGKey(0)\n\nx = jax.random.randint(key=random_key, minval=0, maxval=20000, shape=(1, 1000))\n\ninit_rngs = {\"params\": random_key}\n\ngMLP(num_tokens=20000, dim=512, depth=4).init(init_rngs,x)\n```\n\n## Development\n\n### 1. Conda Approach\n\n```bash\nconda env create --name <env-name> sauravmaheshkar/gmlp\nconda activate <env-name>\n```\n\n## Citations\n\n```bibtex\n@misc{liu2021pay,\n    title   = {Pay Attention to MLPs},\n    author  = {Hanxiao Liu and Zihang Dai and David R. So and Quoc V. Le},\n    year    = {2021},\n    eprint  = {2105.08050},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/SauravMaheshkar/gMLP",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "gmlp-flax",
            "package_url": "https://pypi.org/project/gmlp-flax/",
            "platform": "",
            "project_url": "https://pypi.org/project/gmlp-flax/",
            "project_urls": {
                "Homepage": "https://github.com/SauravMaheshkar/gMLP"
            },
            "release_url": "https://pypi.org/project/gmlp-flax/0.0.1/",
            "requires_dist": [
                "einops (>=0.3)",
                "flax",
                "jax"
            ],
            "requires_python": "",
            "summary": "Flax implementation of gMLP from Pay Attention to MLPs",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 10772389,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "b9c759f2a1b347510decaad2477f0572",
                    "sha256": "7c2139446c2e929214569eaad7ceb4e52d3da9c26d90cd8e03d0dbb88ee09583"
                },
                "downloads": -1,
                "filename": "gmlp_flax-0.0.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "b9c759f2a1b347510decaad2477f0572",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": null,
                "size": 5755,
                "upload_time": "2021-06-29T04:48:58",
                "upload_time_iso_8601": "2021-06-29T04:48:58.285460Z",
                "url": "https://files.pythonhosted.org/packages/6b/91/47663495b01c6d63d7ca10799ca4834e713af8e9210ff667713f73dc8a98/gmlp_flax-0.0.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "693699a0608b70e7654decfb8800932f",
                    "sha256": "ce1ffb7030361807d342d64456a399df19c59136a6e2e2495fe5c31b22311c04"
                },
                "downloads": -1,
                "filename": "gmlp-flax-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "693699a0608b70e7654decfb8800932f",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 5061,
                "upload_time": "2021-06-29T04:49:00",
                "upload_time_iso_8601": "2021-06-29T04:49:00.150716Z",
                "url": "https://files.pythonhosted.org/packages/81/4c/df3f55624b23ae6c8fef6036bd4d313a9355ab248ee3809139ec378e49f8/gmlp-flax-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}