{
    "0.0.0": {
        "info": {
            "author": "Naren Dasan",
            "author_email": "narens@nvidia.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/nvidia/trtorch",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "torch-tensorrt",
            "package_url": "https://pypi.org/project/torch-tensorrt/",
            "platform": "",
            "project_url": "https://pypi.org/project/torch-tensorrt/",
            "project_urls": {
                "Homepage": "https://github.com/nvidia/trtorch"
            },
            "release_url": "https://pypi.org/project/torch-tensorrt/0.0.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Placeholder for upcoming torch-tensorrt package",
            "version": "0.0.0",
            "yanked": true,
            "yanked_reason": "Non-functional release, please install torch-tensorrt  using pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases"
        },
        "last_serial": 12989373,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "3849daa8f885f65914e3a8dd4281d206",
                    "sha256": "27f6d4a1f54fa5c375371e6c9a417024a5a3f262dba6b07a1bd35aa28f1774d0"
                },
                "downloads": -1,
                "filename": "torch_tensorrt-0.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "3849daa8f885f65914e3a8dd4281d206",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 762,
                "upload_time": "2021-08-05T00:06:17",
                "upload_time_iso_8601": "2021-08-05T00:06:17.222928Z",
                "url": "https://files.pythonhosted.org/packages/fb/57/b47757fb16eb52a4364be119864ee0f8c134e1a784e5349b73bd621b1a14/torch_tensorrt-0.0.0.tar.gz",
                "yanked": true,
                "yanked_reason": "Non-functional release, please install torch-tensorrt  using pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases"
            }
        ],
        "vulnerabilities": []
    },
    "0.0.0.post1": {
        "info": {
            "author": "Naren Dasan",
            "author_email": "narens@nvidia.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Environment :: GPU :: NVIDIA CUDA",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: BSD License",
                "Operating System :: POSIX :: Linux",
                "Programming Language :: C++",
                "Programming Language :: Python",
                "Programming Language :: Python :: Implementation :: CPython",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries"
            ],
            "description": "Torch-TensorRT\n==============\n\n**WARNING:** The package uploaded to PYPI is not functional, to install\nplease run the following command\n\n.. code-block:: bash\n\n   pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n\ntorch_tensorrt\n==============\n\n   Ahead of Time (AOT) compiling for PyTorch JIT\n\nTorch-TensorRT is a compiler for PyTorch/TorchScript, targeting NVIDIA\nGPUs via NVIDIA\u2019s TensorRT Deep Learning Optimizer and Runtime. Unlike\nPyTorch\u2019s Just-In-Time (JIT) compiler, Torch-TensorRT is an\nAhead-of-Time (AOT) compiler, meaning that before you deploy your\nTorchScript code, you go through an explicit compile step to convert a\nstandard TorchScript program into an module targeting a TensorRT engine.\nTorch-TensorRT operates as a PyTorch extention and compiles modules that\nintegrate into the JIT runtime seamlessly. After compilation using the\noptimized graph should feel no different than running a TorchScript\nmodule. You also have access to TensorRT\u2019s suite of configurations at\ncompile time, so you are able to specify operating precision\n(FP32/FP16/INT8) and other settings for your module.\n\nExample Usage\n-------------\n\n.. code-block:: python\n\n   import torch_tensorrt\n\n   ...\n\n   trt_ts_module = torch_tensorrt.compile(torch_script_module,\n       inputs = [example_tensor, # Provide example tensor for input shape or...\n           torch_tensorrt.Input( # Specify input object with shape and dtype\n               min_shape=[1, 3, 224, 224],\n               opt_shape=[1, 3, 512, 512],\n               max_shape=[1, 3, 1024, 1024],\n               # For static size shape=[1, 3, 224, 224]\n               dtype=torch.half) # Datatype of input tensor. Allowed options torch.(float|half|int8|int32|bool)\n       ],\n       enabled_precisions = {torch.half}, # Run with FP16)\n\n   result = trt_ts_module(input_data) # run inference\n   torch.jit.save(trt_ts_module, \"trt_torchscript_module.ts\") # save the TRT embedded Torchscript\n\nBuilding from source\n--------------------\n\n+---------------------------+------------------------------------------+\n| ABI / Platform            | Installation command                     |\n+===========================+==========================================+\n| Pre CXX11 ABI (Linux      | python3 setup.py install                 |\n| x86_64)                   |                                          |\n+---------------------------+------------------------------------------+\n| CXX ABI (Linux x86_64)    | python3 setup.py install \u2013use-cxx11-abi  |\n+---------------------------+------------------------------------------+\n| Pre CXX11 ABI (Jetson     | python3 setup.py install                 |\n| platform aarch64)         | \u2013jetpack-version 4.6                     |\n+---------------------------+------------------------------------------+\n| CXX11 ABI (Jetson         | python3 setup.py install                 |\n| platform aarch64)         | \u2013jetpack-version 4.6 \u2013use-cxx11-abi      |\n+---------------------------+------------------------------------------+\n\nFor Linux x86_64 platform, Pytorch libraries default to pre cxx11 abi.\nSo, please use ``python3 setup.py install``.\n\nOn Jetson platforms, NVIDIA hosts pre-built Pytorch wheel files. These\nwheel files are built with CXX11 ABI. So on jetson platforms, please use\n``python3 setup.py install --jetpack-version 4.6 --use-cxx11-abi``\n\nUnder the Hood\n--------------\n\nWhen a traced module is provided to Torch-TensorRT, the compiler takes\nthe internal representation and transforms it into one like this:\n\n.. code-block::\n\n   graph(%input.2 : Tensor):\n       %2 : Float(84, 10) = prim::Constant[value=<Tensor>]()\n       %3 : Float(120, 84) = prim::Constant[value=<Tensor>]()\n       %4 : Float(576, 120) = prim::Constant[value=<Tensor>]()\n       %5 : int = prim::Constant[value=-1]() # x.py:25:0\n       %6 : int[] = prim::Constant[value=annotate(List[int], [])]()\n       %7 : int[] = prim::Constant[value=[2, 2]]()\n       %8 : int[] = prim::Constant[value=[0, 0]]()\n       %9 : int[] = prim::Constant[value=[1, 1]]()\n       %10 : bool = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\n       %11 : int = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n       %12 : bool = prim::Constant[value=0]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n       %self.classifer.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\n       %self.classifer.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\n       %self.classifer.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\n       %self.feat.conv2.weight : Float(16, 6, 3, 3) = prim::Constant[value=<Tensor>]()\n       %self.feat.conv2.bias : Float(16) = prim::Constant[value=<Tensor>]()\n       %self.feat.conv1.weight : Float(6, 1, 3, 3) = prim::Constant[value=<Tensor>]()\n       %self.feat.conv1.bias : Float(6) = prim::Constant[value= 0.0530 -0.1691  0.2802  0.1502  0.1056 -0.1549 [ CUDAFloatType{6} ]]()\n       %input0.4 : Tensor = aten::_convolution(%input.2, %self.feat.conv1.weight, %self.feat.conv1.bias, %9, %8, %9, %12, %8, %11, %12, %12, %10) # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\n       %input0.5 : Tensor = aten::relu(%input0.4) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n       %input1.2 : Tensor = aten::max_pool2d(%input0.5, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n       %input0.6 : Tensor = aten::_convolution(%input1.2, %self.feat.conv2.weight, %self.feat.conv2.bias, %9, %8, %9, %12, %8, %11, %12, %12, %10) # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\n       %input2.1 : Tensor = aten::relu(%input0.6) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n       %x.1 : Tensor = aten::max_pool2d(%input2.1, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n       %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0\n       %27 : Tensor = aten::matmul(%input.1, %4)\n       %28 : Tensor = trt::const(%self.classifer.fc1.bias)\n       %29 : Tensor = aten::add_(%28, %27, %11)\n       %input0.2 : Tensor = aten::relu(%29) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n       %31 : Tensor = aten::matmul(%input0.2, %3)\n       %32 : Tensor = trt::const(%self.classifer.fc2.bias)\n       %33 : Tensor = aten::add_(%32, %31, %11)\n       %input1.1 : Tensor = aten::relu(%33) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n       %35 : Tensor = aten::matmul(%input1.1, %2)\n       %36 : Tensor = trt::const(%self.classifer.fc3.bias)\n       %37 : Tensor = aten::add_(%36, %35, %11)\n       return (%37)\n   (CompileGraph)\n\nThe graph has now been transformed from a collection of modules much\nlike how your PyTorch Modules are collections of modules, each managing\ntheir own parameters into a single graph with the parameters inlined\ninto the graph and all of the operations laid out. Torch-TensorRT has\nalso executed a number of optimizations and mappings to make the graph\neasier to translate to TensorRT. From here the compiler can assemble the\nTensorRT engine by following the dataflow through the graph.\n\nWhen the graph construction phase is complete, Torch-TensorRT produces a\nserialized TensorRT engine. From here depending on the API, this engine\nis returned to the user or moves into the graph construction phase. Here\nTorch-TensorRT creates a JIT Module to execute the TensorRT engine which\nwill be instantiated and managed by the Torch-TensorRT runtime.\n\nHere is the graph that you get back after compilation is complete:\n\n.. code-block::\n\n   graph(%self.1 : __torch__.___torch_mangle_10.LeNet_trt,\n       %2 : Tensor):\n       %1 : int = prim::Constant[value=94106001690080]()\n       %3 : Tensor = trt::execute_engine(%1, %2)\n       return (%3)\n   (AddEngineToGraph)\n\nYou can see the call where the engine is executed, based on a constant\nwhich is the ID of the engine, telling JIT how to find the engine and\nthe input tensor which will be fed to TensorRT. The engine represents\nthe exact same calculations as what is done by running a normal PyTorch\nmodule but optimized to run on your GPU.\n\nTorch-TensorRT converts from TorchScript by generating layers or\nsubgraphs in correspondance with instructions seen in the graph.\nConverters are small modules of code used to map one specific operation\nto a layer or subgraph in TensorRT. Not all operations are support, but\nif you need to implement one, you can in C++.\n\nRegistering Custom Converters\n-----------------------------\n\nOperations are mapped to TensorRT through the use of modular converters,\na function that takes a node from a the JIT graph and produces an\nequivalent layer or subgraph in TensorRT. Torch-TensorRT ships with a\nlibrary of these converters stored in a registry, that will be executed\ndepending on the node being parsed. For instance a\n``aten::relu(%input0.4)`` instruction will trigger the relu converter to\nbe run on it, producing an activation layer in the TensorRT graph. But\nsince this library is not exhaustive you may need to write your own to\nget Torch-TensorRT to support your module.\n\nShipped with the Torch-TensorRT distribution are the internal core API\nheaders. You can therefore access the converter registry and add a\nconverter for the op you need.\n\nFor example, if we try to compile a graph with a build of Torch-TensorRT\nthat doesn\u2019t support the flatten operation (``aten::flatten``) you may\nsee this error:\n\n::\n\n   terminate called after throwing an instance of 'torch_tensorrt::Error'\n   what():  [enforce fail at core/conversion/conversion.cpp:109] Expected converter to be true but got false\n   Unable to convert node: %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0 (conversion.AddLayer)\n   Schema: aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)\n   Converter for aten::flatten requested, but no such converter was found.\n   If you need a converter for this operator, you can try implementing one yourself\n   or request a converter: https://www.github.com/NVIDIA/Torch-TensorRT/issues\n\nWe can register a converter for this operator in our application. All of\nthe tools required to build a converter can be imported by including\n``Torch-TensorRT/core/conversion/converters/converters.h``. We start by\ncreating an instance of the self-registering\n``class torch_tensorrt::core::conversion::converters::RegisterNodeConversionPatterns()``\nwhich will register converters in the global converter registry,\nassociating a function schema like\n``aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)``\nwith a lambda that will take the state of the conversion, the\nnode/operation in question to convert and all of the inputs to the node\nand produces as a side effect a new layer in the TensorRT network.\nArguments are passed as a vector of inspectable unions of TensorRT\nITensors and Torch IValues in the order arguments are listed in the\nschema.\n\nBelow is a implementation of a ``aten::flatten`` converter that we can\nuse in our application. You have full access to the Torch and TensorRT\nlibraries in the converter implementation. So for example we can quickly\nget the output size by just running the operation in PyTorch instead of\nimplementing the full calculation outself like we do below for this\nflatten converter.\n\n.. code-block:: cpp\n\n   #include \"torch/script.h\"\n   #include \"torch_tensorrt/torch_tensorrt.h\"\n   #include \"torch_tensorrt/core/conversion/converters/converters.h\"\n\n   static auto flatten_converter = torch_tensorrt::core::conversion::converters::RegisterNodeConversionPatterns()\n       .pattern({\n           \"aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)\",\n           [](torch_tensorrt::core::conversion::ConversionCtx* ctx,\n              const torch::jit::Node* n,\n              torch_tensorrt::core::conversion::converters::args& args) -> bool {\n               auto in = args[0].ITensor();\n               auto start_dim = args[1].unwrapToInt();\n               auto end_dim = args[2].unwrapToInt();\n               auto in_shape = torch_tensorrt::core::util::toVec(in->getDimensions());\n               auto out_shape = torch::flatten(torch::rand(in_shape), start_dim, end_dim).sizes();\n\n               auto shuffle = ctx->net->addShuffle(*in);\n               shuffle->setReshapeDimensions(torch_tensorrt::core::util::toDims(out_shape));\n               shuffle->setName(torch_tensorrt::core::util::node_info(n).c_str());\n\n               auto out_tensor = ctx->AssociateValueAndTensor(n->outputs()[0], shuffle->getOutput(0));\n               return true;\n           }\n       });\n\nTo use this converter in Python, it is recommended to use PyTorch\u2019s `C++\n/ CUDA\nExtention <https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions>`__\ntemplate to wrap your library of converters into a ``.so`` that you can\nload with ``ctypes.CDLL()`` in your Python application.\n\nYou can find more information on all the details of writing converters\nin the contributors documentation (`Writing\nConverters <https://nvidia.github.io/Torch-TensorRT/contributors/writing_converters.html#writing-converters>`__).\nIf you find yourself with a large library of converter implementations,\ndo consider upstreaming them, PRs are welcome and it would be great for\nthe community to benefit as well.\n\n\n",
            "description_content_type": "",
            "docs_url": null,
            "download_url": "https://github.com/NVIDIA/Torch-TensorRT/releases",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/NVIDIA/Torch-TensorRT",
            "keywords": "nvidia,deep learning,machine learning,supervised learning,unsupervised learning,reinforcement learning,logging",
            "license": "BSD",
            "maintainer": "Naren Dasan",
            "maintainer_email": "narens@nvidia.com",
            "name": "torch-tensorrt",
            "package_url": "https://pypi.org/project/torch-tensorrt/",
            "platform": "Linux",
            "project_url": "https://pypi.org/project/torch-tensorrt/",
            "project_urls": {
                "Download": "https://github.com/NVIDIA/Torch-TensorRT/releases",
                "Homepage": "https://github.com/NVIDIA/Torch-TensorRT"
            },
            "release_url": "https://pypi.org/project/torch-tensorrt/0.0.0.post1/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Torch-TensorRT is a package which allows users to automatically compile PyTorch and TorchScript modules to TensorRT while remaining in PyTorch",
            "version": "0.0.0.post1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12989373,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "78bf839c0496121d37e0683164b3445c",
                    "sha256": "ebe119f7783ebd4ef4e44815c0a3014941172a2deb4f325ffc410369891138d7"
                },
                "downloads": -1,
                "filename": "torch-tensorrt-0.0.0.post1.tar.gz",
                "has_sig": false,
                "md5_digest": "78bf839c0496121d37e0683164b3445c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 8988,
                "upload_time": "2022-02-23T23:28:26",
                "upload_time_iso_8601": "2022-02-23T23:28:26.155653Z",
                "url": "https://files.pythonhosted.org/packages/f3/5e/81c357454c59acedb1b83bfa1e2032f95bde827b5631bb695af6911f5a14/torch-tensorrt-0.0.0.post1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}