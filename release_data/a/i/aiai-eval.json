{
    "0.0.1": {
        "info": {
            "author": "Dan Saattrup Nielsen",
            "author_email": "dan.nielsen@alexandra.dk",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description": "<div align='center'>\n<img src=\"https://raw.githubusercontent.com/alexandrainst/AIAI-eval/main/gfx/aiai-eval-logo.png\" width=\"auto\" height=\"224\">\n</div>\n\n### Evaluation of finetuned models.\n\n______________________________________________________________________\n[![PyPI Status](https://badge.fury.io/py/aiai_eval.svg)](https://pypi.org/project/aiai_eval/)\n[![Documentation](https://img.shields.io/badge/docs-passing-green)](https://alexandrainst.github.io/AIAI-eval/aiai_eval.html)\n[![License](https://img.shields.io/github/license/alexandrainst/AIAI-eval)](https://github.com/alexandrainst/AIAI-eval/blob/main/LICENSE)\n[![LastCommit](https://img.shields.io/github/last-commit/alexandrainst/AIAI-eval)](https://github.com/alexandrainst/AIAI-eval/commits/main)\n[![Code Coverage](https://img.shields.io/badge/Coverage-79%25-yellowgreen.svg)](https://github.com/alexandrainst/AIAI-eval/tree/main/tests)\n\n\nDevelopers:\n\n- Dan Saattrup Nielsen (dan.nielsen@alexandra.dk)\n- Anders Jess Pedersen (anders.j.pedersen@alexandra.dk)\n\n\n## Installation\nTo install the package simply write the following command in your favorite terminal:\n```\n$ pip install aiai-eval\n```\n\n## Quickstart\n### Benchmarking from the Command Line\nThe easiest way to benchmark pretrained models is via the command line interface. After\nhaving installed the package, you can benchmark your favorite model like so:\n```\n$ evaluate --model-id <model_id> --task <task>\n```\n\nHere `model_id` is the HuggingFace model ID, which can be found on the [HuggingFace\nHub](https://huggingface.co/models), and `task` is the task you want to benchmark the\nmodel on, such as \"ner\" for named entity recognition. See all options by typing\n```\n$ evaluate --help\n```\n\nThe specific model version to use can also be added after the suffix '@':\n```\n$ evaluate --model_id <model_id>@<commit>\n```\n\nIt can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.\n\nMultiple models and tasks can be specified by just attaching multiple arguments. Here\nis an example with two models:\n```\n$ evaluate --model_id <model_id1> --model_id <model_id2> --task ner\n```\n\nSee all the arguments and options available for the `evaluate` command by typing\n```\n$ evaluate --help\n```\n\n### Benchmarking from a Script\nIn a script, the syntax is similar to the command line interface. You simply initialise\nan object of the `Evaluator` class, and call this evaluate object with your favorite\nmodels and/or datasets:\n```\n>>> from aiai_eval import Evaluator\n>>> evaluator = Evaluator()\n>>> evaluator('<model_id>', '<task>')\n```\n\n\n## Project structure\n```\n.\n\u251c\u2500\u2500 .flake8\n\u251c\u2500\u2500 .github\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ci.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 docs.yaml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 gfx\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aiai-eval-logo.png\n\u251c\u2500\u2500 makefile\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 poetry.toml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aiai_eval\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 automatic_speech_recognition.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 cli.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 co2.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 country_codes.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluator.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 exceptions.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hf_hub.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 image_to_text.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 named_entity_recognition.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 question_answering.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 scoring.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 task.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 task_configs.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 task_factory.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 text_classification.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utils.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 fix_dot_env_file.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 versioning.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 test_cli.py\n    \u251c\u2500\u2500 test_co2.py\n    \u251c\u2500\u2500 test_config.py\n    \u251c\u2500\u2500 test_country_codes.py\n    \u251c\u2500\u2500 test_evaluator.py\n    \u251c\u2500\u2500 test_exceptions.py\n    \u251c\u2500\u2500 test_hf_hub.py\n    \u251c\u2500\u2500 test_image_to_text.py\n    \u251c\u2500\u2500 test_named_entity_recognition.py\n    \u251c\u2500\u2500 test_question_answering.py\n    \u251c\u2500\u2500 test_scoring.py\n    \u251c\u2500\u2500 test_task.py\n    \u251c\u2500\u2500 test_task_configs.py\n    \u251c\u2500\u2500 test_task_factory.py\n    \u251c\u2500\u2500 test_text_classification.py\n    \u2514\u2500\u2500 test_utils.py\n```\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "aiai-eval",
            "package_url": "https://pypi.org/project/aiai-eval/",
            "platform": null,
            "project_url": "https://pypi.org/project/aiai-eval/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/aiai-eval/0.0.1/",
            "requires_dist": [
                "torch (>=1.12.0,<2.0.0)",
                "transformers (>=4.21.0,<5.0.0)",
                "spacy (>=3.4.1,<4.0.0)",
                "sentencepiece (>=0.1.96,<0.2.0)",
                "protobuf (>=4.21.4,<5.0.0)",
                "tqdm (>=4.64.0,<5.0.0)",
                "seqeval (>=1.2.2,<2.0.0)",
                "huggingface-hub (>=0.8.1,<1.0.0)",
                "datasets (>=2.4.0,<3.0.0)",
                "codecarbon (>=2.1.3,<3.0.0)",
                "fsspec (>=2022.7.1,<2023.0.0)",
                "termcolor (>=1.1.0,<2.0.0)",
                "gradio (>=3.1.7,<4.0.0)"
            ],
            "requires_python": ">=3.8,<3.11",
            "summary": "Evaluation of finetuned models.",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 14922493,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "6ee0982306218cf3987735d6cbe0efea",
                    "sha256": "118bec8bd48f91231c7177560bfd76d33df5132f5491c1916a4bc3468647169b"
                },
                "downloads": -1,
                "filename": "aiai_eval-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "6ee0982306218cf3987735d6cbe0efea",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8,<3.11",
                "size": 37321,
                "upload_time": "2022-08-29T09:58:26",
                "upload_time_iso_8601": "2022-08-29T09:58:26.851224Z",
                "url": "https://files.pythonhosted.org/packages/50/1c/ed70b6566bfa8f0e0ebda29ae41d39df83118643cdc263cecab3e59e475f/aiai_eval-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5c401bac1fe565f239e5f124eacb6f87",
                    "sha256": "b5fb105ee9a0a4cfca16402cd2e51362dba90eba5869044fc51afc80cc8a4ca5"
                },
                "downloads": -1,
                "filename": "aiai_eval-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "5c401bac1fe565f239e5f124eacb6f87",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.8,<3.11",
                "size": 32432,
                "upload_time": "2022-08-29T09:58:28",
                "upload_time_iso_8601": "2022-08-29T09:58:28.168889Z",
                "url": "https://files.pythonhosted.org/packages/6e/db/83dfe30ce719237d4ce21fe0f4e442a493671e3366ae044683326d3a6718/aiai_eval-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}