{
    "0.1.0": {
        "info": {
            "author": "Maarten Grootendorst, Yao Su",
            "author_email": "maartengrootendorst@gmail.com, 1092702101@qq.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: MacOS",
                "Operating System :: Microsoft :: Windows",
                "Operating System :: POSIX",
                "Operating System :: Unix",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/deepdialog/ZhKeyBERT",
            "keywords": "nlp bert keyword extraction embeddings for Chinese",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "zhkeybert",
            "package_url": "https://pypi.org/project/zhkeybert/",
            "platform": "",
            "project_url": "https://pypi.org/project/zhkeybert/",
            "project_urls": {
                "Homepage": "https://github.com/deepdialog/ZhKeyBERT"
            },
            "release_url": "https://pypi.org/project/zhkeybert/0.1.0/",
            "requires_dist": [
                "sentence-transformers (>=0.3.8)",
                "scikit-learn (>=0.22.2)",
                "numpy (>=1.18.5)",
                "rich (>=10.4.0)",
                "mkdocs (>=1.1) ; extra == 'dev'",
                "mkdocs-material (>=4.6.3) ; extra == 'dev'",
                "mkdocstrings (>=0.8.0) ; extra == 'dev'",
                "pytest (>=5.4.3) ; extra == 'dev'",
                "pytest-cov (>=2.6.1) ; extra == 'dev'",
                "mkdocs (>=1.1) ; extra == 'docs'",
                "mkdocs-material (>=4.6.3) ; extra == 'docs'",
                "mkdocstrings (>=0.8.0) ; extra == 'docs'",
                "transformers (==3.5.1) ; extra == 'flair'",
                "torch (<1.7.1,>=1.4.0) ; extra == 'flair'",
                "flair (==0.7) ; extra == 'flair'",
                "pytest (>=5.4.3) ; extra == 'test'",
                "pytest-cov (>=2.6.1) ; extra == 'test'"
            ],
            "requires_python": ">=3.6",
            "summary": "Based on KeyBERT performs Chinese documents keyword extraction with state-of-the-art transformer models.",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12046362,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "e450847a58c84a629f561e84d98bbb3d",
                    "sha256": "98804491d1c98f6a6ab58d4a1c6c723d55742d42a1e65c6baed2021d162868d6"
                },
                "downloads": -1,
                "filename": "zhkeybert-0.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "e450847a58c84a629f561e84d98bbb3d",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6",
                "size": 23122,
                "upload_time": "2021-11-17T07:38:08",
                "upload_time_iso_8601": "2021-11-17T07:38:08.790833Z",
                "url": "https://files.pythonhosted.org/packages/30/6f/3f699fef137ed270a92b5dc4eed5f436ce61d567bed70af10ff2423d0db7/zhkeybert-0.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.1.1": {
        "info": {
            "author": "Maarten Grootendorst, Yao Su",
            "author_email": "maartengrootendorst@gmail.com, 1092702101@qq.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: MacOS",
                "Operating System :: Microsoft :: Windows",
                "Operating System :: POSIX",
                "Operating System :: Unix",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/deepdialog/ZhKeyBERT",
            "keywords": "nlp bert keyword extraction embeddings for Chinese",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "zhkeybert",
            "package_url": "https://pypi.org/project/zhkeybert/",
            "platform": "",
            "project_url": "https://pypi.org/project/zhkeybert/",
            "project_urls": {
                "Homepage": "https://github.com/deepdialog/ZhKeyBERT"
            },
            "release_url": "https://pypi.org/project/zhkeybert/0.1.1/",
            "requires_dist": [
                "sentence-transformers (>=0.3.8)",
                "scikit-learn (>=0.22.2)",
                "numpy (>=1.18.5)",
                "rich (>=10.4.0)",
                "mkdocs (>=1.1) ; extra == 'dev'",
                "mkdocs-material (>=4.6.3) ; extra == 'dev'",
                "mkdocstrings (>=0.8.0) ; extra == 'dev'",
                "pytest (>=5.4.3) ; extra == 'dev'",
                "pytest-cov (>=2.6.1) ; extra == 'dev'",
                "mkdocs (>=1.1) ; extra == 'docs'",
                "mkdocs-material (>=4.6.3) ; extra == 'docs'",
                "mkdocstrings (>=0.8.0) ; extra == 'docs'",
                "transformers (==3.5.1) ; extra == 'flair'",
                "torch (<1.7.1,>=1.4.0) ; extra == 'flair'",
                "flair (==0.7) ; extra == 'flair'",
                "pytest (>=5.4.3) ; extra == 'test'",
                "pytest-cov (>=2.6.1) ; extra == 'test'"
            ],
            "requires_python": ">=3.6",
            "summary": "Based on KeyBERT performs Chinese documents keyword extraction with state-of-the-art transformer models.",
            "version": "0.1.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12046362,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "98f843310a02c3bc2b3d14a7bff7b6c2",
                    "sha256": "6275939ad6a32b76f3e2b15b6514d557a3d7dbb0ee13e7990e63c09fb29d4c0c"
                },
                "downloads": -1,
                "filename": "zhkeybert-0.1.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "98f843310a02c3bc2b3d14a7bff7b6c2",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6",
                "size": 23101,
                "upload_time": "2021-11-17T09:04:10",
                "upload_time_iso_8601": "2021-11-17T09:04:10.267388Z",
                "url": "https://files.pythonhosted.org/packages/02/04/4db7df2476940068e3c899e7ac9180db4928513dcfae7d8932f02d0cc4ed/zhkeybert-0.1.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.1.2": {
        "info": {
            "author": "Maarten Grootendorst, Yao Su",
            "author_email": "maartengrootendorst@gmail.com, 1092702101@qq.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: MacOS",
                "Operating System :: Microsoft :: Windows",
                "Operating System :: POSIX",
                "Operating System :: Unix",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "[![PyPI - Python](https://img.shields.io/badge/python-3.6%20|%203.7%20|%203.8-blue.svg)](https://pypi.org/project/keybert/)\n[![PyPI - License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/MaartenGr/keybert/blob/master/LICENSE)\n\n# ZhKeyBERT\n\n[\u4e2d\u6587\u6587\u6863](https://github.com/deepdialog/ZhKeyBERT/blob/master/README-zh.md)\n\n\nBased on [KeyBERT](https://github.com/MaartenGr/KeyBERT), enhance the keyword\nextraction model for Chinese.\n\nCorresponding medium post can be found [here](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea).\n\n<a name=\"toc\"/></a>\n## Table of Contents  \n<!--ts-->\n   1. [About the Project](#about)  \n   2. [Getting Started](#gettingstarted)    \n        2.1. [Installation](#installation)    \n        2.2. [Basic Usage](#usage)     \n        2.3. [Maximal Marginal Relevance](#maximal)  \n        2.4. [Embedding Models](#embeddings)\n<!--te-->\n\n\n<a name=\"about\"/></a>\n## 1. About the Project\n[Back to ToC](#toc)  \n\nAlthough there are already many methods available for keyword generation \n(e.g., \n[Rake](https://github.com/aneesha/RAKE), \n[YAKE!](https://github.com/LIAAD/yake), TF-IDF, etc.) \nI wanted to create a very basic, but powerful method for extracting keywords and keyphrases. \nThis is where **KeyBERT** comes in! Which uses BERT-embeddings and simple cosine similarity\nto find the sub-phrases in a document that are the most similar to the document itself.\n\nFirst, document embeddings are extracted with BERT to get a document-level representation. \nThen, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity \nto find the words/phrases that are the most similar to the document. The most similar words could \nthen be identified as the words that best describe the entire document.  \n\nKeyBERT is by no means unique and is created as a quick and easy method\nfor creating keywords and keyphrases. Although there are many great \npapers and solutions out there that use BERT-embeddings \n(e.g., \n[1](https://github.com/pranav-ust/BERT-keyphrase-extraction),\n[2](https://github.com/ibatra/BERT-Keyword-Extractor),\n[3](https://www.preprints.org/manuscript/201908.0073/download/final_file),\n), I could not find a BERT-based solution that did not have to be trained from scratch and\ncould be used for beginners (**correct me if I'm wrong!**).\nThus, the goal was a `pip install keybert` and at most 3 lines of code in usage.   \n\n<a name=\"gettingstarted\"/></a>\n## 2. Getting Started\n[Back to ToC](#toc)  \n\n<a name=\"installation\"/></a>\n###  2.1. Installation\n\n```\ngit clone https://github.com/deepdialog/ZhKeyBERT\ncd ZhKeyBERT\npython setup.py install --user\n```\n\n<a name=\"usage\"/></a>\n###  2.2. Usage\n\nThe most minimal example can be seen below for the extraction of keywords:\n```python\nfrom zhkeybert import KeyBERT, extract_kws_zh\n\ndocs = \"\"\"\u65f6\u503c10\u670825\u65e5\u6297\u7f8e\u63f4\u671d\u7eaa\u5ff5\u65e5\uff0c\u300a\u957f\u6d25\u6e56\u300b\u7247\u65b9\u53d1\u5e03\u4e86\u201c\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\u6297\u7f8e\u63f4\u671d\u51fa\u56fd\u4f5c\u621871\u5468\u5e74\u7279\u522b\u77ed\u7247\u201d\uff0c\u518d\u6b21\u5411\u4f1f\u5927\u7684\u5fd7\u613f\u519b\u81f4\u656c\uff01\n\u7535\u5f71\u300a\u957f\u6d25\u6e56\u300b\u5168\u60c5\u5168\u666f\u5730\u8fd8\u539f\u4e8671\u5e74\u524d\u6297\u7f8e\u63f4\u671d\u6218\u573a\u4e0a\u90a3\u573a\u53f2\u8bd7\u6218\u5f79\uff0c\u5fd7\u613f\u519b\u594b\u4e0d\u987e\u8eab\u7684\u82f1\u52c7\u7cbe\u795e\u4ee4\u89c2\u4f17\u611f\u53f9\uff1a\u201c\u5c81\u6708\u5ce5\u5d58\u82f1\u96c4\u4e0d\u706d\uff0c\u4e39\u5fc3\u94c1\u9aa8\u519b\u9b42\u6c38\u5b58\uff01\u201d\u5f71\u7247\u4e0a\u6620\u4ee5\u6765\u7968\u623f\u5c61\u521b\u65b0\u9ad8\uff0c\u76ee\u524d\u7a81\u783453\u4ebf\u5143\uff0c\u6682\u5217\u4e2d\u56fd\u5f71\u53f2\u7968\u623f\u603b\u699c\u7b2c\u4e09\u540d\u3002\n\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u8fd9\u90e8\u5f71\u7247\u7684\u5f88\u591a\u4e3b\u521b\u6216\u6709\u519b\u4eba\u7684\u8840\u8109\uff0c\u6216\u6709\u5f53\u5175\u7684\u7ecf\u5386\uff0c\u6216\u8005\u5bb6\u4eba\u662f\u519b\u4eba\u3002\u63d0\u8d77\u8fd9\u4e9b\u4ed6\u4eec\u4e5f\u5145\u6ee1\u81ea\u8c6a\uff0c\u5f71\u7247\u603b\u76d1\u5236\u9ec4\u5efa\u65b0\u79f0\uff1a\u201c\u5f53\u5175\u4ee5\u540e\u4f1a\u6709\u4e00\u79cd\u7279\u522b\u80fd\u575a\u6301\u7684\u52b2\u513f\u3002\u201d\u9970\u6f14\u96f7\u516c\u7684\u80e1\u519b\u900f\u9732\uff1a\u201c\u6211\u7236\u4eb2\u66fe\u7ecf\u53c2\u52a0\u8fc7\u6297\u7f8e\u63f4\u671d\uff0c\u8fd8\u5f97\u4e86\u4e00\u4e2a\u4e09\u7b49\u529f\u3002\u201d\u5f71\u7247\u5386\u53f2\u987e\u95ee\u738b\u6811\u589e\u8868\u793a\uff1a\u201c\u6211\u5f53\u4e86\u4e94\u5341\u591a\u5e74\u7684\u5175\uff0c\u6211\u7684\u8001\u90e8\u961f\u5c31\u662f\u4e0a\u7518\u5cad\u4e0a\u4e0b\u6765\u7684\uff0c\u90a3\u4e9b\u8001\u5175\u90fd\u662f\u6211\u7684\u5076\u50cf\u3002\u201d\n\u201c\u8eab\u5148\u58eb\u5352\u536b\u534e\u590f\u5bb6\u56fd\uff0c\u8840\u6218\u65e0\u754f\u62a4\u5c71\u6cb3\u65e0\u6059\u3002\u201d\u7247\u4e2d\u9970\u6f14\u4e03\u8fde\u8fde\u957f\u4f0d\u5343\u91cc\u7684\u5434\u4eac\u611f\u53f9\uff1a\u201c\u8981\u6c38\u8fdc\u8bb0\u4f4f\u8fd9\u4e9b\u5148\u70c8\u4eec\uff0c\u4ed6\u4eec\u7ed9\u6211\u4eec\u5e26\u6765\u4eca\u5929\u7684\u548c\u5e73\u3002\u611f\u8c22\u4ed6\u4eec\u7684\u4ed8\u51fa\uff0c\u624d\u8ba9\u6211\u4eec\u6709\u4eca\u5929\u7684\u5e78\u798f\u751f\u6d3b\u3002\u201d\u9970\u6f14\u65b0\u5175\u4f0d\u4e07\u91cc\u7684\u6613\u70ca\u5343\u73ba\u8868\u793a\uff1a\u201c\u6218\u4e89\u7684\u6b8b\u9177\u3001\u78be\u538b\u5f0f\u7684\u4f24\u5bb3\uff0c\u5176\u5b9e\u6211\u4eec\u73b0\u5728\u7684\u5e74\u8f7b\u4eba\u51e0\u4e4e\u5f88\u96be\u80fd\u4f53\u4f1a\u5230\uff0c\u5e0c\u671b\u5927\u5bb6\u770b\u5b8c\u7535\u5f71\u540e\u80fd\u660e\u767d\uff0c\u662f\u90a3\u4e9b\u5148\u8f88\u4eec\u7684\u727a\u7272\u5949\u732e\uff0c\u6362\u6765\u4e86\u6211\u4eec\u7684\u73b0\u5728\u3002\u201d\n\u5f71\u7247\u5bf9\u6218\u4e89\u7fa4\u50cf\u7684\u6062\u5f18\u5448\u73b0\uff0c\u5bf9\u4e2a\u4f53\u547d\u8fd0\u7684\u6df1\u5207\u5173\u6000\uff0c\u4ee4\u8bb8\u591a\u89c2\u4f17\u65e0\u6cd5\u63a7\u5236\u81ea\u5df1\u7684\u773c\u6cea\uff0c\u89c2\u4f17\u79f0\uff1a\u201c\u5f53\u770b\u5230\u5f71\u7247\u4e2d\u7684\u60ca\u9669\u6218\u6597\u573a\u9762\uff0c\u770b\u5230\u82f1\u96c4\u4eec\u58ee\u6000\u6fc0\u70c8\u7684\u62fc\u6740\uff0c\u4e3a\u56fd\u6350\u8eaf\u7684\u82f1\u52c7\u65e0\u754f\u548c\u65e0\u6094\u4ed8\u51fa\uff0c\u6211\u660e\u767d\u4e86\u4e3a\u4ec0\u4e48\u8bf4\u4eca\u5929\u7684\u5e78\u798f\u751f\u6d3b\u6765\u4e4b\u4e0d\u6613\u3002\u201d\uff08\u8bb0\u8005 \u738b\u91d1\u8dc3\uff09\"\"\"\nkw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\nextract_kws_zh(docs, kw_model)\n```\n\nComparison\n```python\n>>> extract_kws_zh(docs, kw_model)\n\n[('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\u6297\u7f8e\u63f4\u671d', 0.7034),\n ('\u7535\u5f71\u957f\u6d25\u6e56', 0.6285),\n ('\u5468\u5e74\u7279\u522b\u77ed\u7247', 0.5668),\n ('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b', 0.6894),\n ('\u4f5c\u621871\u5468\u5e74', 0.5637)]\n>>> import jieba; kw_model.extract_keywords(' '.join(jieba.cut(docs)), keyphrase_ngram_range=(1, 3), \n                                            use_mmr=True, diversity=0.25)\n\n[('\u6297\u7f8e\u63f4\u671d \u7eaa\u5ff5\u65e5 \u957f\u6d25\u6e56', 0.796),\n ('\u7eaa\u5ff5 \u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b \u6297\u7f8e\u63f4\u671d', 0.7577),\n ('\u4f5c\u6218 71 \u5468\u5e74', 0.6126),\n ('25 \u6297\u7f8e\u63f4\u671d \u7eaa\u5ff5\u65e5', 0.635),\n ('\u81f4\u656c \u7535\u5f71 \u957f\u6d25\u6e56', 0.6514)]\n```\n\nYou can set `ngram_range`, whose default value is `(1, 3)`,\nto set the length of the resulting keywords/keyphrases:\n\n```python\n>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 1))\n[('\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b', 0.6094),\n ('\u957f\u6d25\u6e56', 0.505),\n ('\u5468\u5e74', 0.4504),\n ('\u5f71\u7247', 0.447),\n ('\u82f1\u96c4', 0.4297)]\n```\n\n```python\n>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 2))\n[('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b', 0.6894),\n ('\u7535\u5f71\u957f\u6d25\u6e56', 0.6285),\n ('\u5e74\u524d\u6297\u7f8e\u63f4\u671d', 0.476),\n ('\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\u6297\u7f8e\u63f4\u671d', 0.6349),\n ('\u4e2d\u56fd\u5f71\u53f2', 0.5534)]\n``` \n\n**NOTE**: For a full overview of all possible transformer models see [sentence-transformer](https://www.sbert.net/docs/pretrained_models.html).\nI would advise `\"paraphrase-multilingual-MiniLM-L12-v2\"` Chinese documents for efficiency\nand acceptable accuracy.\n\n<a name=\"maximal\"/></a>\n###  2.3. Maximal Marginal Relevance\n\nIt's recommended to use Maximal Margin Relevance (MMR) for diversity by\nsetting the optional parameter `use_mmr`, which is `True` in default.  \nTo diversify the results, we can use MMR to create\nkeywords / keyphrases which is also based on cosine similarity. The results \nwith **high diversity**:\n\n```python\n>>> extract_kws_zh(docs, kw_model, use_mmr = True, diversity=0.7)\n[('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\u6297\u7f8e\u63f4\u671d', 0.7034),\n ('\u89c2\u4f17\u65e0\u6cd5\u63a7\u5236\u81ea\u5df1', 0.1212),\n ('\u5c71\u6cb3\u65e0\u6059', 0.2233),\n ('\u5f71\u7247\u4e0a\u6620\u4ee5\u6765', 0.5427),\n ('53\u4ebf\u5143', 0.3287)]\n``` \n\nThe results with **low diversity**:  \n\n```python\n>>> extract_kws_zh(docs, kw_model, use_mmr = True, diversity=0.2)\n[('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\u6297\u7f8e\u63f4\u671d', 0.7034),\n ('\u7535\u5f71\u957f\u6d25\u6e56', 0.6285),\n ('\u7eaa\u5ff5\u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b', 0.6894),\n ('\u5468\u5e74\u7279\u522b\u77ed\u7247', 0.5668),\n ('\u4f5c\u621871\u5468\u5e74', 0.5637)]\n``` \n\nAnd the default and recommended `diversity` is `0.25`.\n\n<a name=\"embeddings\"/></a>\n###  2.4. Embedding Models\nKeyBERT supports many embedding models that can be used to embed the documents and words:\n\n* Sentence-Transformers\n* Flair\n* Spacy\n* Gensim\n* USE\n\nClick [here](https://maartengr.github.io/KeyBERT/guides/embeddings.html) for a full overview of all supported embedding models.\n\n**Sentence-Transformers**  \nYou can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html) \nand pass it through KeyBERT with `model`:\n\n```python\nfrom zhkeybert import KeyBERT\nkw_model = KeyBERT(model='all-MiniLM-L6-v2')\n```\n\nOr select a SentenceTransformer model with your own parameters:\n\n```python\nfrom zhkeybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nkw_model = KeyBERT(model=sentence_model)\n```\n\nFor Chinese keywords extraction, you should choose multilingual models\nlike `paraphrase-multilingual-mpnet-base-v2` and `paraphrase-multilingual-MiniLM-L12-v2`. \n\n**MUSE**  \nMultilingual Universal Sentence Encoder([MUSE](https://arxiv.org/abs/1907.04307))\n\n```python\nfrom zhkeybert import KeyBERT\nimport tensorflow_hub import hub\n\nmodule_url = 'https://hub.tensorflow.google.cn/google/universal-sentence-encoder-multilingual-large/3'\n\nmodel = hub.load(module_url)\nkw_model = KeyBERT(model=model) ## slow but acceptable performance\n```\n\n## Citation\nTo cite KeyBERT in your work, please use the following bibtex reference:\n\n```bibtex\n@misc{grootendorst2020keybert,\n  author       = {Maarten Grootendorst},\n  title        = {KeyBERT: Minimal keyword extraction with BERT.},\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v0.3.0},\n  doi          = {10.5281/zenodo.4461265},\n  url          = {https://doi.org/10.5281/zenodo.4461265}\n}\n```\n\n## References\nBelow, you can find several resources that were used for the creation of KeyBERT \nbut most importantly, these are amazing resources for creating impressive keyword extraction models: \n\n**Papers**:  \n* Sharma, P., & Li, Y. (2019). [Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling.](https://www.preprints.org/manuscript/201908.0073/download/final_file)\n\n**Github Repos**:  \n* https://github.com/thunlp/BERT-KPE\n* https://github.com/ibatra/BERT-Keyword-Extractor\n* https://github.com/pranav-ust/BERT-keyphrase-extraction\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**MMR**:  \nThe selection of keywords/keyphrases was modeled after:\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**NOTE**: If you find a paper or github repo that has an easy-to-use implementation\nof BERT-embeddings for keyword/keyphrase extraction, let me know! I'll make sure to\nadd a reference to this repo. \n\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/deepdialog/ZhKeyBERT",
            "keywords": "nlp bert keyword extraction embeddings for Chinese",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "zhkeybert",
            "package_url": "https://pypi.org/project/zhkeybert/",
            "platform": "",
            "project_url": "https://pypi.org/project/zhkeybert/",
            "project_urls": {
                "Homepage": "https://github.com/deepdialog/ZhKeyBERT"
            },
            "release_url": "https://pypi.org/project/zhkeybert/0.1.2/",
            "requires_dist": [
                "jieba",
                "sentence-transformers (>=0.3.8)",
                "scikit-learn (>=0.22.2)",
                "numpy (>=1.18.5)",
                "rich (>=10.4.0)",
                "mkdocs (>=1.1) ; extra == 'dev'",
                "mkdocs-material (>=4.6.3) ; extra == 'dev'",
                "mkdocstrings (>=0.8.0) ; extra == 'dev'",
                "pytest (>=5.4.3) ; extra == 'dev'",
                "pytest-cov (>=2.6.1) ; extra == 'dev'",
                "mkdocs (>=1.1) ; extra == 'docs'",
                "mkdocs-material (>=4.6.3) ; extra == 'docs'",
                "mkdocstrings (>=0.8.0) ; extra == 'docs'",
                "transformers (==3.5.1) ; extra == 'flair'",
                "torch (<1.7.1,>=1.4.0) ; extra == 'flair'",
                "flair (==0.7) ; extra == 'flair'",
                "pytest (>=5.4.3) ; extra == 'test'",
                "pytest-cov (>=2.6.1) ; extra == 'test'"
            ],
            "requires_python": ">=3.6",
            "summary": "Based on KeyBERT performs Chinese documents keyword extraction with state-of-the-art transformer models.",
            "version": "0.1.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12046362,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "8f68b0a3f9f12c1b0474855ccac99937",
                    "sha256": "01763ddb5c6429f23c48d75343cc874b67b181c7f8e7dac553be013061ace945"
                },
                "downloads": -1,
                "filename": "zhkeybert-0.1.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "8f68b0a3f9f12c1b0474855ccac99937",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6",
                "size": 23111,
                "upload_time": "2021-11-17T09:11:02",
                "upload_time_iso_8601": "2021-11-17T09:11:02.390812Z",
                "url": "https://files.pythonhosted.org/packages/1e/0a/7f2e81bd0b8615490506ba2985e87d026d0749e8b454f74ec9188906661b/zhkeybert-0.1.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}