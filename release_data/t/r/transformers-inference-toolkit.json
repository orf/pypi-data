{
    "0.1.0": {
        "info": {
            "author": "Alexey Burlakov",
            "author_email": "feraturdev@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description": "# Transformers Inference Toolkit\n\ud83e\udd17 [Transformers](https://github.com/huggingface/transformers) library provides great API for manipulating pre-trained NLP (as well as CV and Audio-related) models. However, preparing \ud83e\udd17 Transformers models for use in production usually requires additional effort. The purpose of `transformers-inference-toolkit` is to get rid of boilerplate code and to simplify automatic optimization and inference process of Huggingface Transformers models.\n\n## Installation\nUsing `pip`:\n```bash\npip install transformers-inference-toolkit\n```\n\n## Optimization\nThe original \ud83e\udd17 Transformers library includes `transformers.onnx` package, which can be used to convert PyTorch or TensorFlow models into [ONNX](https://onnx.ai/) format. This Toolkit extends this functionality by giving the user an opportunity to automatically [optimize ONNX model graph](https://onnxruntime.ai/docs/performance/graph-optimizations.html) - this is similar to what \ud83e\udd17 [Optimum](https://github.com/huggingface/optimum) library does, but \ud83e\udd17 Optimum currently has limited support for locally stored pre-trained models as well as for models of less popular architectures (for example, [MPNet](https://github.com/microsoft/MPNet)).\n\nAside from ONNX conversion the Toolkit also supports resaving PyTorch models with half-precision and setting up [DeepSpeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/).\n\n### Prerequisite\nThe Toolkit expects your pretrained model (in PyTorch format) and tokenizer to be saved (using `save_pretrained()` method) inside a common parent directory in `model` and `tokenizer` folders respectively. This is how a file structure of `toxic-bert` model should look like:\n```bash\ntoxic-bert\n\u251c\u2500\u2500 model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pytorch_model.bin\n\u2514\u2500\u2500 tokenizer\n    \u251c\u2500\u2500 merges.txt\n    \u251c\u2500\u2500 special_tokens_map.json\n    \u251c\u2500\u2500 tokenizer_config.json\n    \u2514\u2500\u2500 vocab.json\n```\n\n### How to use\nMost of the popular Transformer model architectures (like BERT and its variations) can be converted with a single command:\n```python\nfrom transformers_inference_toolkit import (\n    Feature,\n    OnnxModelType,\n    OnnxOptimizationLevel,\n    optimizer,\n)\n\noptimizer.pack_onnx(\n    input_path=\"toxic-bert\",\n    output_path=\"toxic-bert-optimized\",\n    feature=Feature.SEQUENCE_CLASSIFICATION,\n    for_gpu=True,\n    fp16=True,\n    optimization_level=OnnxOptimizationLevel.FULL,\n)\n```\nIf your model architecture is not supported out-of-the-box (described [here](https://huggingface.co/docs/transformers/serialization)) you can try writing a custom OnnxConfig class:\n```python\nfrom collections import OrderedDict\nfrom transformers.onnx import OnnxConfig\n\nclass MPNetOnnxConfig(OnnxConfig):\n    @property\n    def default_onnx_opset(self):\n        return 14\n\n    @property\n    def inputs(self):\n        dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n        return OrderedDict(\n            [\n                (\"input_ids\", dynamic_axis),\n                (\"attention_mask\", dynamic_axis),\n            ]\n        )\n\noptimizer.pack_onnx(\n    input_path=\"all-mpnet-base-v2\",\n    output_path=\"all-mpnet-base-v2-optimized\",\n    feature=Feature.DEFAULT,\n    custom_onnx_config_cls=MPNetOnnxConfig,\n)\n```\nONNX is not the only option, it is also possible to resave the model for future inference simply using PyTorch (`optimizer.pack_transformers()` method, `force_fp16` argument to save in half-precision) or [DeepSpeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) (`optimizer.pack_deepspeed()` method):\n```python\noptimizer.pack_deepspeed(\n    input_path=\"gpt-neo\",\n    output_path=\"gpt-neo-optimized\",\n    feature=Feature.CAUSAL_LM,\n    replace_with_kernel_inject=True,\n    mp_size=1,\n)\n```\nAfter calling `optimizer` methods the model and tokenizer would be saved at `output_path`. The output directory will also contain `metadata.json` file that is necessary for the `Predictor` object (described below) to correctly load the model:\n```bash\ntoxic-bert-optimized\n\u251c\u2500\u2500 metadata.json\n\u251c\u2500\u2500 model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.onnx\n\u2514\u2500\u2500 tokenizer\n    \u251c\u2500\u2500 special_tokens_map.json\n    \u251c\u2500\u2500 tokenizer.json\n    \u2514\u2500\u2500 tokenizer_config.json\n```\n## Prediction\nAfter model and tokenizer are packaged using one of the `optimizer` methods, it is possible to initialize a `Predictor` object:\n```python\n>>> from transformers_inference_toolkit import Predictor\n>>> \n>>> predictor = Predictor(\"toxic-bert-optimized\", cuda=True)\n>>> print(predictor(\"I hate this!\"))\n{'logits': array([[ 0.02940369, -7.0195312 , -4.7890625 , -6.0664062 , -5.625     ,\n        -6.09375   ]], dtype=float32)}\n```\nThe `Predictor` object can be simply called with tokenizer arguments (similar to \ud83e\udd17 Transformers `pipeline`s, `return_tensors` argument can be omitted, `padding` and `truncation` are `True` by default). For text generation tasks `Predictor.generate()` method (with [generation arguments](https://huggingface.co/docs/transformers/main_classes/text_generation)) can be used:\n```python\n>>> predictor = Predictor(\"gpt-neo-optimized\", cuda=True)\n>>> predictor.generate(\n...     \"Tommy: Hi Mark!\",\n...     do_sample=True,\n...     top_p=0.9,\n...     num_return_sequences=3,\n...     max_new_tokens=5,\n... )\n['Tommy: Hi Mark!\\nMadelyn: Hello', 'Tommy: Hi Mark! It\u2019s so', 'Tommy: Hi Mark! How are you?\\n']\n```\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/feratur/transformers-inference-toolkit",
            "keywords": "",
            "license": "Apache-2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "transformers-inference-toolkit",
            "package_url": "https://pypi.org/project/transformers-inference-toolkit/",
            "platform": null,
            "project_url": "https://pypi.org/project/transformers-inference-toolkit/",
            "project_urls": {
                "Homepage": "https://github.com/feratur/transformers-inference-toolkit",
                "Repository": "https://github.com/feratur/transformers-inference-toolkit"
            },
            "release_url": "https://pypi.org/project/transformers-inference-toolkit/0.1.0/",
            "requires_dist": [
                "transformers (>=4.24.0,<5.0.0)",
                "torch (>=1.12.1,<2.0.0)",
                "accelerate (>=0.14.0,<0.15.0)",
                "onnxruntime-gpu (>=1.13.1,<2.0.0)",
                "onnx (>=1.12.0,<2.0.0)",
                "deepspeed (>=0.7.4,<0.8.0)"
            ],
            "requires_python": ">=3.7,<3.11",
            "summary": "A collection of helper methods to simplify optimization and inference of Huggingface Transformers-based models",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15932847,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "91883904e890d62f6ca8d42e28f5c9eb",
                    "sha256": "bfc85c1d3d5f0c4ba990b09370addb3b9cd7288b94bad39f482de2f2239f822d"
                },
                "downloads": -1,
                "filename": "transformers_inference_toolkit-0.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "91883904e890d62f6ca8d42e28f5c9eb",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<3.11",
                "size": 14043,
                "upload_time": "2022-11-29T20:43:19",
                "upload_time_iso_8601": "2022-11-29T20:43:19.734916Z",
                "url": "https://files.pythonhosted.org/packages/2d/16/3d203bd527b18673c42faaae297adb955d3659e3eb021b6ab27908f77f16/transformers_inference_toolkit-0.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "6e03bd18983a09980cc7be80122ede21",
                    "sha256": "8faf860cc76f7322b5b86f288bc96dc7d7b0900bc8e287604ca748df9bd421fe"
                },
                "downloads": -1,
                "filename": "transformers_inference_toolkit-0.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "6e03bd18983a09980cc7be80122ede21",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<3.11",
                "size": 13795,
                "upload_time": "2022-11-29T20:43:21",
                "upload_time_iso_8601": "2022-11-29T20:43:21.039468Z",
                "url": "https://files.pythonhosted.org/packages/20/a6/95e4e8be7456fb6e24b4b9f724b274d7fcd8aae6fb4cb01c0018d3ce5109/transformers_inference_toolkit-0.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}