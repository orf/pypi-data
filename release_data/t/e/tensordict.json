{
    "0.0.1a0": {
        "info": {
            "author": "tensordict contributors",
            "author_email": "vmoens@fb.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# TensorDict\n\n`TensorDict` is a dictionary-like class that inherits properties from tensors, such as indexing, shape operations, casting to device etc.\n\nThe main purpose of TensorDict is to make code-bases more _readable_ and _modular_ by abstracting away tailored operations:\n```python\nfor i, tensordict in enumerate(dataset):\n    # the model reads and writes tensordicts\n    tensordict = model(tensordict)\n    loss = loss_module(tensordict)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\nWith this level of abstraction, one can recycle a training loop for highly heterogeneous task.\nEach individual step of the training loop (data collection and transform, model prediction, loss computation etc.)\ncan be tailored to the use case at hand without impacting the others.\nFor instance, the above example can be easily used across classification and segmentation tasks, among many others.\n\n## Installation\n\nTo install the latest stable version of tensordict, simply run\n```bash\npip install tensordict\n```\nThis will work with python 3.7 and upward as well as pytorch 1.12 and upward.\n\nTo enjoy the latest features, one can use\n```bash\npip install tensordict-nightly\n```\n\n## Features\n\n### General\n\nA tensordict is primarily defined by its `batch_size` (or `shape`) and its key-value pairs:\n```python\nfrom tensordict import TensorDict\nimport torch\ntensordict = TensorDict({\n    \"key 1\": torch.ones(3, 4, 5),\n    \"key 2\": torch.zeros(3, 4, 5, dtype=torch.bool),\n}, batch_size=[3, 4])\n```\nThe `batch_size` and the first dimensions of each of the tensors must be compliant.\nThe tensors can be of any dtype and device. Optionally, one can restrict a tensordict to\nlive on a dedicated device, which will send each tensor that is written there:\n```python\ntensordict = TensorDict({\n    \"key 1\": torch.ones(3, 4, 5),\n    \"key 2\": torch.zeros(3, 4, 5, dtype=torch.bool),\n}, batch_size=[3, 4], device=\"cuda:0\")\ntensordict[\"key 3\"] = torch.randn(3, 4, device=\"cpu\")\nassert tensordict[\"key 3\"].device is torch.device(\"cuda:0\")\n```\n\n### Tensor-like features\n\nTensorDict objects can be indexed exactly like tensors. The resulting of indexing\na TensorDict is another TensorDict containing tensors indexed along the required dimension:\n```python\ntensordict = TensorDict({\n    \"key 1\": torch.ones(3, 4, 5),\n    \"key 2\": torch.zeros(3, 4, 5, dtype=torch.bool),\n}, batch_size=[3, 4])\nsub_tensordict = tensordict[..., :2]\nassert sub_tensordict.shape == torch.Size([3, 2])\nassert sub_tensordict[\"key 1\"].shape == torch.Size([3, 2, 5])\n```\n\nSimilarly, one can build tensordicts by stacking or concatenating single tensordicts:\n```python\ntensordicts = [TensorDict({\n    \"key 1\": torch.ones(3, 4, 5),\n    \"key 2\": torch.zeros(3, 4, 5, dtype=torch.bool),\n}, batch_size=[3, 4]) for _ in range(2)]\nstack_tensordict = torch.stack(tensordicts, 1)\nassert stack_tensordict.shape == torch.Size([3, 2, 4])\nassert stack_tensordict[\"key 1\"].shape == torch.Size([3, 2, 4, 5])\ncat_tensordict = torch.cat(tensordicts, 0)\nassert cat_tensordict.shape == torch.Size([6, 4])\nassert cat_tensordict[\"key 1\"].shape == torch.Size([6, 4, 5])\n```\n\nTensorDict instances can also be reshaped, viewed, squeezed and unsqueezed:\n```python\ntensordict = TensorDict({\n    \"key 1\": torch.ones(3, 4, 5),\n    \"key 2\": torch.zeros(3, 4, 5, dtype=torch.bool),\n}, batch_size=[3, 4])\nprint(tensordict.view(-1))  # prints torch.Size([12])\nprint(tensordict.reshape(-1))  # prints torch.Size([12])\nprint(tensordict.unsqueeze(-1))  # prints torch.Size([3, 4, 1])\n```\n\nOne can also send tensordict from device to device, place them in shared memory,\nclone them, update them in-place or not, split them, unbind them, expand them etc.\n\nIf a functionality is missing, it is easy to call it using `apply()` or `apply_()`:\n```python\ntensordict_uniform = tensordict.apply(lambda tensor: tensor.uniform_())\n```\n\n### TensorDict for functional programming using FuncTorch\n\nWe also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).\nFor instance, TensorDict makes it easy to concatenate model weights to do model ensembling:\n```python\nfrom torch import nn\nfrom tensordict import TensorDict\nfrom copy import deepcopy\nfrom tensordict.nn.functional_modules import FunctionalModule\nimport torch\nfrom functorch import vmap\nlayer1 = nn.Linear(3, 4)\nlayer2 = nn.Linear(4, 4)\nmodel1 = nn.Sequential(layer1, layer2)\nmodel2 = deepcopy(model1)\n# we represent the weights hierarchically\nweights1 = TensorDict(model1.state_dict(), []).unflatten_keys(\".\")\nweights2 = TensorDict(model2.state_dict(), []).unflatten_keys(\".\")\nweights = torch.stack([weights1, weights2], 0)\nfmodule, _ = FunctionalModule._create_from(model1)\n# an input we'd like to pass through the model\nx = torch.randn(10, 3)\ny = vmap(fmodule, (0, None))(weights, x)\ny.shape  # torch.Size([2, 10, 4])\n```\n\n### Lazy preallocation\n\nPre-allocating tensors can be cumbersome and hard to scale if the list of preallocated\nitems varies according to the script configuration. TensorDict solves this in an elegant way.\nAssume you are working with a function `foo() -> TensorDict`, e.g.\n```python\ndef foo():\n    tensordict = TensorDict({}, batch_size=[])\n    tensordict[\"a\"] = torch.randn(3)\n    tensordict[\"b\"] = TensorDict({\"c\": torch.zeros(2)}, batch_size=[])\n    return tensordict\n```\nand you would like to call this function repeatedly. You could do this in two ways.\nThe first would simply be to stack the calls to the function:\n```python\ntensordict = torch.stack([foo() for _ in range(N)])\n```\nHowever, you could also choose to preallocate the tensordict:\n```python\ntensordict = TensorDict({}, batch_size=[N])\nfor i in range(N):\n    tensordict[i] = foo()\n```\nwhich also results in a tensordict (when `N = 10`)\n```\nTensorDict(\n    fields={\n        a: Tensor(torch.Size([10, 3]), dtype=torch.float32),\n        b: TensorDict(\n            fields={\n                c: Tensor(torch.Size([10, 2]), dtype=torch.float32)},\n            batch_size=torch.Size([10]),\n            device=None,\n            is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=None,\n    is_shared=False)\n```\nWhen `i==0`, your empty tensordict will automatically be populated with empty tensors\nof batch-size `N`. After that, updates will be written in-place.\nNote that this would also work with a shuffled series of indices (pre-allocation does\nnot require you to go through the tensordict in an ordered fashion).\n\n\n### Nesting TensorDicts\n\nIt is possible to nest tensordict. The only requirement is that the sub-tensordict should be indexable\nunder the parent tensordict, i.e. its batch size should match (but could be longer than) the parent\nbatch size.\n\nWe can switch easily between hierarchical and flat representations.\nFor instance, the following code will result in a single-level tensordict with keys `\"key 1\"` and `\"key 2.sub-key\"`:\n```python\n>>> tensordict = TensorDict({\n...     \"key 1\": torch.ones(3, 4, 5),\n...     \"key 2\": TensorDict({\"sub-key\": torch.randn(3, 4, 5, 6)}, batch_size=[3, 4, 5])\n... }, batch_size=[3, 4])\n>>> tensordict_unflatten = tensordict.unflatten_keys(separator=\".\")\n```\n\nAccessing nested tensordicts can be achieved with a single index:\n```python\n>>> sub_value = tensordict[\"key 2\", \"sub-key\"]\n```\n\n# Disclaimer\n\nTensorDict is at the alpha-stage, meaning that there may be bc-breaking changes introduced at any moment without warranty.\nHopefully that should not happen too often, as the current roadmap mostly involves adding new features and building compatibility\nwith the broader pytorch ecosystem.\n\n## License\nTorchRL is licensed under the MIT License. See [LICENSE](LICENSE) for details.\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/pytorch-labs/tensordict",
            "keywords": "",
            "license": "BSD",
            "maintainer": "",
            "maintainer_email": "",
            "name": "tensordict",
            "package_url": "https://pypi.org/project/tensordict/",
            "platform": null,
            "project_url": "https://pypi.org/project/tensordict/",
            "project_urls": {
                "Homepage": "https://github.com/pytorch-labs/tensordict"
            },
            "release_url": "https://pypi.org/project/tensordict/0.0.1a0/",
            "requires_dist": [
                "torch",
                "numpy",
                "packaging",
                "cloudpickle",
                "torchsnapshot-nightly ; extra == 'checkpointing'",
                "pytest ; extra == 'tests'",
                "pyyaml ; extra == 'tests'",
                "pytest-instafail ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "",
            "version": "0.0.1a0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15757994,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "36cb65fe9479eeed61f0d41c39e7ca94",
                    "sha256": "56a52ae80dcd1ef34815340d4129ad0c697b4c0a2154df4b9b86798151d35305"
                },
                "downloads": -1,
                "filename": "tensordict-0.0.1a0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "36cb65fe9479eeed61f0d41c39e7ca94",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 78261,
                "upload_time": "2022-11-14T09:29:29",
                "upload_time_iso_8601": "2022-11-14T09:29:29.526917Z",
                "url": "https://files.pythonhosted.org/packages/ec/7e/39dae2e45ff5d10d989d18a26ff83d3f465032a6ce17c2a803b13c4f1787/tensordict-0.0.1a0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}