{
    "0.3.0": {
        "info": {
            "author": "Marco Pleines",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description": "[[Installation](#installation)]  [[Usage](#usage)] [[Mortar Mayhem](#mortar-mayhem)] [[Mystery Path](#mystery-path)] [[Searing Spotlights](#searing-spotlights)]\r\n\r\n# Memory Gym: Partially Observable Challenges for Memory-Based Agents\r\n\r\nMemory Gym features the environments **Mortar Mayhem**, **Mystery Path**, and **Searing Spotlights** that are inspired by some mini games of [Pummel Party](http://rebuiltgames.com/). These environments shall benchmark an agent's memory to\r\n- memorize events across long sequences,\r\n- generalize,\r\n- and be robust to noise.\r\n\r\n## Installation\r\n\r\nMajor dependencies:\r\n- gym==0.26.2\r\n- PyGame\r\n```console\r\nconda create -n memory-gym --python=3.7 --yes\r\ncd drl-memory-gym\r\npip install -e .\r\n```\r\n\r\n## Usage\r\n\r\nExecuting the environment using random actions:\r\n```python\r\nimport memory_gym\r\nimport gym\r\n\r\nenv = gym.make(\"SearingSpotlights-v0\")\r\n# env = gym.make(\"MortarMayhem-v0\")\r\n# env = gym.make(\"MortarMayhem-Grid-v0\")\r\n# env = gym.make(\"MortarMayhemB-v0\")\r\n# env = gym.make(\"MortarMayhemB-Grid-v0\")\r\n# env = gym.make(\"MysteryPath-v0\")\r\n# env = gym.make(\"MysteryPath-Grid-v0\")\r\n\r\n# Pass reset parameters to the environment\r\noptions = {\"agent_scale\": 0.25}\r\n\r\nobs = env.reset(options)\r\ndone = False\r\nwhile not done:\r\n    obs, reward, done, info = env.step(env.action_space.sample())\r\n\r\nprint(info)\r\n```\r\n\r\nManually play the environments using the console scripts (works only using an anaconda environment):\r\n```console\r\nmortar_mayhem\r\n# MMAct\r\nmortar_mayhem_b\r\n# MMGrid\r\nmortar_mayhem_grid\r\n# MMAct Grid\r\nmortar_mayhem_b_grid\r\nmystery_path\r\nmystery_path_grid\r\nsearing_spotlights\r\n```\r\n\r\nYou can also execute the python scripts directly, for example:\r\n```\r\npython ./memory_gym/mortar_mayhem.py\r\n```\r\n\r\n## Mortar Mayhem\r\n\r\n![Mortar Mayhem Environment](/docs/assets/mm.jpg)\r\n\r\nMortar Mayhem challenges the agent with a sequence of commands that the agent has to memorize and execute in the right order. During the beginning of the episode, each command is visualized one by one. Mortar Mayhem can be reduced to solely executing commands. In this case, the command sequence is always available as vector observation (one-hot encoded) and, therefore, is not visualized.\r\n\r\nThe max length of an episode can be calculated as follows:\r\n\r\n```\r\nmax episode length = (command_show_duration + command_show_delay) * command_count + (explosion_delay + explosion_duration) * command_count - 2\r\n```\r\n\r\n### Reset Parameters\r\n\r\n| Parameter              | Default | Description                                                                                                                                       |\r\n|------------------------|--------:|---------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| agent_scale            |    0.25 | The dimensions of the agent.                                                                                                                      |\r\n| agent_speed            |     2.5 | The speed of the agent.                                                                                                                           |\r\n| arena_size             |       5 | The grid dimension of the arena (min: 2, max: 6)                                                                                                  |\r\n| allowed_commands       |       9 | Available commands: right, down, left, up, stay, right down, right up, left down, left up. If set to five, the first five commands are available. |\r\n| command_count          |     [5] | The number of commands that are asked to be executed by the agent. This is a list that the environment samples from.                              |\r\n| command_show_duration  |     [3] | The number of steps that one command is shown. This is a list that the environment samples from.                                                  |\r\n| command_show_delay     |     [1] | The number of steps between showing one command. This is a list that the environment samples from.                                                |\r\n| explosion_duration     |     [6] | The number of steps that an agent has to stay on the commanded tile. This is a list that the environment samples form.                            |\r\n| explosion_delay        |    [18] | The entire duration in steps that the agent has to execute the current command. This is a list that the environments samples from.                |\r\n| visual_feedback        |    True | Whether to turn off the visualization of the feedback. Upon command evaluation, the wrong tiles are rendered red.                                 |\r\n| reward_command_failure |     0.0 | What reward to signal upon failing at the current command.                                                                                        |\r\n| reward_command_success |     0.1 | What reward to signal upon succeeding at the current command.                                                                                       |\r\n| reward_episode_success |     0.0 | What reward to signal if the entire command sequence is successfully solved by the agent.                                                         |\r\n\r\n## Mystery Path\r\n\r\n![Mystery Path Environment](/docs/assets/mp.jpg)\r\n\r\nMystery Path procedurally generates an invisible path for the agent to cross from the origin to the goal. Per default, only the origin of the path is visible. Upon falling off the path, the agent has to restart from the origin. Note that the episode is not terminated by falling off. Hence, the agent has to memorize where it fell off and where it did not.\r\n\r\n### Reset Parameters\r\n\r\n| Parameter              |      Default | Explanation                                                                                                                 |\r\n|------------------------|-------------:|-----------------------------------------------------------------------------------------------------------------------------|\r\n| max_steps              |          512 | The maximum number of steps for the agent to play one episode.                                                              |\r\n| agent_scale            |         0.25 | The dimensions of the agent.                                                                                                |\r\n| agent_speed            |          2.5 | The speed of the agent.                                                                                                     |\r\n| cardinal_origin_choice | [0, 1, 2, 3] | Allowed cardinal directions for the path generation to place the origin. This is a list that the environment samples from.  |\r\n| show_origin            |         True | Whether to hide or show the origin tile of the generated path.                                                              |\r\n| show_goal              |        False | Whether to hide or show the goal tile of the generated path.                                                                |\r\n| visual_feedback        |         True | Whether to visualize that the agent is off the path. A red cross is rendered on top of the agent.                           |\r\n| reward_goal            |          1.0 | What reward to signal when reaching the goal tile.                                                                          |\r\n| reward_fall_off        |          0.0 | What reward to signal when falling off.                                                                                     |\r\n| reward_path_progress   |          0.0 | What reward to signal when making progress on the path. This is only signaled for reaching another tile for the first time. |\r\n| reward_step            |          0.0 | What reward to signal for each step.                                                                                        |\r\n\r\n## Searing Spotlights\r\n\r\n![Searing Spotlights Environment](/docs/assets/spots.jpg)\r\n\r\nSearing Spotlights is a pitch black surrounding to the agent. The environment is initially fully observable but the light is dimmed untill off during the first few frames. Only randomly moving spotlights unveil information on the environment's ground truth, while posing a threat to the agent. If spotted by spotlight, the agent looses health points. While the agent must avoid closing in spotlights, it further has to collect coins. After collecting all coins, the agent has to take the environment's exit.\r\n\r\n\r\n### Reset Parameters\r\n\r\n| Parameter                | Default | Explanation                                                                                                     |\r\n|--------------------------|--------:|-----------------------------------------------------------------------------------------------------------------|\r\n| max_steps                |     512 | The maximum number of steps for the agent to play one episode.                                                  |\r\n| agent_scale              |    0.25 | The dimensions of the agent.                                                                                    |\r\n| agent_speed              |     2.5 | The speed of the agent.                                                                                         |\r\n| agent_health             |     100 | The initial health points of the agent.                                                                         |\r\n| agent_visible            |   False | Whether to make the agent permanently visible.                                                                  |\r\n| sample_agent_position    |    True | Whether to hide or show the goal tile of the generated path.                                                    |\r\n| num_coins                |     [1] | The number of coins that are spawned. This is a list that the environment samples from.                         |\r\n| coin_scale               |   0.375 | The scale of the coins.                                                                                         |\r\n| coins_visible            |   False | Whether to make the coins permanently visible.                                                                  |\r\n| use_exit                 |    True | Whether to spawn and use the exit task. The exit is accessible by the agent after collecting all coins.         |\r\n| exit_scale               |     0.0 | The scale of the exit.                                                                                          |\r\n| exit_visible             | False   | Whether to make the exit permanently visible.                                                                   |\r\n| initial_spawns           | 4       | The number of spotlights that are initially spawned.                                                            |\r\n| num_spawns               | 30      | The number of spotlights that are to be spawned.                                                                |\r\n| initial_spawn_interval   | 30      | The number of steps until the next spotlight is spawned.                                                        |\r\n| spawn_interval_threshold | 10      | The spawn interval is decayed until reaching this lower threshold.                                              |\r\n| spawn_interval_decay     | 0.95    | The decay rate of the spotlight spawn interval.                                                                 |\r\n| spot_min_radius          | 7.5     | The minimum radius of the spotlights. The radius is sampled from the range min to max.                          |\r\n| spot_max_radius          | 13.75   | The maximum radius of the spotlights. The radius is sampled from the range min to max.                          |\r\n| spot_min_speed           | 0.0025  | The minimum speed of the spotlights. The speed is sampled from the range min to max.                            |\r\n| spot_max_speed           | 0.0075  | The maximum speed of the spotlights. The speed is sampled from the range min to max.                            |\r\n| spot_damage              | 1.0     | Damage per step while the agent is spotted by one spotlight.                                                    |\r\n| light_dim_off_duration   | 6       | The number of steps to dim off the global light.                                                                |\r\n| light_threshold          | 255     | The threshold for dimming the global light. A value of 255 indicates that the light will dimmed of completely.  |\r\n| visual_feedback          | True    | Whether to render the tiled background red if the agent is spotted.                                             |\r\n| black_background         | False   | Whether to render the environments background black, while the spotlights are rendered as white circumferences. |\r\n| hide_chessboard          | False   | Whether to hide the chessboard background. This renders the background of the environment white.                           |\r\n| reward_inside_spotlight  | 0.0     | What reward to signal for each step while being inside a spotlight.                                             |\r\n| reward_outside_spotlight | 0.0     | What reward to signal for each step while being outside of a spotlight.                                         |\r\n| reward_death             | 0.0     | What reward to signal upon losing all health points.                                                            |\r\n| reward_exit              | 1.0     | What reward to signal after successfully using the exit.                                                        |\r\n| reward_max_steps         | 0.0     | What reward to signal if max steps is reached.                                                                  |\r\n| reward_coin              | 0.25    | What reward to signal upon collecting one coin.                                                                 |\r\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/MarcoMeter/drl-memory-gym",
            "keywords": "Deep Reinforcement Learning,gym,POMDP,Imperfect Information,Partial Observation",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "memory-gym",
            "package_url": "https://pypi.org/project/memory-gym/",
            "platform": null,
            "project_url": "https://pypi.org/project/memory-gym/",
            "project_urls": {
                "Bug Tracker": "https://github.com/MarcoMeter/drl-memory-gym/issues",
                "Github": "https://github.com/MarcoMeter/drl-memory-gym",
                "Homepage": "https://github.com/MarcoMeter/drl-memory-gym"
            },
            "release_url": "https://pypi.org/project/memory-gym/0.3.0/",
            "requires_dist": [
                "gym (>=0.26.2)",
                "pygame (>=2.1.2)"
            ],
            "requires_python": ">=3.6",
            "summary": "A gym that contains the memory benchmarks Mortar Mayhem, Mystery Maze and Searing Spotlights",
            "version": "0.3.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15836278,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "23202d27d3feb71a1e3006f59a4b72f5",
                    "sha256": "aeac8791a1e9f7be5e0757cdbab281dce54bb844ca0c653dbd8d311d7a48a5b6"
                },
                "downloads": -1,
                "filename": "memory_gym-0.3.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "23202d27d3feb71a1e3006f59a4b72f5",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.6",
                "size": 46580,
                "upload_time": "2022-11-21T05:24:13",
                "upload_time_iso_8601": "2022-11-21T05:24:13.592211Z",
                "url": "https://files.pythonhosted.org/packages/7e/91/c18732338c03874bca97133cbd8237f3f8ddf9f6a77494074bb9a7affa81/memory_gym-0.3.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "32e824afde0b2d024050587885d50295",
                    "sha256": "bbff5a434b6f415382ad43295ecd76957f159f5a0db292bbd5eda41f45adaf44"
                },
                "downloads": -1,
                "filename": "memory-gym-0.3.0.tar.gz",
                "has_sig": false,
                "md5_digest": "32e824afde0b2d024050587885d50295",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 28764,
                "upload_time": "2022-11-21T05:24:16",
                "upload_time_iso_8601": "2022-11-21T05:24:16.141794Z",
                "url": "https://files.pythonhosted.org/packages/be/70/94792bb3f530c1069b3f8162f4c040af54c823ae14529fb3eefde7d3a140/memory-gym-0.3.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}