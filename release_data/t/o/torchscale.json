{
    "0.1.0": {
        "info": {
            "author": "TorchScale Team",
            "author_email": "Shuming.Ma@microsoft.com",
            "bugtrack_url": null,
            "classifiers": [
                "Programming Language :: Python :: 3"
            ],
            "description": "# TorchScale - A Library for Transformers at (Any) Scale\n\n<p>\n  <a href=\"https://github.com/microsoft/torchscale/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n</p>\n\nTorchScale is a PyTorch library that allows researchers and developeres to scale up Transformers efficiently and effectively.\nIt has the implemetention of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.\n\n- Stability - [**DeepNet**](https://arxiv.org/abs/2203.00555): scaling Transformers to 1,000 Layers and beyond\n- Generality - [**Foundation Transformers (Magneto)**](https://arxiv.org/abs/2210.06423)\n- Efficiency - [**X-MoE**](https://arxiv.org/abs/2204.09179): scalable & finetunable sparse Mixture-of-Experts (MoE)\n\n## Installation\n\nTo install:\n```\npip install torchscale\n```\n\nAlternatively, you can develop it locally:\n```\ngit clone https://github.com/microsoft/torchscale.git\ncd torchscale\npip install -e .\n```\n\n## Getting Started\n\nIt takes only several lines of code to create a model with the above fundamental research features enabled. Here is how to quickly obtain a BERT-like encoder:\n\n```python\n>>> from torchscale.architecture.config import EncoderConfig\n>>> from torchscale.architecture.encoder import Encoder\n\n>>> config = EncoderConfig(vocab_size=64000)\n>>> model = Encoder(config)\n\n>>> print(model)\n```\n\nWe also support the `Decoder` architecture and the `EncoderDecoder` architecture:\n\n```python\n# Creating a decoder model\n>>> from torchscale.architecture.config import DecoderConfig\n>>> from torchscale.architecture.decoder import Decoder\n\n>>> config = DecoderConfig(vocab_size=64000)\n>>> decoder = Decoder(config)\n>>> print(decoder)\n\n# Creating a encoder-decoder model\n>>> from torchscale.architecture.config import EncoderDecoderConfig\n>>> from torchscale.architecture.encoder_decoder import EncoderDecoder\n\n>>> config = EncoderDecoderConfig(vocab_size=64000)\n>>> encdec = EncoderDecoder(config)\n>>> print(encdec)\n```\n\n## Examples\n\nWe have the examples of how to integrate TorchScale with [FairSeq](https://github.com/facebookresearch/fairseq) in the following scenarios/tasks:\n\n- [BERT-style Pretraining](examples/fairseq/README.md#example-bert-pretraining)\n\n- [Neural Machine Translation](examples/fairseq/README.md#example-machine-translation)\n\n- [BEiT-3 Pretraining](torchscale/model/BEiT3.py) [In progress]\n\nWe plan to provide more examples regarding different tasks (e.g. vision pretraining and speech recognition) and various deep learning toolkits (e.g. [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)). Any comments or PRs are welcome!\n\n## Results\n\n### Stability Evaluation\n\n<p align=\"center\">\n  <img src=\"./assets/convergence.png\" width=\"800\"/>\n</p>\n\nThe training curve is smooth by using TorchScale, while the baseline Transformer cannot converge.\n\n### Scaling-up Experiments\n\n<p align=\"center\">\n  <img src=\"./assets/scaling_curve.png\" width=\"800\"/>\n</p>\n\nTorchScale supports arbitrary depths and widths, successfully scaling-up the models without pain.\n\n## Acknowledgments\n\nSome implementations in TorchScale are either adapted from or inspired by the [FairSeq](https://github.com/facebookresearch/fairseq) repository and the [UniLM](https://github.com/microsoft/unilm) repository.\n\n## Citations\n\nIf you find this repository useful, please consider citing our work:\n\n```\n@article{deepnet,\n  author    = {Hongyu Wang and\n               Shuming Ma and\n               Li Dong and\n               Shaohan Huang and\n               Dongdong Zhang and\n               Furu Wei},\n  title     = {{DeepNet}: Scaling Transformers to 1,000 Layers},\n  journal   = {CoRR},\n  volume    = {abs/2203.00555},\n  year      = {2022},\n}\n```\n\n```\n@article{magneto,\n  author    = {Hongyu Wang and\n               Shuming Ma and\n               Shaohan Huang and\n               Li Dong and\n               Wenhui Wang and\n               Zhiliang Peng and\n               Yu Wu and\n               Payal Bajaj and\n               Saksham Singhal and\n               Alon Benhaim and\n               Barun Patra and\n               Zhun Liu and\n               Vishrav Chaudhary and\n               Xia Song and\n               Furu Wei},\n  title     = {Foundation Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2210.06423},\n  year      = {2022}\n}\n```\n\n```\n@article{xmoe,\n  author    = {Zewen Chi and\n               Li Dong and\n               Shaohan Huang and\n               Damai Dai and\n               Shuming Ma and\n               Barun Patra and\n               Saksham Singhal and\n               Payal Bajaj and\n               Xia Song and\n               Furu Wei},\n  title     = {On the Representation Collapse of Sparse Mixture of Experts},\n  journal   = {CoRR},\n  volume    = {abs/2204.09179},\n  year      = {2022}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/msranlp/torchscale",
            "keywords": "Transformers at any scale",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "torchscale",
            "package_url": "https://pypi.org/project/torchscale/",
            "platform": null,
            "project_url": "https://pypi.org/project/torchscale/",
            "project_urls": {
                "Homepage": "https://github.com/msranlp/torchscale"
            },
            "release_url": "https://pypi.org/project/torchscale/0.1.0/",
            "requires_dist": [
                "apex",
                "torch (>=1.8)",
                "fairscale (==0.4.0)",
                "timm (==0.4.12)"
            ],
            "requires_python": ">=3.8.0",
            "summary": "Transformers at any scale",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15850230,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "3a52dff62d5d72e926448ce7da97a527",
                    "sha256": "5eaefd4f4f1d686066e765eb620774f993ff219f3864669d4fede0d787bb58bb"
                },
                "downloads": -1,
                "filename": "torchscale-0.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "3a52dff62d5d72e926448ce7da97a527",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8.0",
                "size": 51834,
                "upload_time": "2022-11-22T07:04:14",
                "upload_time_iso_8601": "2022-11-22T07:04:14.217108Z",
                "url": "https://files.pythonhosted.org/packages/cd/c5/18cb7189ad3004dc9010c5f21e17a45b6f23e437a3299eb28af470be5ffe/torchscale-0.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "02a589e14a3e38d930924798eb71950b",
                    "sha256": "b4206fc4310092d5bbe9764b2aeca048191010ca4d30405dd540b5fcca68a4cd"
                },
                "downloads": -1,
                "filename": "torchscale-0.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "02a589e14a3e38d930924798eb71950b",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.8.0",
                "size": 40780,
                "upload_time": "2022-11-22T07:04:16",
                "upload_time_iso_8601": "2022-11-22T07:04:16.428081Z",
                "url": "https://files.pythonhosted.org/packages/39/75/46d81b1e5590192bb5587b562d202bff9e2ccd64a305a644e5fcc4f3e82b/torchscale-0.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}