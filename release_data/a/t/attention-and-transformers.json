{
    "0.0.1": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.1/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow (==2.10.0) ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF. https://github.com/veb-101/Attention-and-Transformers",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16124696,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "c3613d59a4186f4f13fa17dfbb08999c",
                    "sha256": "3035de547414eb7fc292a4e4e3dcb4bc4b57aa8bb544a1cd4e625eb647f23069"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "c3613d59a4186f4f13fa17dfbb08999c",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 8379,
                "upload_time": "2022-11-18T21:02:35",
                "upload_time_iso_8601": "2022-11-18T21:02:35.618551Z",
                "url": "https://files.pythonhosted.org/packages/8f/f0/8046683eb8b6738c8fafa41c8f6ab317d2331000fa5303fd713134b87ee2/Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "c8511ddbbafc8f9bd4819debf82c1bcf",
                    "sha256": "34559a79d900db5e40f3056e34a1d733d95b249bbf4f2c3d9df8c4db3e259a56"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "c8511ddbbafc8f9bd4819debf82c1bcf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 7613,
                "upload_time": "2022-11-18T21:02:37",
                "upload_time_iso_8601": "2022-11-18T21:02:37.262685Z",
                "url": "https://files.pythonhosted.org/packages/5f/11/abb921146cdd95edd31e8b51a35354a246b9f3e141ea614c8c71ffdc2a7d/Attention_and_Transformers-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.3": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.3/",
            "requires_dist": null,
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16124696,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "dea5abfb566146f2728602b0953aad59",
                    "sha256": "ec624967f2b4815fa5ea6494f44f4d4c9fbd7edcf732ff05fae5dfc57b6364ab"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.3.tar.gz",
                "has_sig": false,
                "md5_digest": "dea5abfb566146f2728602b0953aad59",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 14346,
                "upload_time": "2022-12-16T10:23:52",
                "upload_time_iso_8601": "2022-12-16T10:23:52.173061Z",
                "url": "https://files.pythonhosted.org/packages/fc/c7/7907357579e81de1bd36f118726adb56b1a9e4e1bee697249350091cd303/Attention_and_Transformers-0.0.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.4": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "## Attention mechanisms and Transformers\n\n\n\n[![Python 3.10.4](https://img.shields.io/badge/Python-3.10.4-3776AB)](https://www.python.org/downloads/release/python-3104/) [![TensorFlow 2.10.0](https://img.shields.io/badge/TensorFlow-2.10.0-FF6F00?logo=tensorflow)](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0) [![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)\n\n\n\n* This goal of this repository is to host basic architecture and model traning code associated with the different attention mechanisms and transformer architecture.\n\n* At the moment, I more interested in learning and recreating these new architectures from scratch than full-fledged training. For now, I'll just be training these models on small datasets.\n\n\n\n#### Installation\n\n\n\n* Using pip to install from [pypi](https://pypi.org/project/Attention-and-Transformers/)\n\n\n\n```bash\n\npip install Attention-and-Transformers\n\n```\n\n\n\n* Using pip to install latest version from github\n\n\n\n```bash\n\npip install git+https://github.com/veb-101/Attention-and-Transformers.git\n\n```\n\n\n\n* Local clone and install\n\n\n\n```bash\n\ngit clone https://github.com/veb-101/Attention-and-Transformers.git atf\n\ncd atf\n\npython setup.py install\n\n```\n\n\n\n**Test Installation**\n\n\n\n```bash\n\npython load_test.py\n\n```\n\n\n\n**Attention Mechanisms**\n\n\n\n<table>\n\n<thead>\n\n<tr>\n\n<th style=\"text-align:center\">\n\n<strong># No.</strong>\n\n</th>\n\n<th style=\"text-align:center\">\n\n<strong>Mechanism</strong>\n\n</th>\n\n<th style=\"text-align:center\">\n\n<strong>Paper</strong>\n\n</th>\n\n</tr>\n\n</thead>\n\n<tbody>\n\n\n\n<tr>\n\n<td style=\"text-align:center\">1</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/VisionTransformers/multihead_self_attention.py\">Multi-head Self Attention</a>\n\n</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:center\">2</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/multihead_self_attention_2D.py\">Multi-head Self Attention 2D</a>\n\n</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT V1</a>\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:center\">2</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v2/linear_attention.py\">Separable Self Attention</a>\n\n</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/2206.02680\">MobileViT V2</a>\n\n</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\n\n\n**Transformer Models**\n\n\n\n<table>\n\n<thead>\n\n<tr>\n\n<th style=\"text-align:center\">\n\n<strong># No.</strong>\n\n</th>\n\n<th style=\"text-align:center\">\n\n<strong>Models</strong>\n\n</th>\n\n<th style=\"text-align:center\">\n\n<strong>Paper</strong>\n\n</th>\n\n</tr>\n\n</thead>\n\n<tbody>\n\n<tr>\n\n<td style=\"text-align:center\">1</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/VisionTransformers/vision_transformer.py\">Vision Transformer</a>\n\n</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words:</a>\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:center\">2</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/mobile_vit_v1.py\">MobileViT-V1</a>\n\n</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a>\n\n</td>\n\n</tr>\n\n<tr>\n\n<td style=\"text-align:center\">3</td>\n\n<td style=\"text-align:center\">MobileViT-V2 (under development)</td>\n\n<td style=\"text-align:center\">\n\n<a href=\"https://arxiv.org/abs/2206.02680\">Separable Self-attention for Mobile Vision Transformers</a>\n\n</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\n\n\n<!-- **Attention Mechanisms**\n\n\n\n|:---------:|:----------------------------:|:-------------------------------------------------------------:|\n\n| 1         | [Multi-head Self Attention](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT-v1/multihead_self_attention_2D.py)    | [Attention is all you need](https://arxiv.org/abs/1706.03762) |\n\n| 2         | [Multi-head Self Attention 2D](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT_v1/multihead_self_attention_2D.py) | [MobileViT V1](https://arxiv.org/abs/2110.02178)              |\n\n\n\n**Transformer Models**\n\n\n\n| **# No.** | **Models**         | **Paper**                                                          |\n\n|:---------:|:------------------:|:------------------------------------------------------------------:|\n\n| 1         | [Vision Transformer](https://github.com/veb-101/Attention-and-Transformers/blob/main/VisionTransformers/vision_transformer.py) | [An Image is Worth 16x16 Words:](https://arxiv.org/abs/2010.11929) |\n\n| 2         | [MobileViT-V1](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT_v1/mobile_vit.py)     | [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)                   |\n\n| 3         | MobileViT-V2 (under development)| [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680)                   | -->\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.4/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow (==2.10.0) ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.4",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16124696,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "623ae09ecea23deadd4984d276f045ea",
                    "sha256": "9e7f2b06608123826cb4dd3b99f0e11c117e042fa2084ced19597e43a14d51d1"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.4-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "623ae09ecea23deadd4984d276f045ea",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 21521,
                "upload_time": "2022-12-16T10:26:55",
                "upload_time_iso_8601": "2022-12-16T10:26:55.046699Z",
                "url": "https://files.pythonhosted.org/packages/da/7f/6d90b9c3ef1ac1418bd6d369508b5a87c56dc327050090cbbd83cb524600/Attention_and_Transformers-0.0.4-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "b2bf390fab5671b187dd4dcb583bd2ba",
                    "sha256": "05277b4e479aeb2667715c573adc62b066238973abb2ec600e909cd6ee0a53c4"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.4.tar.gz",
                "has_sig": false,
                "md5_digest": "b2bf390fab5671b187dd4dcb583bd2ba",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 14299,
                "upload_time": "2022-12-16T10:26:56",
                "upload_time_iso_8601": "2022-12-16T10:26:56.579546Z",
                "url": "https://files.pythonhosted.org/packages/ce/60/23491f05d18e95266a188036f9a3c98992d051bc7b798f3314f6ff7a80cc/Attention_and_Transformers-0.0.4.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}