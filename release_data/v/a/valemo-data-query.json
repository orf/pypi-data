{
    "1.0.1": {
        "info": {
            "author": "Arthur_LAFFARGUE",
            "author_email": "arthur.laffargue@valemo.fr",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# VALEMO_DATA_QUERY\n## _Outils de requ\u00eates de donn\u00e9es sur Python pour Valemo_\n\n## Fonctionnalit\u00e9s\n\n#### azure_datalake_gen2\n\n- Rechercher les fichiers .csv dans une architecture partitionn\u00e9e du datalake gen2 ; \n- Filtrer les partitions et s\u00e9lectionner les dossiers ; \n- T\u00e9l\u00e9charger les .csv depuis le datalake et les agr\u00e9ger en dataframes Pandas ; \n- Transformer les donn\u00e9es et pr\u00e9parer des analyses ; \n\n#### user_credentials\n\n- enregistrer vos informations de connexion et les importer facilement ;\n \n## Installation\nInstaller _valemo-data-query_ via command prompt\n```sh\npip install --upgrade valemo-data-query\n```\n\nD\u00e9sinstaller _valemo-data-query_ via command prompt\n```sh\npip uninstall valemo-data-query\n```\n\n_valemo-data-query_ requiert : \n- Un abonnement Azure. Consultez la page d'obtention d\u2019un essai gratuit d\u2019[Azure](https://azure.microsoft.com/fr-fr/free/) ;\n- Un compte de stockage dot\u00e9 d\u2019un espace de noms hi\u00e9rarchique activ\u00e9. Pour cr\u00e9er un test, suivez [ces](https://docs.microsoft.com/fr-fr/azure/storage/blobs/create-data-lake-storage-account) instructions ; \n- Pour plus d'informations sur la gestion de datalake gen2 de microsoft via Python voir cette [page](https://docs.microsoft.com/fr-fr/azure/storage/blobs/data-lake-storage-directory-file-acl-python) ; \n- Installation \u00e0 jour de Python 3.8 ou sup\u00e9rieure. Distribution [Anaconda](https://www.anaconda.com/products/individual) recommand\u00e9e. ; \n- Librairie Pandas version 1.1.4 ou sup\u00e9rieure ; \n- Librairie Numpy version 1.21.3 ou sup\u00e9rieure ;\n- Librairie Azure (azure-storage-file-datalake) version 12.5.0 ;  \n\n## Exemple : enregistrer ses connexions \n```python\nfrom valemo_data_query import user_credentials\n\ncredentials = user_credentials()\n\n# D\u00e9clarer une nouvelle connexion \ndatalake_1 = {\"account\" : \"datalake_name\",\n                \"key\" : \"password\"}\ncredentials.add_datasource(datalake_1 ,\"datalake_1\")\n\ndatalake_2 = {\"account\" : \"datalake_name_2\",\n                \"key\" : \"password\"}\ncredentials.add_datasource(datalake_2 ,\"datalake_2\")\n\n# R\u00e9cup\u00e9rer une connexion \ndataconnexion = credentials.get_credentials\ndatalake_1 = dataconnexion[\"datalake_1\"]\n\n#Supprimer toutes les connexions \ncredentials.clear_credentials()\n\n```\n\n## Exemple : Azure datalake - g\u00e9rer les partitions\nApr\u00e8s avoir configurer ses connexions au datalake Azure gen2, la fonction _read_csv_partition_ reconstruit l'arborescence des fichiers csv sans lire le contenu. Lors de cette phase des filtres peuvent \u00eatre appliqu\u00e9s bas\u00e9s sur la reconnaissance des hi\u00e9rarchies _projet=_, _year=_ et _month=_. La m\u00e9thode statique _get_file_partition_df_ renvoie les partitions des fichiers csv. Un deuxi\u00e8me filtre bas\u00e9 sur ce dataframe est applicable avec _apply_user_partition_filter_ avant la lecture  et la concat\u00e9nation des donn\u00e9es via la m\u00e9thode _concatenate_pandas_.\n\n```python\nfrom valemo_data_query import azure_datalake_gen2, user_credentials\ndatalake_connexion = user_credentials().get_credentials[\"datalake_1\"]\n\n\"\"\"\nOn recherche les fichiers csv dans le dossier : \n    \"detailed/wind/metrics/csv/project=X/year=Y/month=Z/...\"\n    Avec X,Y,Z : \n     X = [\"BAM\",\"DSN\",\"PUDP\",\"PAACS\"]\n     Y = [\"2021\",\"2022\"]\n     Z : Mois [de 06 \u00e0 10]\n\"\"\"\nbasepath = \"detailed/wind/metrics/csv/\"\nazure_query = azure_datalake_gen2(datalake_connexion,basepath)\nazure_query.filter_partitionProject([\"BAM\",\"DSN\",\"PUDP\",\"PAACS\"])\nazure_query.filter_partitionYear([\"2021\",\"2022\"])\n# Pour les mois on pourrait sp\u00e9cifier le filtre via .filter_partitionMonth\n# ou utiliser .apply_user_partition_filter bas\u00e9 sur le dataframe des \n# partitions renvoy\u00e9 par la m\u00e9thode .get_file_partition_df\n\nazure_query.read_csv_partition() #recherche de l'arborescence des fichiers csv\npartition_df = azure_query.get_file_partition_df\n\"\"\"\n    filepath               partition  ...                              year       month\n0   detailed/wind/metr...  project=BAM/year=2021/month=10/day=01  ...  year=2021  month=10\n1   detailed/wind/metr...  project=BAM/year=2021/month=10/day=02  ...  year=2021  month=10\n2   detailed/wind/metr...  project=BAM/year=2021/month=10/day=03  ...  year=2021  month=10\n3   detailed/wind/metr...  project=BAM/year=2021/month=10/day=04  ...  year=2021  month=10\n\"\"\"\nmonth_number = partition_df[\"month\"].str.replace(\"month=\",\"\").astype(int) \nfilter_month = (month_number>=6)&(month_number<=10)\nazure_query.apply_user_partition_filter(filter_month)\n\nazure_query.concatenate_pandas(aggregation_level=['year','month']) #concat\u00e9ner par mois\nazure_query.write_concatenated_dataset_csv(\"my/local_path/\",\n                                        file_prefixe=\"Wind\")\n# g\u00e9n\u00e8re plusieurs fichiers \n# CONCAT DATASET ... \n# my/local_path/Wind_year=2021_month=06.csv\n# my/local_path/Wind_year=2021_month=07.csv\n# ... \n```\n\n## Exemple : Azure datalake - retravailler les donn\u00e9es\nLorsque l'arborescence des fichiers ne n\u00e9cessite pas de filtre via _apply_user_partition_filter_, il est possible de rechercher les partitions et lire les fichiers csv trouv\u00e9s dynamiquement en utilisant _concatenate_pandas_ sans _read_csv_partition_. Les fonctions _select_columns_ et _dataframe_transformation_ permettent de g\u00e9rer les dataframes lus de mani\u00e8re dynamique \u00e0 chaque lecture de fichiers. \nIl est possible de diff\u00e9rer ces op\u00e9rations apr\u00e8s la lecture des donn\u00e9es (apr\u00e8s concatenate_pandas). L'option _return_copy_ renvoie une copie des transformations appliqu\u00e9es en diff\u00e9r\u00e9.  \n```python\nfrom valemo_data_query import azure_datalake_gen2, user_credentials\nimport pandas as pd\ndatalake_connexion = user_credentials().get_credentials[\"datalake_1\"]\n\n\nbasepath = \"consolidated/pv/metrics/subs/csv/\"\nprojets = [\"Project1\",\"Project2\"]\nannees = [\"2021\"]\nmois = [\"09\",\"10\"]\nquery_azure = azure_datalake_gen2(credentials_dict,basepath)\n\n# Filtrage\nquery_azure.filter_partitionProject(projets)\nquery_azure.filter_partitionYear(annees)\nquery_azure.filter_partitionMonth(mois)\n\n\n#Transformation des donn\u00e9es et s\u00e9lection\n\nselect_col = [\"projectCode\",\"localTimestamp\",\"activePower\",\"gii\"]\n\ndef daily_metrics_func(data:pd.DataFrame):\n    \"\"\"\n    agr\u00e9gation des donn\u00e9es par jour, par parc\n    \"\"\"\n    data[\"day\"] = data[\"localTimestamp\"].astype(\"datetime64[D]\")\n    aggdata = data.drop(columns='localTimestamp').\\\n                   groupby([\"projectCode\",\"day\"]).\\\n                   sum().\\\n                   rename(columns={\"activePower\":\"activeEnergy\",\n                                   \"irradiance\":\"irradianceEnergy\"}).\\\n                   reset_index(drop=False)\n    aggdata[[\"activeEnergy\",\n            \"irradianceEnergy\"]] = aggdata[[\"activeEnergy\",\n                                            \"irradianceEnergy\"]]/(1000*6)\n    return aggdata\n\n#Les transformations + s\u00e9lections sont appliqu\u00e9es \u00e0 la lecture. \nquery_azure.select_columns(select_col)\nquery_azure.dataframe_transformation(daily_metrics_func)\n\n\n#Lecture des fichiers\n\nquery_azure.concatenate_pandas(aggregation_level=['project','year',\"month\"],\n                                convert_dtypes=True)\n\nquery_azure.write_concatenated_dataset_csv(\"dataset/\")\n\n\n\n```\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "GNU",
            "maintainer": "",
            "maintainer_email": "",
            "name": "valemo-data-query",
            "package_url": "https://pypi.org/project/valemo-data-query/",
            "platform": "",
            "project_url": "https://pypi.org/project/valemo-data-query/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/valemo-data-query/1.0.1/",
            "requires_dist": [
                "pandas",
                "azure-storage-file-datalake",
                "numpy",
                "parquet"
            ],
            "requires_python": "",
            "summary": "",
            "version": "1.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12046558,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "e2bac3dae8b895ce29471e56c1ab64ce",
                    "sha256": "cb07cd97cca4fe405a6319658a92f6467bd7258b28195625179fc137075c26c9"
                },
                "downloads": -1,
                "filename": "valemo_data_query-1.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "e2bac3dae8b895ce29471e56c1ab64ce",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 23373,
                "upload_time": "2021-11-17T09:35:52",
                "upload_time_iso_8601": "2021-11-17T09:35:52.871268Z",
                "url": "https://files.pythonhosted.org/packages/5b/44/35f630617d33993c9cd9295d9032c3cff242710caafcf1031a481aa6fc2a/valemo_data_query-1.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}