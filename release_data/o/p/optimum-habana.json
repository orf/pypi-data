{
    "1.0.1": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/habana",
            "keywords": "transformers,quantization,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.0.1/",
            "requires_dist": [
                "transformers (==4.18.0)",
                "optimum",
                "datasets",
                "tokenizers",
                "torch",
                "sentencepiece",
                "scipy",
                "pillow",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "1eccf19f5eea20d6282fc551936db30a",
                    "sha256": "93626c83d2c45d5358a74755d1bbf10a38dffa8fb191a6cac3227cc009324459"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "1eccf19f5eea20d6282fc551936db30a",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 31825,
                "upload_time": "2022-04-26T10:15:32",
                "upload_time_iso_8601": "2022-04-26T10:15:32.307260Z",
                "url": "https://files.pythonhosted.org/packages/ce/dd/1fc0859462a585791fe842af7c324cb4745a45b34702f76ef265df2935d8/optimum_habana-1.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "91b71a6ec5e0d99847e4517f213a645f",
                    "sha256": "0919380b0a2e2dc1544e6c30a5f9719415de5f9b885892bf116dbe05756ea313"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "91b71a6ec5e0d99847e4517f213a645f",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 31092,
                "upload_time": "2022-04-26T10:15:33",
                "upload_time_iso_8601": "2022-04-26T10:15:33.820426Z",
                "url": "https://files.pythonhosted.org/packages/a8/44/1bf236a7659ac24867ebcc2beeac8fc140249c1e7a104188646db813a43a/optimum-habana-1.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.1.0": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.1.0/",
            "requires_dist": [
                "transformers (>=4.20.0)",
                "optimum",
                "datasets",
                "tokenizers",
                "torch",
                "sentencepiece",
                "scipy",
                "pillow",
                "dill (<0.3.5)",
                "multiprocess (<0.70.13)",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "ec07adab67effb90efa4175941a4b4c2",
                    "sha256": "c5be69af2c6466cc8ce4d3e86fcd1e19d4ba41b8183ce4ecc86f68a30bc6139f"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "ec07adab67effb90efa4175941a4b4c2",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 58662,
                "upload_time": "2022-07-15T10:30:17",
                "upload_time_iso_8601": "2022-07-15T10:30:17.809179Z",
                "url": "https://files.pythonhosted.org/packages/7d/5d/77b3cf3ea7b5be9a7594b5b15b47067a5ee84305d3ea971bd74006e93f38/optimum_habana-1.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "63a52d39cd55a372fe315a3f9105cafa",
                    "sha256": "42d2ab34c1b5fd2802881b896b1c4824df91e80ed22ba67e85b424a3449ea6ca"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "63a52d39cd55a372fe315a3f9105cafa",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 53150,
                "upload_time": "2022-07-15T10:30:19",
                "upload_time_iso_8601": "2022-07-15T10:30:19.530352Z",
                "url": "https://files.pythonhosted.org/packages/dc/e6/b29cdd5ef242239239fef927e56aec34757835dfda125eb075a255cdf9de/optimum-habana-1.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.1.1": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.1.1/",
            "requires_dist": [
                "transformers (>=4.20.0)",
                "optimum",
                "datasets",
                "tokenizers",
                "torch",
                "sentencepiece",
                "scipy",
                "pillow",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.1.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f41ad71f40e89ea405dabfc3613f9c6d",
                    "sha256": "7fc4bb7f7d1b8ec21b789811edc0f7d6041cae03077dc97e365aa8c3772f8a4c"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.1.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "f41ad71f40e89ea405dabfc3613f9c6d",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 61002,
                "upload_time": "2022-08-02T07:40:37",
                "upload_time_iso_8601": "2022-08-02T07:40:37.539780Z",
                "url": "https://files.pythonhosted.org/packages/32/a7/aeca1e7298b5b8a58722fc8d11bee648478440c793bf3770c38825a2a302/optimum_habana-1.1.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "2beca795d7d91dca1bbf4c0d275d0cdf",
                    "sha256": "dd5c93dad5b035bb08f52a0ae70c194be67a5db29cd6ae4c27100321e8dc6120"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.1.1.tar.gz",
                "has_sig": false,
                "md5_digest": "2beca795d7d91dca1bbf4c0d275d0cdf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 55175,
                "upload_time": "2022-08-02T07:40:39",
                "upload_time_iso_8601": "2022-08-02T07:40:39.210110Z",
                "url": "https://files.pythonhosted.org/packages/0c/8c/eb1c44b48be1a8cdeceb18b0429fd1ebf6ceb95d6bbb6d4eab31448f9eba/optimum-habana-1.1.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.1.2": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.1.2/",
            "requires_dist": [
                "transformers (>=4.20.0)",
                "optimum",
                "datasets",
                "tokenizers",
                "torch",
                "sentencepiece",
                "scipy",
                "pillow",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and fine-tuning on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.1.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "6d6ace656ab7d84a156fb119d4b2f65b",
                    "sha256": "15b6bbb00162923f86fecc55587c1972e79ef8506493f0dae5238c08816dbf8e"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.1.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "6d6ace656ab7d84a156fb119d4b2f65b",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 74065,
                "upload_time": "2022-08-12T07:46:52",
                "upload_time_iso_8601": "2022-08-12T07:46:52.000070Z",
                "url": "https://files.pythonhosted.org/packages/98/a3/9b9227eea98d0094ec0c8e84d5473fa1e515f8cefab9248fec55a0260afa/optimum_habana-1.1.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "724fc14307fb73e033e959a5869d2909",
                    "sha256": "bfe02df164de0250c0e75336d9bb1f92e674e9724e03750c94f0d1e508d64dc8"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.1.2.tar.gz",
                "has_sig": false,
                "md5_digest": "724fc14307fb73e033e959a5869d2909",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 67762,
                "upload_time": "2022-08-12T07:46:54",
                "upload_time_iso_8601": "2022-08-12T07:46:54.095672Z",
                "url": "https://files.pythonhosted.org/packages/6c/13/5b5e623688df5e63bce0b23e54484b138bb9f90c85c84edf2eb7608060b9/optimum-habana-1.1.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.0": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.2.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.2.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "2206be4c2b52b916f8a16368f178508e",
                    "sha256": "654bc062e8ddfec79c848199d37b24711710806bf47ed78e0794a3e4700b199b"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.2.0.tar.gz",
                "has_sig": false,
                "md5_digest": "2206be4c2b52b916f8a16368f178508e",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 62509,
                "upload_time": "2022-09-12T08:44:19",
                "upload_time_iso_8601": "2022-09-12T08:44:19.501730Z",
                "url": "https://files.pythonhosted.org/packages/b5/a0/10401f2bc7f2e1c9e033bfc03d773b740fd70c7037e9439ad1041ac47392/optimum-habana-1.2.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.1": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.2.1/",
            "requires_dist": [
                "transformers (<4.22,>=4.21)",
                "optimum",
                "datasets",
                "tokenizers",
                "torch",
                "sentencepiece",
                "scipy",
                "pillow",
                "accelerate",
                "black ; extra == 'quality'",
                "isort ; extra == 'quality'",
                "hf-doc-builder ; extra == 'quality'",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.2.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "b100704cdfac8784f1b2f8517ddc65fa",
                    "sha256": "2ad642b710b3365b4c19ef09e8c0495ae20d13be35bd6726b125f753a72d9df9"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.2.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "b100704cdfac8784f1b2f8517ddc65fa",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 68828,
                "upload_time": "2022-09-12T08:56:24",
                "upload_time_iso_8601": "2022-09-12T08:56:24.902378Z",
                "url": "https://files.pythonhosted.org/packages/26/3b/477dd3a9cec83d3967e86cec4816537d1781dd8608b8f83de7dd70ced950/optimum_habana-1.2.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "e1f50643fa153ddf1cc3e0a05946d235",
                    "sha256": "7db41a131e80297c49a928b22ee37a1179356c2f478162bba08eeed34124c780"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.2.1.tar.gz",
                "has_sig": false,
                "md5_digest": "e1f50643fa153ddf1cc3e0a05946d235",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 62369,
                "upload_time": "2022-09-12T08:56:26",
                "upload_time_iso_8601": "2022-09-12T08:56:26.845178Z",
                "url": "https://files.pythonhosted.org/packages/55/d9/41502eb1c4aa28046895872bad7dc8b84a4ac9c50bf03f1b6f7f5a005361/optimum-habana-1.2.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.2": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.2.2/",
            "requires_dist": [
                "transformers (<4.23,>=4.22)",
                "optimum",
                "torch",
                "accelerate",
                "black ; extra == 'quality'",
                "isort ; extra == 'quality'",
                "hf-doc-builder ; extra == 'quality'",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'",
                "sentencepiece ; extra == 'tests'",
                "datasets ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.2.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "32dd234cf205982fcd64e2e3242f1fad",
                    "sha256": "f8eaa762b4e6ead2e3c51319855d5731afb213091c8d082a3e4b3d4cd31a64eb"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.2.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "32dd234cf205982fcd64e2e3242f1fad",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 69440,
                "upload_time": "2022-10-02T20:09:19",
                "upload_time_iso_8601": "2022-10-02T20:09:19.986764Z",
                "url": "https://files.pythonhosted.org/packages/87/99/1a7d714f2f3ca038c3bef62a72be832862110aba5332da13b664007c6299/optimum_habana-1.2.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "fc77357e38f66836785d70d02ea9d512",
                    "sha256": "736ea38b461095ccf3cf1ca9a76b5aab619a3ea11ad1517631ef76a1d93d0016"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.2.2.tar.gz",
                "has_sig": false,
                "md5_digest": "fc77357e38f66836785d70d02ea9d512",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 62911,
                "upload_time": "2022-10-02T20:09:21",
                "upload_time_iso_8601": "2022-10-02T20:09:21.968313Z",
                "url": "https://files.pythonhosted.org/packages/39/42/05475557fea98b96b618e1a740e04ef787a5495c91264f7dfb62dc4122d6/optimum-habana-1.2.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.3": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "<!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n![](https://github.com/huggingface/optimum-habana/blob/main/readme_logo.png)\n\n\n# Optimum Habana\n\n\ud83e\udd17 Optimum Habana is the interface between the \ud83e\udd17 Transformers library and [Habana's Gaudi processor (HPU)](https://docs.habana.ai/en/latest/index.html).\nIt provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.\nThe list of officially validated models and tasks is available [here](https://github.com/huggingface/optimum-habana#validated-models). Users can try other models and tasks with only few changes.\n\n\n## What is a Habana Processing Unit (HPU)?\n\nQuote from the Hugging Face [blog post](https://huggingface.co/blog/habana):\n\n> Habana Gaudi training solutions, which power Amazon\u2019s EC2 DL1 instances and Supermicro\u2019s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habana\u2019s SynapseAI\u00ae is optimized\u2014at inception\u2014to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications.\n\n\n## Install\nTo install the latest release of this package:\n\n```bash\npip install optimum[habana]\n```\n\n> To use DeepSpeed on HPUs, you also need to run the following command:\n>```bash\n>pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.6.1\n>```\n\nOptimum Habana is a fast-moving project, and you may want to install it from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum-habana.git\n```\n\n> Alternatively, you can install the package without pip as follows:\n> ```bash\n> git clone https://github.com/huggingface/optimum-habana.git\n> cd optimum-habana\n> python setup.py install\n> ```\n\nLast but not least, don't forget to install requirements for every example:\n\n```bash\ncd <example-folder>\npip install -r requirements.txt\n```\n\n\n## How to use it?\n\n\ud83e\udd17 Optimum Habana was designed with one goal in mind: **make training and evaluation straightforward for any \ud83e\udd17 Transformers user while leveraging the complete power of Gaudi processors**.\nThere are two main classes one needs to know:\n- GaudiTrainer: the trainer class that takes care of compiling (lazy or eager mode) and distributing the model to run on HPUs, and of performing traning and evaluation.\n- GaudiConfig: the class that enables to configure Habana Mixed Precision and to decide whether optimized operators and optimizers should be used or not.\n\nThe `GaudiTrainer` is very similar to the [\ud83e\udd17 Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the Trainer to make it work with Gaudi will mostly consist in simply swapping the `Trainer` class for the `GaudiTrainer` one.\nThat's how most of the [example scripts](https://github.com/huggingface/optimum-habana/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).\n\nOriginal script:\n```python\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n  # training arguments...\n)\n\n# A lot of code here\n\n# Initialize our Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,  # Original training arguments.\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n```\n\n\nTransformed version that can run on Gaudi:\n```python\nfrom optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments\n\ntraining_args = GaudiTrainingArguments(\n  # same training arguments...\n  use_habana=True,\n  use_lazy_mode=True,  # whether to use lazy or eager mode\n  gaudi_config_name=path_to_gaudi_config,\n)\n\n# A lot of the same code as the original script here\n\n# Initialize our Trainer\ntrainer = GaudiTrainer(\n    model=model,\n    # You can manually specify the Gaudi configuration to use with\n    # gaudi_config=my_gaudi_config\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n```\n\nwhere `gaudi_config_name` is the name of a model from the [Hub](https://huggingface.co/Habana) (Gaudi configurations are stored in model repositories). You can also give the path to a custom Gaudi configuration written in a JSON file such as this one:\n```json\n{\n  \"use_habana_mixed_precision\": true,\n  \"hmp_opt_level\": \"O1\",\n  \"hmp_is_verbose\": false,\n  \"use_fused_adam\": true,\n  \"use_fused_clip_norm\": true,\n  \"hmp_bf16_ops\": [\n    \"add\",\n    \"addmm\",\n    \"bmm\",\n    \"div\",\n    \"dropout\",\n    \"gelu\",\n    \"iadd\",\n    \"linear\",\n    \"layer_norm\",\n    \"matmul\",\n    \"mm\",\n    \"rsub\",\n    \"softmax\",\n    \"truediv\"\n  ],\n  \"hmp_fp32_ops\": [\n    \"embedding\",\n    \"nll_loss\",\n    \"log_softmax\"\n  ]\n}\n```\n\nIf you prefer to instantiate a Gaudi configuration to work on it before giving it to the trainer, you can do it as follows:\n```python\ngaudi_config = GaudiConfig.from_pretrained(\n    gaudi_config_name,\n    cache_dir=model_args.cache_dir,\n    revision=model_args.model_revision,\n    use_auth_token=True if model_args.use_auth_token else None,\n)\n```\n\n\n## Validated Models\n\nThe following model architectures, tasks and device distributions have been validated for \ud83e\udd17 Optimum Habana:\n|            | Text Classification | Question Answering | Language Modeling  | Summarization      | Translation        | Image Classification | Single Card        | Multi Card         | DeepSpeed          |\n|------------|:-------------------:|:------------------:|:------------------:|:------------------:|:-----------------:|:--------------------:|:------------------:|:------------------:|:------------------:|\n| BERT       | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: | \u2717                  | \u2717                  | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| RoBERTa    | \u2717                   | :heavy_check_mark: | :heavy_check_mark: | \u2717                  | \u2717                  | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| ALBERT     | \u2717                   | :heavy_check_mark: | :heavy_check_mark: | \u2717                  | \u2717                  | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| DistilBERT | \u2717                   | :heavy_check_mark: | :heavy_check_mark: | \u2717                  | \u2717                  | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| GPT2       | \u2717                   | \u2717                  | :heavy_check_mark: | \u2717                  | \u2717                  | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| T5         | \u2717                   | \u2717                  | \u2717                  | :heavy_check_mark: | :heavy_check_mark: | \u2717                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| ViT        | \u2717                   | \u2717                  | \u2717                  | \u2717                  | \u2717                  | :heavy_check_mark:   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| Swin       | \u2717                   | \u2717                  | \u2717                  | \u2717                  | \u2717                  | :heavy_check_mark:   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n\nOther models and tasks supported by the \ud83e\udd17 Transformers library may also work. You can refer to this [section](https://github.com/huggingface/optimum-habana#how-to-use-it) for using them with \ud83e\udd17 Optimum Habana. Besides, [this page](https://github.com/huggingface/optimum-habana/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the \ud83e\udd17 Transformers library to make it work with \ud83e\udd17 Optimum Habana.\n\nIf you find any issue while using those, please open an issue or a pull request.\n\n\n## Gaudi Setup\n\nPlease refer to Habana Gaudi's official [installation guide](https://docs.habana.ai/en/latest/Installation_Guide/index.html).\n\n> Tests should be run in a Docker container based on Habana Docker images.\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://huggingface.co/hardware/habana",
            "keywords": "transformers,mixed-precision training,fine-tuning,gaudi,hpu",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-habana",
            "package_url": "https://pypi.org/project/optimum-habana/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-habana/",
            "project_urls": {
                "Homepage": "https://huggingface.co/hardware/habana"
            },
            "release_url": "https://pypi.org/project/optimum-habana/1.2.3/",
            "requires_dist": [
                "transformers (<4.24,>=4.23)",
                "optimum",
                "torch",
                "accelerate",
                "black ; extra == 'quality'",
                "isort ; extra == 'quality'",
                "hf-doc-builder ; extra == 'quality'",
                "pytest ; extra == 'tests'",
                "psutil ; extra == 'tests'",
                "parameterized ; extra == 'tests'",
                "GitPython ; extra == 'tests'",
                "optuna ; extra == 'tests'",
                "sentencepiece ; extra == 'tests'",
                "datasets ; extra == 'tests'"
            ],
            "requires_python": "",
            "summary": "Optimum Habana is the interface between the Hugging Face Transformers library and Habana Gaudi Processor (HPU). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.",
            "version": "1.2.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15395990,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "1ba15fe0b24d2545eff79da309b73d53",
                    "sha256": "2b579a067f5ab20dceba6cfeba7d250e94e945defe0c63a71f83f813653defd4"
                },
                "downloads": -1,
                "filename": "optimum_habana-1.2.3-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "1ba15fe0b24d2545eff79da309b73d53",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 69973,
                "upload_time": "2022-10-13T08:04:38",
                "upload_time_iso_8601": "2022-10-13T08:04:38.135456Z",
                "url": "https://files.pythonhosted.org/packages/56/e7/2a977547959256bcfa9986d1cccb781585c0c97e6e8b0a24b43a8e26adad/optimum_habana-1.2.3-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "61d4b5432cf9e693c44188e81d296934",
                    "sha256": "b4ff6a89cbd58939b0db9b365e1aad58cb6a914963e83413231c16ca8a9afa99"
                },
                "downloads": -1,
                "filename": "optimum-habana-1.2.3.tar.gz",
                "has_sig": false,
                "md5_digest": "61d4b5432cf9e693c44188e81d296934",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 63471,
                "upload_time": "2022-10-13T08:04:39",
                "upload_time_iso_8601": "2022-10-13T08:04:39.874180Z",
                "url": "https://files.pythonhosted.org/packages/6d/0c/6e2ae53fe7280a4fa4009abbabe97c1b8e59dd4f5f2ac22e5a801b017188/optimum-habana-1.2.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}