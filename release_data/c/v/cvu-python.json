{
    "0.0.1a1": {
        "info": {
            "author": "BlueMirrors",
            "author_email": "contact.bluemirrors@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9"
            ],
            "description": "# <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"30\"> CVU: Computer Vision Utils\n\n[![CodeFactor](https://www.codefactor.io/repository/github/bluemirrors/cvu/badge?s=700eb6a402321377322a7f4c15ebf99055e0c299)](https://www.codefactor.io/repository/github/bluemirrors/cvu) [![stability-alpha](https://img.shields.io/badge/stability-alpha-f4d03f.svg)](https://github.com/mkenney/software-guides/blob/master/STABILITY-BADGES.md#alpha)\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) [![Downloads](https://pepy.tech/badge/cvu-python)](https://pepy.tech/project/cvu-python)\n<br><br>\n\nComputer Vision deployment tools for dummies and experts.<br>\n\n```bash\npip install cvu-python\n```\n\n<!-- <br>\n\n\u2728\u2728\nCVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"12\"> is participating in \ud83d\ude80 [Yolov5's export competition](https://github.com/ultralytics/yolov5/discussions/3213) \ud83d\ude80. Please checkout our and other's great submissions, and maybe consider voting!\u2728\u2728\n<br> -->\n\n# Index \ud83d\udccb\n\n- [Getting Started](#cvu-says-hi-)\n- [What and why is CVU?](#why-cvu)\n- [Object Detection (Yolov5)](#yolov5-object-detection)\n  - [TensorRT](#tensorrt)\n  - [Torch](#torch)\n  - [ONNX](#onnx)\n  - [TensorFlow](#tensorflow)\n  - [TFLite](#tflite)\n- [Devices (CPU, GPU, TPU)](#devices)\n- [Benchmark-Tool (Yolov5)](#benchmarks-yolov5)\n- [Benchmarks Results (Yolov5)](#yolov5-benchmark-results)\n- [Precission Accuracy (Yolov5)](#precission-accuracy-yolov5))\n- [Examples](https://github.com/BlueMirrors/cvu/tree/master/examples)\n- [References](#references)\n\n<br>\n\n# CVU Says Hi <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"25\">!\n\n[Index](#index-)\n\nWhether you are developing an optimized computer vision pipeline or just looking to use some quick computer vision in your project, CVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"12\"> can help! Designed to be used by both the experts and the novice, CVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"12\"> aims at making CV pipelines easier to build and consistent around platforms, devices and models.<br><br>\n\nFor example, how much installation-steps and code would you need to run object detection on a video with a TensorRT backend? How complicated can it be to test that pipeline out in Colab?<br><br>\n\nWith CVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"12\">, you just need the following! No extra installation steps needed to run on Colab, just pip install our tool, and you're all set to go!<br>\n\n```python\nfrom vidsz.opencv import Reader, Writer\nfrom cvu.detector import Detector\n\n# set video reader and writer, you can also use normal OpenCV\nreader = Reader(\"example.mp4\")\nwriter = Writer(reader, name=\"output.mp4\")\n\n\n# create detector\ndetector = Detector(classes=\"coco\", backend=\"tensorrt\")\n\n# process frames\nfor frame in reader:\n\n    # make predictions.\n    preds = detector(frame)\n\n    # draw it on frame\n    preds.draw(frame)\n\n    # write it to output\n    writer.write(frame)\n\nwriter.release()\nreader.release()\n\n```\n\n<br>\n\nWants to use even less lines of code? How about this! <br>\n\n```python\nfrom cvu.detector import Detector\nfrom vidsz.opencv import Reader, Writer\n\ndetector = Detector(classes=\"coco\", backend=\"tensorrt\")\n\n\nwith Reader(\"example.mp4\") as reader:\n    with Writer(reader, name=\"output.mp4\") as writer:\n        writer.write_all(map(lambda frame:detector(frame).draw(frame), reader))\n```\n\n<br>\n\nWant to switch to non-cuda devic? Just set `device=\"cpu\"`, and backend to `\"onnx\"`, `\"tflite\"`, `\"torch\"` or `\"tensorflow\"`.\n\n<br>\n\n```python\ndetector = Detector(classes=\"coco\", backend=\"onnx\", device=\"cpu\")\n```\n\n<br>\n\nWant to use TPU? Just set `device=\"tpu\"` and choose a supported backend (only `\"tensorflow\"` as of the latest release)\n\n<br>\n\n```python\ndetector = Detector(classes=\"coco\", backend=\"tensorflow\", device=\"tpu\")\n```\n\nYou can change device, platforms and backends as much as you need and want, without having to change your main pipeline.\n<br><br>\n\n# Why CVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"25\">?\n\n[Index](#index-)\n\nThere are countless great open-source state-of-the-art computer vision models that are pushing Computer Vision field ahead every moment. But many of them are either too complicated or hard to use in deployment scenario or to integrate in simple projects.\n\nCVU <img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/logo.png\" width=\"12\"> can handle all the Computer vision related stuff (even installation of the required frameworks/libraries in most cases), while you can focus on building awesome projects!!\n\nWe are working to optimize open source state-of-the-art models for various CV use cases, and aim to make them more accessible and available to everyone!\n<br> <br>\n\n# Yolov5 Object Detection\n\n[Index](#index-)\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"torch\",\n                    weight = \"yolov5s\", device = \"auto\"))\n```\n\nDetector Arguments\n\n- `classes (Union[str, List[str]])`: name of classes to be detected.\n  It can be set to individual classes like 'coco', 'person', 'cat' etc.\n  Alternatively, it also accepts list of classes such as ['person', 'cat'].\n  For default models/weights, 'classes' is used to filter out objects\n  according to provided argument from coco class. For custom models, all\n  classes should be provided in original order as list of strings.\n\n- `backend (str, optional)`: name of the backend to be used for inference purposes.\n  Defaults to \"torch\".\n\n- `weight (str, optional)`: path to weight files (according to selected backend).\n  Alternatively, it also accepts identifiers (such as yolvo5s, yolov5m, etc.)\n  to load pretrained models. Defaults to \"yolov5s\".\n\n- `device (str, optional)`: name of the device to be used. Valid\n  devices can be \"cpu\", \"gpu\", \"cuda\", \"tpu\", \"auto\". Defaults to \"auto\" which tries\n  to use the device best suited for selected backend and the hardware avaibility.\n\n<br>\n\nEvery object detector expects BGR formatted single image (batching is not supported yet), and returns a [Predictions](https://github.com/BlueMirrors/cvu/blob/master/cvu/detector/predictions.py) object which represents a group/list of detected objects. You can access individual detections using indexing or looping through [Predictions](https://github.com/BlueMirrors/cvu/blob/master/cvu/detector/predictions.py). A single detection is represented by [Prediction](https://github.com/BlueMirrors/cvu/blob/master/cvu/detector/prediction.py).\n\n```python\nimport cv2\nfrom cvu.detector import Detector\n\n# read image, initiate detector\nimg = cv2.imread(\"example.jpg\")\ndetector = Detector(classes=\"coco\")\n\n# inference\npredictions = detector(img)\n\n# predictions info\nprint(predictions)\n\n# draw on frame (inplace + returns img)\npredictions.draw(img)\n\n# class-wise counter object\nprint(predictions.count())\n\n# loop through\nfor prediction in predictions:\n    # print info\n    print(prediction)\n\n    # access specific things\n    print(prediction.bbox, prediction.confidence)\n    print(prediction.class_id, prediction.class_name)\n\n    # draw specific prediction (only inplace, doesn't return img)\n    prediction.draw(img)\n\n# save img\ncv2.imwrite(\"output.jpg\", img)\n```\n\nThese wrappers around detections provides various functionalities for drawing boxes, accessing detection info (individually as well as a group). And they implement CVU's common [predictions interface](https://github.com/BlueMirrors/cvu/blob/master/cvu/interface/predictions.py) for consistency.\n\n<br>\n\nEvery Object detectors is implemented in `cvu.detector`, following a common [interface](https://github.com/BlueMirrors/cvu/blob/master/cvu/interface/core.py).\n\nAs of our first alpha release, we only support `Yolov5s` models.\n\n<br>\n\n# Yolov5\n\n[Index](#index-)\n\n[Yolov5](https://github.com/BlueMirrors/cvu/blob/master/cvu/detector/yolov5/core.py) is one of the state of the art Object Detection models. Please check out more about it and train your own custom models through it's [official repo](https://github.com/ultralytics/yolov5). CVU also supports custom weights for all the backends.\n\nCheckout following backends for more specific information\n\n- [TensorRT (NVIDIA-GPU)](#tensorrt)\n- [ONNX (GPU, CPU)](#onnx)\n- [Torch (GPU, CPU)](#torch)\n- [TensorFlow (GPU, CPU, TPU)](#tensorflow)\n- [TFLite (CPU)](#tflite)\n\n<br>\n\n# TensorRT\n\n[Index](#index-)\n\n(Only supported for NVIDIA-GPUs, Tested on Linux Devices, Partial Dynamic Support)\n\nYou can use TensorRT powered detector by specifying the backend parameter.\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"tensorrt\"))\n```\n\nInternally, Detector will build TensorRT Cuda-Engine using pretrained ONNX Yolov5s weight file.\n\nIf you want to run detector for your custom weights, simply do following\n\n- [Train Yolov5 on your custom dataset](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n- [Export Weights PyTorch weights to ONNX](https://github.com/ultralytics/yolov5/blob/master/export.py)\n\nMake sure you use the `---dynamic` flag while exporting your custom weights.\n\n```bash\npython export.py --weights $PATH_TO_PYTORCH_WEIGHTS --dynamic --include onnx\n```\n\nNow simply set parameter `weights=\"path_to_custom_weights.onnx` in Detector initialization, and you're ready for inference.\n\n<br>\nNotes\n\n- Unlike other backends, TensorRT backend is not fully dynamic (for optimization reasons). You can initiate Detector and inference on any shape of image and it'll setup engine's input shape to the first input's shape. To run inference on a different shaped image, you'll have to create new detector.\n\n- Building TensorRT Engine and first inference can take sometime to complete (specially if it also has to install all the dependecies for the first time).\n\n- A new engine is built for an unseen input shape. But once built, engine file is cached and used for future inference.\n\n<br><br>\n\n# Torch\n\n[Index](#index-)\n\n(Supports every device and platform except TPU, Full Dynamic Support)\n\nYou can use Torch powered detector by specifying the backend parameter.\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"torch\"))\n```\n\nInternally, Detector will load Torchscript (JIT) pretrained Yolov5s weight model.\n\nIf you want to run detector for your custom weights, simply do following\n\n- [Train Yolov5 on your custom dataset](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n- [Export Weights PyTorch weights to TorchScript](https://github.com/ultralytics/yolov5/blob/master/export.py)\n\nMake sure your model is on correct device (CUDA can save torchscript in Float16 format which is unavailable/inefficient in many CPU) while exporting your custom weights. It's recommended to add `--half` flag for CUDA usage.\n\n```bash\npython export.py --weights $PATH_TO_PYTORCH_WEIGHTS --include torchscript\n```\n\nNow simply set parameter `weights=\"path_to_custom_weights.pt` in Detector initialization, and you're ready for inference.\n\n<br><br>\n\n# ONNX\n\n[Index](#index-)\n\n(Supports every device and platform except TPU, Full Dynamic Support)\n\nYou can use ONNX powered detector by specifying the backend parameter.\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"onnx\"))\n```\n\nInternally, Detector will load ONNX pretrained Yolov5s weight model.\n\nIf you want to run detector for your custom weights, simply do following\n\n- [Train Yolov5 on your custom dataset](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n- [Export Weights PyTorch weights to ONNX](https://github.com/ultralytics/yolov5/blob/master/export.py)\n\nMake sure you use the `---dynamic` flag while exporting your custom weights.\n\n```bash\npython export.py --weights $PATH_TO_PYTORCH_WEIGHTS --dynamic --include onnx\n```\n\nNow simply set parameter `weights=\"path_to_custom_weights.onnx` in Detector initialization, and you're ready for inference.\n\n<br><br>\n\n# TFLite\n\n[Index](#index-)\n\n(Supports CPU on every platform, Full Dynamic Support)\n\nYou can use TFLite powered detector by specifying the backend parameter.\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"tflite\"))\n```\n\nInternally, Detector will load TFLite pretrained Yolov5s weight model.\n\nWe will update dynamic export info soon, please check back again.\n\n<br>\n\nNotes\n\n- We currently use `tf.lite` for Interpreter creation. In next update, we'll provide option of `tflite_runtime` and `pycoral`\n\n<br><br>\n\n# Tensorflow\n\n[Index](#index-)\n\n(Supports every device on every platform including TPU, Full Dynamic Support)\n\nYou can use TensorFlow powered detector by specifying the backend parameter.\n\n```python\nfrom cvu.detector import Detector\ndetector = Detector(classes=\"coco\", backend = \"tensorflow\"))\n```\n\nInternally, Detector will load Tensorflow SavedModel pretrained Yolov5s weight model. You can also set `device='tpu'` (tested on colab)\n\nWe will update dynamic export info soon, please check back again.\n\n<br><br>\n\n# Devices\n\n[Index](#index-)\n\n### Support Info\n\nFollowing is latest support matrix\n\n| Device | TensorFlow | Torch | TFLite | ONNX | TensorRT |\n| ------ | ---------- | ----- | ------ | ---- | -------- |\n| GPU    | \u2705         | \u2705    | \u274c     | \u2705   | \u2705       |\n| CPU    | \u2705         | \u2705    | \u2705     | \u2705   | \u274c       |\n| TPU    | \u2705         | \u274c    | \u274c     | \u274c   | \u274c       |\n\n<br>\n\n### Recommended Backends (in order)\n\nBased on FPS performance and various benchmarks\n\n- GPU: `TensorRT` > `Torch` > `ONNX` > `TensorFlow`\n- CPU: `ONNX` > `TFLite` > `TensorFlow` > `Torch`\n- TPU: `TensorFlow`\n\n<br><br>\n\n# Benchmarks (Yolov5)\n\n[Index](#index-)\n\nYou can run your own benchmarks using our [Benchmarker](https://github.com/BlueMirrors/cvu/blob/master/examples/benchmark.py)\n\nRun Benchmark over all supported backends for GPU/CPU/TPU (without and without read/write overhead)\n\n```bash\npython benchmark.py -device $DEVICE_NAME\n```\n\nAlternatively you can benchmark specific backend on specifc device, with specific benchmark settings.\n\n```bash\npython benchmark.py -device $DEVICE_NAME -backend $BACKEND_NAME -warmups $WARMUP_ITERATIONS -iterations $ITERATIONS_COUNT\n```\n\nCheckout [Benchmarker](https://github.com/BlueMirrors/cvu/blob/master/examples/benchmark.py) for more information about all available command line arguments.\n\n<br>\n\n# Yolov5 Benchmark Results\n\n[Index](#index-)\n\n## GPU (Colab-NVIDIA T4)\n\nBased on 5000 inference interations after 50 iterations of warmups. Includes Image Preprocessing (letterboxing etc.), Model Inference and Output Postprocessing (NMS, Scale-Coords, etc.) time.\n\n| Backend    | FPS     |\n| ---------- | ------- |\n| TensorRT   | 157-165 |\n| Torch      | 150-155 |\n| ONNX       | 92-96   |\n| TensorFlow | 43-47   |\n\n<br>\n\n## GPU (Colab-NVIDIA P4)\n\nBased on 5000 inference interations after 50 iterations of warmups. Includes Image Preprocessing (letterboxing etc.), Model Inference and Output Postprocessing (NMS, Scale-Coords, etc.) time.\n\n| Backend    | FPS   |\n| ---------- | ----- |\n| TensorRT   | 82-86 |\n| Torch      | 65-69 |\n| ONNX       | 62-66 |\n| TensorFlow | 39-42 |\n\n<br>\n\n## CPU (Colab)\n\nNote: We are performing more benchmarks, we will update info later on.\n\nBased on 500 inference interations after 10 iterations of warmups. Includes Image Preprocessing (letterboxing etc.), Model Inference and Output Postprocessing (NMS, Scale-Coords, etc.) time.\n\n| Backend    | FPS     |\n| ---------- | ------- |\n| ONNX       | 5.4-7.6 |\n| TFLite     | 4.0-5.1 |\n| TensorFlow | 3.5-4.2 |\n| Torch      | 3.2-4.0 |\n\n<br><br>\n\n# Precission Accuracy (Yolov5)\n\n[Index](#index-)\n\nFor a successful deployment, precission is also important (i.e. closness to results of native-framework of trained weights). We will add more numerical results soon, for now we provide image comparison.\n\n## Torch\n\n<details open>\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/zidane_out_torch.jpg\">\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/bus_out_torch.jpg\">\n</details>\n\n<br>\n\n## ONNX\n\n<details close>\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/zidane_out_onnx.jpg\">\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/bus_out_onnx.jpg\">\n</details>\n\n<br>\n\n## TensorFlow\n\n<details close>\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/zidane_out_tensorflow.jpg\">\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/bus_out_tensorflow.jpg\">\n</details>\n\n<br>\n\n## TFLite\n\n<details close>\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/zidane_out_tflite.jpg\">\n<img src=\"https://raw.githubusercontent.com/BlueMirrors/cvu/master/static/bus_out_tflite.jpg\">\n</details>\n\n<br>\n\n<br>\n\n# References\n\n- **_Logo-Attribution_**\n  <a href=\"http://www.freepik.com\">Designed by roserodionova / Freepik</a>\n- [Yolov5 (Default Object Detection Model)](https://github.com/ultralytics/yolov5)\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/BlueMirrors/cvu",
            "keywords": "",
            "license": "Apache Software License v2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "cvu-python",
            "package_url": "https://pypi.org/project/cvu-python/",
            "platform": "",
            "project_url": "https://pypi.org/project/cvu-python/",
            "project_urls": {
                "Homepage": "https://github.com/BlueMirrors/cvu"
            },
            "release_url": "https://pypi.org/project/cvu-python/0.0.1a1/",
            "requires_dist": [
                "opencv-python",
                "vidsz",
                "numpy",
                "requests"
            ],
            "requires_python": "",
            "summary": "Computer Vision deployment tools for dummies and experts.",
            "version": "0.0.1a1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 11064730,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "11ef781d12c6084990cd73ebd4ad69d3",
                    "sha256": "262199afc0d8bfda291e47f0f291e7d628663c0845c5854159d07012f9d0296b"
                },
                "downloads": -1,
                "filename": "cvu_python-0.0.1a1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "11ef781d12c6084990cd73ebd4ad69d3",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 56832,
                "upload_time": "2021-08-01T23:54:08",
                "upload_time_iso_8601": "2021-08-01T23:54:08.968097Z",
                "url": "https://files.pythonhosted.org/packages/d9/d5/cde02aecaebdf7fbe1083cd8608797a9b9a680649ebef687397915490db2/cvu_python-0.0.1a1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "46110447230f576d0dd19995770af548",
                    "sha256": "a92cfacd2804fead2f42fdd616bc7c3571a8b7f63985ef5e4ad886d4f64dccde"
                },
                "downloads": -1,
                "filename": "cvu-python-0.0.1a1.tar.gz",
                "has_sig": false,
                "md5_digest": "46110447230f576d0dd19995770af548",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 37046,
                "upload_time": "2021-08-01T23:54:11",
                "upload_time_iso_8601": "2021-08-01T23:54:11.085011Z",
                "url": "https://files.pythonhosted.org/packages/40/e8/4ef0ae1f6c6daa8831d2d6b019d38b076a6129542a9688f55d50821a810e/cvu-python-0.0.1a1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}