{
    "1.0rc1": {
        "info": {
            "author": "Yongkyoon No \ub178\uc6a9\uade0",
            "author_email": "yno@linguist.cnu.ac.kr",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
                "Natural Language :: Korean",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering :: Information Analysis",
                "Topic :: Text Processing :: Linguistic"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/noyongkyoon/hangul",
            "keywords": "Korean tokenization word_segmentation",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "hangul-korean",
            "package_url": "https://pypi.org/project/hangul-korean/",
            "platform": "",
            "project_url": "https://pypi.org/project/hangul-korean/",
            "project_urls": {
                "Homepage": "https://github.com/noyongkyoon/hangul"
            },
            "release_url": "https://pypi.org/project/hangul-korean/1.0rc1/",
            "requires_dist": [
                "pandas",
                "tensorflow",
                "tensorflow-addons"
            ],
            "requires_python": ">=3.8",
            "summary": "Word segmentation for the Korean Language",
            "version": "1.0rc1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 9180601,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "309e41a7e296fd0b51a9a1a4d58c9bca",
                    "sha256": "b5171884a2d2ea6b527ac404a90e5f174297969440015db25eefa4e3a5e7fa15"
                },
                "downloads": -1,
                "filename": "hangul_korean-1.0rc1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "309e41a7e296fd0b51a9a1a4d58c9bca",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8",
                "size": 60183809,
                "upload_time": "2020-12-05T08:16:54",
                "upload_time_iso_8601": "2020-12-05T08:16:54.578842Z",
                "url": "https://files.pythonhosted.org/packages/f1/27/3d95a7f0d3820bf4fffe809d95b5d0d35dc6457796da9a4f0d2d5e04d8b2/hangul_korean-1.0rc1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "470b3dc3425a8b8d04ec1e9b16ba5528",
                    "sha256": "2ddb312c099983d6c7365e2c807e644b6a2b631dfda0866bec61d7c9b5bfba8b"
                },
                "downloads": -1,
                "filename": "hangul-korean-1.0rc1.tar.gz",
                "has_sig": false,
                "md5_digest": "470b3dc3425a8b8d04ec1e9b16ba5528",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.8",
                "size": 19965,
                "upload_time": "2020-12-05T08:16:57",
                "upload_time_iso_8601": "2020-12-05T08:16:57.603450Z",
                "url": "https://files.pythonhosted.org/packages/60/4a/76a09497d3799e14a029f7fd3fa8e7312d941fb9dc333c4ffc1c975a1e68/hangul-korean-1.0rc1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.0rc2": {
        "info": {
            "author": "Yongkyoon No \ub178\uc6a9\uade0",
            "author_email": "yno@linguist.cnu.ac.kr",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
                "Natural Language :: Korean",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering :: Information Analysis",
                "Topic :: Text Processing :: Linguistic"
            ],
            "description": "# Package **hangul-korean**\nHangul is the alphabet for the Korean language. **hangul-korean** is a package\nwhich currently contains a module for segmenting words in written Korean texts.\n\n# Segmentation of Korean Words \ud55c\uad6d\uc5b4 \ub0b1\ub9d0 \ubd84\uc808\nWritten Korean texts do employ white space characters. However, more often than not,\nKorean words occur in a text concatenated immediately to adjacent words without\nan intervening space character. This low degree of separation of words from\neach other in writing is due somewhat to an abundance of what linguists call \"endoclitics\" \nin the language.\n\nAs the language has been subjected to principled and rigorous study for a few\ndecades, the issue of which strings of sounds, or letters, are words and which are\nnot, has been settled among a small group of selected linguists. This kind of\nadvancement has not been propagated to the general public yet, and nlp\nengineers working on Korean cannot but make do with whatever inconsistent\ngrammars they happen to have access to. Thus, a major source of difficulty in\ndeveloping competent Korean text processors has been, and still is, the notion of a word\nas the smallest syntactic unit.\n\n# The **WordSegmenter** class\nThe module **tokenizer** in this package defines the class **WordSegmenter**.\nIt has, among other things, the following methods:\n* __init__(modelFile, word2idxFile)\n* infile(fileName)\n* outfile(fileName)\n* inputAsString(aStr)\n* doSegment()\n* segmentedOutput()\n\nAfter creating a WordSegmenter object, say wseg, give it a file or a string and then\nissue wseg.doSegment(). The word-segmented text will be in a file (if specified\nwith outfile()) or in an instance variable accessible via segmentedOutput().\n\n# Typical use\n\n```python\nfrom hangul.tokenizer import WordSegmenter\n\nwsg = WordSegmenter()\naPassage = \"\uc5b4\ud734\uc7e4\uac00 \uc65c\uc800\ub798? \uc815\ub9d0\uc6b0\uc2a4\uc6cc\uc8fd\uaca0\ub124\"  # \ubb38\uc790\uc5f4\uc744 \ud568\uc218 inputAsString\uc758\nwsg.inputAsString (aPassage)                    # \ub9e4\uac1c\ubcc0\uc778\uc73c\ub85c \uc8fc\uac70\ub098\n# inFile = \"tobesegmented\"        # \ud30c\uc77c \uc548\uc5d0 \ub2f4\uae34 \uae00\uc744 \ub0b1\ub9d0\ubd84\uc808\ud560 \ub54c\uc5d0\ub294 \n# wsg.infile(inFile)              # \uadf8 \ud30c\uc77c \uc774\ub984\uc744 \ud568\uc218 infile\uc5d0 \ub118\uae34\ub2e4.\n# wsg.outfile(\"output.txt\")\nwsg.doSegment() \n# infile()\uc774 \uc0ac\uc6a9\ub418\uba74 \ub0b1\ub9d0 \ubd84\uc808 \ub41c \uae00\uc774 \uc801\ud78c \ud30c\uc77c\uc774 \uc0dd\uaca8 \ub09c\ub2e4.\n# \uc774 \ud30c\uc77c\uc740 \ud568\uc218 outfile()\ub85c \uc9c0\uc815\ub420 \uc218 \uc788\uace0 \ub514\ud3f4\ud2b8 \uac12\uc740 \"segmented_yyddd_hhmm.txt\"\uc774\ub2e4.\n# with open(\"segmented_yyddd_hhmm.txt\", \"r\") as f:\n#   lines = f.readlines()\n# for aLine in lines:\n#   print(aLine)\n# inputAsString()\uc774 \uc0ac\uc6a9\ub41c \uacbd\uc6b0\uc5d0\ub294\nprint(wsg.segmentedOutput)\n```\n\n# Forms of a verb are not analyzed into morphs\nThe lexical category verb is inflected in hundreds (or thousands) of ways in\nthe language and it is the only category that inflects. We do not analyze\na form of a verb into its morphs. Such an analysis is best reserved for\na separate component of inflectional morphology and is certainly required in\norder to do syntactic analyses of various kinds.\n\n# Slim size of the model\nThe model this package uses is of a very compact size: it is merely ten\nmegabytes long.\n\n# Forthcoming in the package\nThe next version of this package might well contain a POS tagger. A higher\nF-measure of the word segmentation system (which currently is 0.970\nwhile that of the open access model is somewhat lower) is something we would \nlike to see as well.\n\n# Status of papers that describe this package\nA draft is to be submitted to a journal, which describes the way the model for\nthis package is obtained.\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/noyongkyoon/hangul",
            "keywords": "Korean tokenization word_segmentation",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "hangul-korean",
            "package_url": "https://pypi.org/project/hangul-korean/",
            "platform": "",
            "project_url": "https://pypi.org/project/hangul-korean/",
            "project_urls": {
                "Homepage": "https://github.com/noyongkyoon/hangul"
            },
            "release_url": "https://pypi.org/project/hangul-korean/1.0rc2/",
            "requires_dist": [
                "pandas",
                "tensorflow"
            ],
            "requires_python": ">=3.8",
            "summary": "Word segmentation for the Korean Language",
            "version": "1.0rc2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 9180601,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "ee14ae16d79182496d1f3a571384a30b",
                    "sha256": "ed081e53aa103974deaf5d269183b82928e44055551b4ec9192a14607d6c7554"
                },
                "downloads": -1,
                "filename": "hangul_korean-1.0rc2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "ee14ae16d79182496d1f3a571384a30b",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.8",
                "size": 9083966,
                "upload_time": "2021-01-20T08:29:57",
                "upload_time_iso_8601": "2021-01-20T08:29:57.371025Z",
                "url": "https://files.pythonhosted.org/packages/83/8c/bd911dc6de8b6f69ab36c38b1aa0f8521bcf69c13cab172581af1e7642e3/hangul_korean-1.0rc2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "01f3411e28abc463ad42a0c1fcf12d7a",
                    "sha256": "39db724ce8f5781f2b7bc442388784c77307c136dec53fd15b833e410a8f1578"
                },
                "downloads": -1,
                "filename": "hangul-korean-1.0rc2.tar.gz",
                "has_sig": false,
                "md5_digest": "01f3411e28abc463ad42a0c1fcf12d7a",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.8",
                "size": 5504,
                "upload_time": "2021-01-20T08:29:58",
                "upload_time_iso_8601": "2021-01-20T08:29:58.997531Z",
                "url": "https://files.pythonhosted.org/packages/f0/0a/b7c771815cac84ccf0726a62813760728abc8b7e0421c82cc012bfc8c32b/hangul-korean-1.0rc2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}