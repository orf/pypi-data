{
    "0.1.0": {
        "info": {
            "author": "Thoughtworks",
            "author_email": "thoughtworks@thoughtworks.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.10"
            ],
            "description": "# Data Protection Framework\nData Protection Framework is a python library/command line application for identification, anonymization and de-anonymization of Personally Identifiable Information data.\n\nThe framework aims to work on a two-fold principle for detecting PII:\n1. Using RegularExpressions using a pattern\n2. Using NLP for detecting NER (Named Entity Recognitions)\n\n## Features and Current Status\n\n### Completed\n * Following Global detectors have been completed:\n   * [x] EMAIL_ADDRESS :  An email address identifies the mailbox that emails are sent to or from. The maximum length of the domain name is 255 characters, and the maximum length of the local-part is 64 characters.\n   * [x] CREDIT_CARD_NUMBER : A credit card number is 12 to 19 digits long. They are used for payment transactions globally.\n\n * Following detectors specific to Singapore have been completed:\n   * [x] PHONE_NUMBER : A telephone number.\n   * [x] FIN/NRIC : A unique set of nine alpha-numeric characters on the Singapore National Registration Identity Card.\n\n * Following anonymizers have been added\n    * [x] Redaction: Deletes all or part of a detected sensitive value.\n    * [x] Encryption :  Encrypts the original sensitive data value using a cryptographic key. Cloud DLP supports several types of tokenization, including transformations that can be reversed, or \"re-identified.\"\n\n### TO-DO\nFollowing features  are part of the backlog with more features coming soon\n * Detectors:\n    * [ ] NAME\n    * [ ] ADDRESS\n * Anonymizers:\n    * [ ] Masking: Replaces a number of characters of a sensitive value with a specified surrogate character, such as a hash (#) or asterisk (*).\n    * [ ] Bucketing: \"Generalizes\" a sensitive value by replacing it with a range of values. (For example, replacing a specific age with an age range,\n    or temperatures with ranges corresponding to \"Hot,\" \"Medium,\" and \"Cold.\")\n    * [ ] Replacement: Replaces a detected sensitive value with a specified surrogate value.\n\n\nYou can have a detailed at upcoming features and backlog in this [Github Board](https://github.com/thoughtworks-datakind/anonymizer/projects/1?fullscreen=true)\n\n## Development setup\n\nClone the [repo](https://github.com/thoughtworks-datakind/anonymizer) and follow the below instructions:  <br/>\n_Assuming that $pwd is where you cloned the repo_\n2. Setup venv : `./bin/setup_venv_locally.sh`\n3. Activate venv : `source ./.venv/bin/activate`\n4. Install dependencies : `pip install -r requirements-dev.txt`\n\n### Config JSON\nAn example for the config JSON is located at `<PROJECT_ROOT>/config.json`\n```\n{\n  \"acquire\": {\n    \"file_path\": <FILE PATH TO YOUR INPUT CSV>,\n    \"delimiter\": <YOUR CSV DELIMITER>\n  },\n  \"analyze\": {\n\n  },\n  \"report\" : {\n    \"location\" : <PATH TO YOUR REPORT OUTPUT FOLDER>,\n    \"level\" : <LOG LEVEL>\n  },\n  \"anonymize\": {\n    \"output_file_path\" : <PATH TO YOUR CSV OUTPUT FOLDER>\n  }\n}\n```\n\n### Running Tests\nUpdate this file first `<PROJECT_ROOT>/src/tests/config/test_config.json` \\\nYou can run the tests by triggering shell script located at `<PROJECT_ROOT>/bin/run_tests.sh`\n\n### Trying out on local\n\n##### Anonymizing a delimited csv file\n1. Set up a JSON config file similar to the one seen at the project root.\nIn the 'acquire' section of the json, populate the input file path and the delimiter.\nIn the 'report' section, provide the output path, where you want the PII detection report to be generated.\nA 'high' level report just calls out which columns have PII attributes.\nA 'medium' level report calls out the percentage of PII in each column and the associated PII (email, credit card, etc)type for the same.\n2. Run the main class - `python src/dpf_main.py --config <absolute path of the config file>`\nYou should see the report being appended to the file named 'report_\\<date\\>.log' in the output path specified in the\nconfig file.\n\n### Packaging\nRun `python setup.py bdist_wheel` and the `.whl` file will be created in the `dist` folder.\n\n### Spark-submit\nTo run spark-submit locally, you can run the following command\n`spark-submit --py-files dist/SomePackage-*.whl src_spark/main.py --config config.json`\n\n\n### Licensing\nDistributed under the MIT license. See ``LICENSE`` for more information.\n\n\n### Contributing\n\nYou want to help out? _Awesome_!\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pii-anonymizer",
            "package_url": "https://pypi.org/project/pii-anonymizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/pii-anonymizer/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/pii-anonymizer/0.1.0/",
            "requires_dist": [
                "pyspark (>=3.3.0,<3.4.0)",
                "pandas (>=1.5.0,<2.0.0)"
            ],
            "requires_python": ">=3.10,<4.0",
            "summary": "Data Protection Framework is a python library/command line application for identification, anonymization and de-anonymization of Personally Identifiable Information data.",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15733463,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "cbbe4be35ace42b6fb34bbaa972377bc",
                    "sha256": "12c4063541cc729793ada495a439dba0be069e94c9f66f4c550eb896802740ca"
                },
                "downloads": -1,
                "filename": "pii_anonymizer-0.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "cbbe4be35ace42b6fb34bbaa972377bc",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.10,<4.0",
                "size": 44975,
                "upload_time": "2022-11-11T08:33:42",
                "upload_time_iso_8601": "2022-11-11T08:33:42.665663Z",
                "url": "https://files.pythonhosted.org/packages/0b/6d/aea3e3bb22d464d9fb3e4d2eeffc87d11982f732eed7708fe845cd2a6867/pii_anonymizer-0.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "f959921c355b880b98bdd77e8243a0c7",
                    "sha256": "b27a8b3f652125941e4cbbdc309d03b45ff5595ccedfc4ac1037cc044cea2e79"
                },
                "downloads": -1,
                "filename": "pii_anonymizer-0.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "f959921c355b880b98bdd77e8243a0c7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.10,<4.0",
                "size": 26783,
                "upload_time": "2022-11-11T08:33:44",
                "upload_time_iso_8601": "2022-11-11T08:33:44.326842Z",
                "url": "https://files.pythonhosted.org/packages/57/3c/eaeaa29c1ff0857591651bd035ef6a0d6dc50f7e9fda48291654228f7bcc/pii_anonymizer-0.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}