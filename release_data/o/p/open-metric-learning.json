{
    "0.3.0": {
        "info": {
            "author": "Shabanov Aleksei",
            "author_email": "shabanoff.aleksei@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Environment :: Console",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Operating System :: OS Independent",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Programming Language :: Python :: Implementation :: CPython",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Image Recognition"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/OML-Team/open-metric-learning",
            "keywords": "data-science,computer-vision,deep-learning,pytorch,metric-learning,representation-learning,pytorch-lightning",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "open-metric-learning",
            "package_url": "https://pypi.org/project/open-metric-learning/",
            "platform": null,
            "project_url": "https://pypi.org/project/open-metric-learning/",
            "project_urls": {
                "Bug Tracker": "https://github.com/OML-Team/open-metric-learning/issues",
                "Homepage": "https://github.com/OML-Team/open-metric-learning"
            },
            "release_url": "https://pypi.org/project/open-metric-learning/0.3.0/",
            "requires_dist": [
                "albumentations (==1.0.3)",
                "gdown (==4.5.1)",
                "grad-cam (==1.3.7)",
                "hydra-core (==1.2.0)",
                "jupyter (==1.0.0)",
                "matplotlib (==3.5.2)",
                "numpy (==1.21.6)",
                "omegaconf (==2.2.2)",
                "opencv-python (==4.5.4.60)",
                "pandas (==1.1.5)",
                "pillow (==9.2.0)",
                "pytest (==6.2.2)",
                "python-dotenv (==0.17.0)",
                "pytorch-lightning (==1.5.9)",
                "torch (==1.12.0)",
                "torchvision (==0.13.0)",
                "validators (==0.20.0)"
            ],
            "requires_python": ">=3.7,<4.0",
            "summary": "OML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.",
            "version": "0.3.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15675744,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "d192310b259151e8170cf7350e241931",
                    "sha256": "299c528383a9deaabd9e42271a899f0e5c2897e2e0262e2265fb83c9f6c5ab8e"
                },
                "downloads": -1,
                "filename": "open_metric_learning-0.3.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "d192310b259151e8170cf7350e241931",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<4.0",
                "size": 150151,
                "upload_time": "2022-10-06T11:52:08",
                "upload_time_iso_8601": "2022-10-06T11:52:08.194819Z",
                "url": "https://files.pythonhosted.org/packages/e7/38/4277b29ea4c66440fa562bbcf70c93400d2cacae1398d785610d58319487/open_metric_learning-0.3.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "dadc485cd27389c2b1c14d70fce574ef",
                    "sha256": "89b7e8d00fa40b40c8ef7034253eb242738a9300ff425a6041011b64813fd8a7"
                },
                "downloads": -1,
                "filename": "open-metric-learning-0.3.0.tar.gz",
                "has_sig": false,
                "md5_digest": "dadc485cd27389c2b1c14d70fce574ef",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<4.0",
                "size": 113474,
                "upload_time": "2022-10-06T11:52:09",
                "upload_time_iso_8601": "2022-10-06T11:52:09.639250Z",
                "url": "https://files.pythonhosted.org/packages/7e/b8/a391381db4d61b22061bd4bb351c2b34daf30530d9250bb70e5748bfd8f5/open-metric-learning-0.3.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.3.1": {
        "info": {
            "author": "Shabanov Aleksei",
            "author_email": "shabanoff.aleksei@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Environment :: Console",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Operating System :: OS Independent",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Programming Language :: Python :: Implementation :: CPython",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Image Recognition"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/OML-Team/open-metric-learning",
            "keywords": "data-science,computer-vision,deep-learning,pytorch,metric-learning,representation-learning,pytorch-lightning",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "open-metric-learning",
            "package_url": "https://pypi.org/project/open-metric-learning/",
            "platform": null,
            "project_url": "https://pypi.org/project/open-metric-learning/",
            "project_urls": {
                "Bug Tracker": "https://github.com/OML-Team/open-metric-learning/issues",
                "Homepage": "https://github.com/OML-Team/open-metric-learning"
            },
            "release_url": "https://pypi.org/project/open-metric-learning/0.3.1/",
            "requires_dist": [
                "albumentations (==1.0.3)",
                "gdown (==4.5.1)",
                "grad-cam (==1.3.7)",
                "hydra-core (==1.2.0)",
                "jupyter (==1.0.0)",
                "matplotlib (==3.5.2)",
                "neptune-client (==0.14.2)",
                "numpy (==1.21.6)",
                "omegaconf (==2.2.2)",
                "opencv-python (==4.5.4.60)",
                "pandas (==1.1.5)",
                "pillow (==9.2.0)",
                "pytest (==6.2.2)",
                "python-dotenv (==0.17.0)",
                "pytorch-lightning (==1.5.9)",
                "torch (==1.12.0)",
                "torchvision (==0.13.0)",
                "validators (==0.20.0)"
            ],
            "requires_python": ">=3.7,<4.0",
            "summary": "OML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.",
            "version": "0.3.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15675744,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "b76ffcc10e39b81dd3ab5edaa3f4aed2",
                    "sha256": "fda1619b615e85b6f95a33578ae2b636d05075736bcb2c7afaa8e5e6e2f7f514"
                },
                "downloads": -1,
                "filename": "open_metric_learning-0.3.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "b76ffcc10e39b81dd3ab5edaa3f4aed2",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<4.0",
                "size": 154644,
                "upload_time": "2022-10-19T08:30:43",
                "upload_time_iso_8601": "2022-10-19T08:30:43.924563Z",
                "url": "https://files.pythonhosted.org/packages/fa/98/e66fbf6a20a85f24e051b6df667ddcb115c960a716e11304bfebf314a7ae/open_metric_learning-0.3.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d1343dd419bb2c7c1af676fa0161fc37",
                    "sha256": "f0ae2e47edebe56e6f8f8c71f52880130e09e3b172372539be4ce3650a80feb7"
                },
                "downloads": -1,
                "filename": "open-metric-learning-0.3.1.tar.gz",
                "has_sig": false,
                "md5_digest": "d1343dd419bb2c7c1af676fa0161fc37",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<4.0",
                "size": 119138,
                "upload_time": "2022-10-19T08:30:46",
                "upload_time_iso_8601": "2022-10-19T08:30:46.023813Z",
                "url": "https://files.pythonhosted.org/packages/44/0b/28ad5566e0684b9d8be3b823c2712ab36db93cf7265b3923439a51c03c99/open-metric-learning-0.3.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.3.2": {
        "info": {
            "author": "Shabanov Aleksei",
            "author_email": "shabanoff.aleksei@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Environment :: Console",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Operating System :: OS Independent",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Programming Language :: Python :: Implementation :: CPython",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Image Recognition"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/OML-Team/open-metric-learning",
            "keywords": "data-science,computer-vision,deep-learning,pytorch,metric-learning,representation-learning,pytorch-lightning",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "open-metric-learning",
            "package_url": "https://pypi.org/project/open-metric-learning/",
            "platform": null,
            "project_url": "https://pypi.org/project/open-metric-learning/",
            "project_urls": {
                "Bug Tracker": "https://github.com/OML-Team/open-metric-learning/issues",
                "Homepage": "https://github.com/OML-Team/open-metric-learning"
            },
            "release_url": "https://pypi.org/project/open-metric-learning/0.3.2/",
            "requires_dist": [
                "albumentations (==1.0.3)",
                "gdown (==4.5.1)",
                "grad-cam (==1.3.7)",
                "hydra-core (==1.2.0)",
                "jupyter (==1.0.0)",
                "matplotlib (==3.5.2)",
                "neptune-client (==0.14.2)",
                "numpy (==1.21.6)",
                "omegaconf (==2.2.2)",
                "opencv-python (==4.5.4.60)",
                "pandas (==1.1.5)",
                "pillow (==9.2.0)",
                "pytest (==6.2.2)",
                "python-dotenv (==0.17.0)",
                "pytorch-lightning (==1.5.9)",
                "torch (==1.12.0)",
                "torchvision (==0.13.0)",
                "validators (==0.20.0)"
            ],
            "requires_python": ">=3.7,<4.0",
            "summary": "OML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.",
            "version": "0.3.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15675744,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "c7063a82a1e0e75e02728a56a453de8a",
                    "sha256": "d56001db765b56c0429d375e0a98201760009a2755ac841045b9510b6b6f18cb"
                },
                "downloads": -1,
                "filename": "open_metric_learning-0.3.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "c7063a82a1e0e75e02728a56a453de8a",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<4.0",
                "size": 154728,
                "upload_time": "2022-10-30T20:48:28",
                "upload_time_iso_8601": "2022-10-30T20:48:28.312431Z",
                "url": "https://files.pythonhosted.org/packages/fa/82/2ce3fb6a0610489b547d194e8b6541b5ec0145c93511d7ad90424b2d6849/open_metric_learning-0.3.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "de6aa121ffeb25431fcfb9558e203210",
                    "sha256": "c15dea591824f6155a1eb85b1c58a6a9792059f9f2ac8989983ab45107da90cd"
                },
                "downloads": -1,
                "filename": "open-metric-learning-0.3.2.tar.gz",
                "has_sig": false,
                "md5_digest": "de6aa121ffeb25431fcfb9558e203210",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<4.0",
                "size": 119924,
                "upload_time": "2022-10-30T20:48:29",
                "upload_time_iso_8601": "2022-10-30T20:48:29.945281Z",
                "url": "https://files.pythonhosted.org/packages/73/b7/b6f94d08b5831012bd1566951ef9401137eb7dadb25a9018f9ee486615f8/open-metric-learning-0.3.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.3.3": {
        "info": {
            "author": "Shabanov Aleksei",
            "author_email": "shabanoff.aleksei@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Environment :: Console",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Natural Language :: English",
                "Operating System :: OS Independent",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Programming Language :: Python :: Implementation :: CPython",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Image Recognition"
            ],
            "description": "<div align=\"center\">\n<img src=\"https://i.ibb.co/wsmD5r4/photo-2022-06-06-17-40-52.jpg\" width=\"400px\">\n\n![example workflow](https://github.com/OML-Team/open-metric-learning/actions/workflows/pre-commit-workflow.yaml/badge.svg)\n![example workflow](https://github.com/OML-Team/open-metric-learning/actions/workflows/tests-workflow.yaml/badge.svg?)\n[![Documentation Status](https://readthedocs.org/projects/open-metric-learning/badge/?version=latest)](https://open-metric-learning.readthedocs.io/en/latest/?badge=latest)\n[![PyPI Status](https://pepy.tech/badge/open-metric-learning)](https://pepy.tech/project/open-metric-learning)\n[![Pipi version](https://img.shields.io/pypi/v/open-metric-learning.svg)](https://pypi.org/project/open-metric-learning/)\n![example workflow](https://github.com/OML-Team/open-metric-learning/actions/workflows/python-versions.yaml/badge.svg?)\n[![python](https://img.shields.io/badge/python_3.7-passing-success)](https://github.com/OML-Team/open-metric-learning/actions/workflows/python-versions.yaml/badge.svg?)\n[![python](https://img.shields.io/badge/python_3.8-passing-success)](https://github.com/OML-Team/open-metric-learning/actions/workflows/python-versions.yaml/badge.svg?)\n[![python](https://img.shields.io/badge/python_3.9-passing-success)](https://github.com/OML-Team/open-metric-learning/actions/workflows/python-versions.yaml/badge.svg?)\n[![python](https://img.shields.io/badge/python_3.10-passing-success)](https://github.com/OML-Team/open-metric-learning/actions/workflows/python-versions.yaml/badge.svg?)\n\n<div align=\"left\">\n\nOML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.\n\n## FAQ\n\n<details>\n<summary>Why do I need OML?</summary>\n<p>\n\nYou may think *\"If I need image embeddings I can simply train a vanilla classifier and take its penultimate layer\"*.\nWell, it makes sense as a starting point. But there are several possible drawbacks:\n\n* If you want to use embeddings to perform searching you need to calculate some distance among them (for example, cosine or L2).\n  Usually, **you don't directly optimize these distances during the training** in the classification setup. So, you can only hope that\n  final embeddings will have the desired properties.\n\n* **The second problem is the validation process**.\n  In the searching setup, you usually care how related your top-N outputs are to the query.\n  The natural way to evaluate the model is to simulate searching requests to the reference set\n  and apply one of the retrieval metrics.\n  So, there is no guarantee that classification accuracy will correlate with these metrics.\n\n* Finally, you may want to implement a metric learning pipeline by yourself.\n  **There is a lot of work**: to use triplet loss you need to form batches in a specific way,\n  implement different kinds of triplets mining, tracking distances, etc. For the validation, you also need to\n  implement retrieval metrics,\n  which include effective embeddings accumulation during the epoch, covering corner cases, etc.\n  It's even harder if you have several gpus and use DDP.\n  You may also want to visualize your search requests by highlighting good and bad search results.\n  Instead of doing it by yourself, you can simply use OML for your purposes.\n\n</p>\n</details>\n\n\n<details>\n<summary>What is the difference between Open Metric Learning and PyTorch Metric Learning?</summary>\n<p>\n\n[PML](https://github.com/KevinMusgrave/pytorch-metric-learning) is the popular library for Metric Learning,\nand it includes a rich collection of losses, miners, distances, and reducers; that is why we provide straightforward\n[examples](https://github.com/OML-Team/open-metric-learning#usage-with-pytorch-metric-learning) of using them with OML.\nInitially, we tried to use PML, but in the end, we came up with our library, which is more pipeline / recipes oriented.\nThat is how OML differs from PML:\n\n* OML has [Config API](https://open-metric-learning.readthedocs.io/en/latest/examples/config.html)\n  which allows training models by preparing a config and your data in the required format\n  (it's like converting data into COCO format to train a detector from [mmdetection](https://github.com/open-mmlab/mmdetection)).\n\n* OML focuses on end-to-end pipelines and practical use cases.\n  It has config based examples on popular benchmarks close to real life (like photos of products of thousands ids).\n  We found some good combinations of hyperparameters on these datasets, trained and published models and their configs.\n  Thus, it makes OML more recipes oriented than PML, and its author\n  [confirms](https://github.com/KevinMusgrave/pytorch-metric-learning/issues/169#issuecomment-670814393)\n  this saying that his library is a set of tools rather the recipes, moreover, the examples in PML are mostly for CIFAR and MNIST datasets.\n\n* OML has the [Zoo](https://github.com/OML-Team/open-metric-learning#zoo) of pretrained models that can be easily accessed from\n  the code in the same way as in `torchvision` (when you type `resnet50(pretrained=True)`).\n\n* OML is integrated with [PyTorch Lightning](https://www.pytorchlightning.ai/), so, we can use the power of its\n  [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html).\n  This is especially helpful when we work with DDP, so, you compare our\n  [DDP example](https://open-metric-learning.readthedocs.io/en/latest/examples/python.html)\n  and the\n  [PMLs one](https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/DistributedTripletMarginLossMNIST.ipynb).\n  By the way, PML also has [Trainers](https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/), but it's not\n  in the examples and custom `train` / `test` functions are used instead.\n\nWe believe that having Config API, laconic examples, and Zoo of pretrained models sets the entry threshold to a really low value.\n\n</p>\n</details>\n\n\n<details>\n<summary>What is Metric Learning?</summary>\n<p>\n\nMetric Learning problem (also known as *extreme classification* problem) means a situation in which we\nhave thousands of ids of some entities, but only a few samples for every entity.\nOften we assume that during the test stage (or production) we will deal with unseen entities\nwhich makes it impossible to apply the vanilla classification pipeline directly. In many cases obtained embeddings\nare used to perform search or matching procedures over them.\n\nHere are a few examples of such tasks from the computer vision sphere:\n* Person/Animal Re-Identification\n* Face Recognition\n* Landmark Recognition\n* Searching engines for online shops\n and many others.\n</p>\n</details>\n\n\n<details>\n<summary>Glossary (Naming convention) </summary>\n<p>\n\n* `embedding` - model's output (also known as `features vector` or `descriptor`).\n* `query` - a sample which is used as a request in the retrieval procedure.\n* `gallery set` - the set of entities to search items similar to `query` (also known as `reference` or `index`).\n* `Sampler` - an argument for `DataLoader` which is used to form batches\n* `Miner` - the object to form pairs or triplets after the batch was formed by `Sampler`. It's not necessary to form\n  the combinations of samples only inside the current batch, thus, the memory bank may be a part of `Miner`.\n* `Samples`/`Labels`/`Instances` - as an example let's consider DeepFashion dataset. It includes thousands of\n  fashion item ids (we name them `labels`) and several photos for each item id\n  (we name the individual photo as `instance` or `sample`). All of the fashion item ids have their groups like\n  \"skirts\", \"jackets\", \"shorts\" and so on (we name them `categories`).\n  Note, we avoid using the term `class` to avoid misunderstanding.\n* `training epoch` - batch samplers which we use for combination-based losses usually have a length equal to\n  `[number of labels in training dataset] / [numbers of labels in one batch]`. It means that we don't observe all of\n  the available training samples in one epoch (as opposed to vanilla classification),\n  instead, we observe all of the available labels.\n\n</p>\n</details>\n\n\n<details>\n<summary>How does OML work under the hood? </summary>\n<p>\n\n**Training part** implies using losses, well-established for metric learning, such as the angular losses\n(like *ArcFace*) or the combinations based losses (like *TripletLoss* or *ContrastiveLoss*).\nThe latter benefits from effective mining schemas of triplets/pairs, so we pay great attention to it.\nThus, during the training we:\n   1. Use `DataLoader` + `Sampler` to form batches (for example `BalanceSampler`)\n   2. [Only for losses based on combinations] Use `Miner` to form effective pairs or triplets, including those which utilize a memory bank.\n   3. Compute loss.\n\n**Validation part** consists of several steps:\n  1. Accumulating all of the embeddings (`EmbeddingMetrics`).\n  2. Calculating distances between them with respect to query/gallery split.\n  3. Applying some specific retrieval techniques like query reranking or score normalisation.\n  4. Calculating retrieval metrics like *CMC@k*, *Precision@k* or *MeanAveragePrecision@k*.\n\n</p>\n</details>\n\n\n<details>\n<summary>What about Self-Supervised Learning?</summary>\n<p>\n\nRecent research in SSL definitely obtained great results. The problem is that these approaches\nrequired an enormous amount of computing to train the model. But in our framework, we consider the most common case\nwhen the average user has no more than a few GPUs.\n\nAt the same time, it would be unwise to ignore success in this sphere, so we still exploit it in two ways:\n* As a source of checkpoints that would be great to start training with. From publications and our experience,\n  they are much better as initialisation than the default supervised model trained on ImageNet. Thus, we added the possibility\n  to initialise your models using these pretrained checkpoints only by passing an argument in the config or the constructor.\n* As a source of inspiration. For example, we adapted the idea of a memory bank from *MoCo* for the *TripletLoss*.\n\n</p>\n</details>\n\n\n<details>\n<summary>Do I need to know other frameworks to use OML?</summary>\n<p>\n\nNo, you don't. OML is a framework-agnostic. Despite we use PyTorch Lightning as a loop\nrunner for the experiments, we also keep the possibility to run everything on pure PyTorch.\nThus, only the tiny part of OML is Lightning-specific and we keep this logic separately from\nother code (see `oml.lightning`). Even when you use Lightning, you don't need to know it, since\nwe provide ready to use [Config API](https://github.com/OML-Team/open-metric-learning/blob/main/examples/).\n\nThe possibility of using pure PyTorch and modular structure of the code leaves a room for utilizing\nOML with your favourite framework after the implementation of the necessary wrappers.\n\n</p>\n</details>\n\n\n<details>\n<summary>Can I use OML without any knowledge in DataScience?</summary>\n<p>\n\nYes. To run the experiment with [Config API](https://github.com/OML-Team/open-metric-learning/blob/main/examples/)\nyou only need to write a converter\nto our format (it means preparing the\n`.csv` table with 5 predefined columns).\nThat's it!\n\nProbably we already have a suitable pre-trained model for your domain\nin our *Models Zoo*. In this case, you don't even need to train it.\n</p>\n</details>\n\n## Documentation\n\nDocumentation is available via the [link](https://open-metric-learning.readthedocs.io/en/latest/index.html).\n\n## Installation\n\nOML is available in PyPI:\n\n```shell\npip install -U open-metric-learning\n```\n\nYou can also pull the prepared image from DockerHub...\n\n```shell\ndocker pull omlteam/oml:gpu\ndocker pull omlteam/oml:cpu\n```\n\n...or build one by your own\n\n```shell\nmake docker_build RUNTIME=cpu\nmake docker_build RUNTIME=gpu\n```\n\n## Get started using Config API\n\nUsing configs is the best option if your dataset and pipeline are standard enough or if you are not\nexperienced in Machine Learning or Python. You can find more details in the\n[examples](https://github.com/OML-Team/open-metric-learning/blob/main/examples/).\n\n## Get started using Python\n\nThe most flexible, but knowledge-requiring approach.\nYou are not limited by our project structure and you can use only that part of the functionality which you need.\nYou can start with fully working code snippets below that train and validate the model\non a tiny dataset of\n[figures](https://drive.google.com/drive/folders/1plPnwyIkzg51-mLUXWTjREHgc1kgGrF4?usp=sharing).\n\n\n<details>\n<summary>Training</summary>\n<p>\n\n[comment]:vanilla-train-start\n```python\nimport torch\nfrom tqdm import tqdm\n\nfrom oml.datasets.base import DatasetWithLabels\nfrom oml.losses.triplet import TripletLossWithMiner\nfrom oml.miners.inbatch_all_tri import AllTripletsMiner\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.samplers.balance import BalanceSampler\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\ndataset_root = \"mock_dataset/\"\ndf_train, _ = download_mock_dataset(dataset_root)\n\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False).train()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n\ntrain_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)\ncriterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner())\nsampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=2)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)\n\nfor batch in tqdm(train_loader):\n    embeddings = model(batch[\"input_tensors\"])\n    loss = criterion(embeddings, batch[\"labels\"])\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n[comment]:vanilla-train-end\n</p>\n</details>\n\n\n<details>\n<summary>Validation</summary>\n<p>\n\n[comment]:vanilla-validation-start\n```python\nimport torch\nfrom tqdm import tqdm\n\nfrom oml.datasets.base import DatasetQueryGallery\nfrom oml.metrics.embeddings import EmbeddingMetrics\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\ndataset_root =  \"mock_dataset/\"\n_, df_val = download_mock_dataset(dataset_root)\n\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False).eval()\n\nval_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)\n\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\ncalculator = EmbeddingMetrics()\ncalculator.setup(num_samples=len(val_dataset))\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        batch[\"embeddings\"] = model(batch[\"input_tensors\"])\n        calculator.update_data(batch)\n\nmetrics = calculator.compute_metrics()\n```\n[comment]:vanilla-validation-end\n</p>\n</details>\n\n<details>\n<summary>Training + Validation [Lightning]</summary>\n<p>\n\n[comment]:lightning-start\n```python\nimport pytorch_lightning as pl\nimport torch\n\nfrom oml.datasets.base import DatasetQueryGallery, DatasetWithLabels\nfrom oml.lightning.modules.retrieval import RetrievalModule\nfrom oml.lightning.callbacks.metric import MetricValCallback\nfrom oml.losses.triplet import TripletLossWithMiner\nfrom oml.metrics.embeddings import EmbeddingMetrics\nfrom oml.miners.inbatch_all_tri import AllTripletsMiner\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.samplers.balance import BalanceSampler\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\ndataset_root =  \"mock_dataset/\"\ndf_train, df_val = download_mock_dataset(dataset_root)\n\n# model\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False)\n\n# train\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\ntrain_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)\ncriterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner())\nbatch_sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=3)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=batch_sampler)\n\n# val\nval_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\nmetric_callback = MetricValCallback(metric=EmbeddingMetrics())\n\n# run\npl_model = RetrievalModule(model, criterion, optimizer)\ntrainer = pl.Trainer(max_epochs=1, callbacks=[metric_callback], num_sanity_val_steps=0)\ntrainer.fit(pl_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n```\n[comment]:lightning-end\n</p>\n</details>\n\u3164\n\u3164\n\nIf you want to train your model in the DDP regime (Distributed Data Parallel), you\nonly need to slightly change only few lines of code in the example below.\n\n<details>\n<summary>Training + Validation [Lightning Distributed]</summary>\n<p>\n\n[comment]:lightning-ddp-start\n```python\nimport pytorch_lightning as pl\nimport torch\n\nfrom oml.datasets.base import DatasetQueryGallery, DatasetWithLabels\nfrom oml.lightning.modules.retrieval import RetrievalModuleDDP\nfrom oml.lightning.callbacks.metric import MetricValCallbackDDP\nfrom oml.losses.triplet import TripletLossWithMiner\nfrom oml.metrics.embeddings import EmbeddingMetricsDDP\nfrom oml.miners.inbatch_all_tri import AllTripletsMiner\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.samplers.balance import BalanceSampler\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\ndataset_root = \"mock_dataset/\"\ndf_train, df_val = download_mock_dataset(dataset_root)\n\n# model\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False)\n\n# train\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\ntrain_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)\ncriterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner())\nbatch_sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=3)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=batch_sampler)\n\n# val\nval_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\nmetric_callback = MetricValCallbackDDP(metric=EmbeddingMetricsDDP())  # DDP specific\n\n# run\npl_model = RetrievalModuleDDP(model=model, criterion=criterion, optimizer=optimizer,\n                              loaders_train=train_loader, loaders_val=val_loader  # DDP specific\n                              )\n\nddp_args = {\"accelerator\": \"auto\", \"devices\": 2, \"strategy\": pl.plugins.DDPPlugin(), \"replace_sampler_ddp\": False} # DDP specific\ntrainer = pl.Trainer(max_epochs=1, callbacks=[metric_callback], num_sanity_val_steps=0, **ddp_args)\ntrainer.fit(pl_model)  # we don't pass loaders to .fit() in DDP\n```\n[comment]:lightning-ddp-end\n</p>\n</details>\n\n## Usage with PyTorch Metric Learning\n\nYou can easily access a lot of content from [PyTorch Metric Learning](https://github.com/KevinMusgrave/pytorch-metric-learning)\nwith our library. You can see that the examples below are different from the basic ones only in a few lines of code:\n\n<details>\n<summary>Training with loss from PML</summary>\n<p>\n\n```python\nimport torch\nfrom tqdm import tqdm\n\nfrom oml.datasets.base import DatasetWithLabels\nfrom oml.losses.triplet import TripletLossWithMiner\nfrom oml.miners.inbatch_all_tri import AllTripletsMiner\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.samplers.balance import BalanceSampler\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\nfrom pytorch_metric_learning import losses, distances, reducers, miners\n\ndataset_root = \"mock_dataset/\"\ndf_train, _ = download_mock_dataset(dataset_root)\n\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False).train()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n\ntrain_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)\n\n# PML specific\n# criterion = losses.TripletMarginLoss(margin=0.2, triplets_per_anchor=\"all\")\ncriterion = losses.ArcFaceLoss(num_classes=df_train[\"label\"].nunique(), embedding_size=model.feat_dim)  # for classification-like losses\n\nsampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=2)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)\n\nfor batch in tqdm(train_loader):\n    embeddings = model(batch[\"input_tensors\"])\n    loss = criterion(embeddings, batch[\"labels\"])\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n</p>\n</details>\n\n\n<details>\n<summary>Training with distance, reducer, miner and loss from PML</summary>\n<p>\n\n```python\nimport torch\nfrom tqdm import tqdm\n\nfrom oml.datasets.base import DatasetWithLabels\nfrom oml.losses.triplet import TripletLossWithMiner\nfrom oml.miners.inbatch_all_tri import AllTripletsMiner\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.samplers.balance import BalanceSampler\nfrom oml.utils.download_mock_dataset import download_mock_dataset\n\nfrom pytorch_metric_learning import losses, distances, reducers, miners\n\ndataset_root = \"mock_dataset/\"\ndf_train, _ = download_mock_dataset(dataset_root)\n\nmodel = ViTExtractor(\"vits16_dino\", arch=\"vits16\", normalise_features=False).train()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n\ntrain_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)\n\n# PML specific\ndistance = distances.LpDistance(p=2)\nreducer = reducers.ThresholdReducer(low=0)\ncriterion = losses.TripletMarginLoss()\nminer = miners.TripletMarginMiner(margin=0.2, distance=distance, type_of_triplets=\"all\")\n\nsampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=2)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)\n\nfor batch in tqdm(train_loader):\n    embeddings = model(batch[\"input_tensors\"])\n    loss = criterion(embeddings, batch[\"labels\"], miner(embeddings, batch[\"labels\"]))  # PML specific\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n</p>\n</details>\n\n\u3164\n\nNote, during the validation process OpenMetricLearning computes *L2* distances. Thus, when choosing a distance from PML,\nwe recommend you to pick `distances.LpDistance(p=2)`.\n\nTo use content from PyTorch Metric Learning with our Config API just follow the standard\n[tutorial](https://open-metric-learning.readthedocs.io/en/latest/examples/config.html#how-to-use-my-own-implementation-of-loss-model-augmentations-etc)\nof adding custom loss.\n\n## Zoo\n\nBelow are the models trained with OML on 4 public datasets.\nFor more details about the training process, please, visit *examples* submodule and it's\n[Readme](https://github.com/OML-Team/open-metric-learning/blob/main/examples/).\n\n|                            model                            | cmc1  |         dataset          |                                              weights                                              |                                           configs                                            | hash (the beginning) |\n|:-----------------------------------------------------------:|:-----:|:------------------------:|:-------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------:|:--------------------:|\n| `ViTExtractor(weights=\"vits16_inshop\", arch=\"vits16\", ...)` | 0.921 |    DeepFashion Inshop    |    [link](https://drive.google.com/file/d/1niX-TC8cj6j369t7iU2baHQSVN3MVJbW/view?usp=sharing)     | [link](https://github.com/OML-Team/open-metric-learning/tree/main/examples/inshop/configs)   |        e1017d        |\n|  `ViTExtractor(weights=\"vits16_sop\", arch=\"vits16\", ...)`   | 0.866 | Stanford Online Products |   [link](https://drive.google.com/file/d/1zuGRHvF2KHd59aw7i7367OH_tQNOGz7A/view?usp=sharing)      | [link](https://github.com/OML-Team/open-metric-learning/tree/main/examples/sop/configs)      |        85cfa5        |\n|  `ViTExtractor(weights=\"vits16_cars\", arch=\"vits16\", ...)`  | 0.907 |         CARS 196         |   [link](https://drive.google.com/drive/folders/17a4_fg94dox2sfkXmw-KCtiLBlx-ut-1?usp=sharing)    | [link](https://github.com/OML-Team/open-metric-learning/tree/main/examples/cars/configs)     |        9f1e59        |\n|  `ViTExtractor(weights=\"vits16_cub\", arch=\"vits16\", ...)`   | 0.837 |       CUB 200 2011       |   [link](https://drive.google.com/drive/folders/1TPCN-eZFLqoq4JBgnIfliJoEK48x9ozb?usp=sharing)    | [link](https://github.com/OML-Team/open-metric-learning/tree/main/examples/cub/configs)      |        e82633        |\n\nWe also provide an integration with the models pretrained by other researchers:\n\n|                        model                              |   Stanford Online Products |   DeepFashion InShop |   CUB 200 2011 |   CARS 196 |\n|:---------------------------------------------------------:|:--------------------------:|:--------------------:|:--------------:|:----------:|\n| `ViTCLIPExtractor(\"sber_vitb32_224\", \"vitb32_224\")`       |                      0.547 |                0.514 |          0.448 |      0.618 |\n| `ViTCLIPExtractor(\"sber_vitb16_224\", \"vitb16_224\")`       |                      0.565 |                0.565 |          0.524 |      0.648 |\n| `ViTCLIPExtractor(\"sber_vitl14_224\", \"vitl14_224\")`       |                      0.512 |                0.555 |          0.606 |      0.707 |\n| `ViTCLIPExtractor(\"openai_vitb32_224\", \"vitb32_224\")`     |                      0.612 |                0.491 |          0.560 |      0.693 |\n| `ViTCLIPExtractor(\"openai_vitb16_224\", \"vitb16_224\")`     |                      0.648 |                0.606 |          0.665 |      0.767 |\n| `ViTCLIPExtractor(\"openai_vitl14_224\", \"vitl14_224\")`     |                      0.670 |                0.675 |          0.745 |      0.844 |\n| `ViTExtractor(\"vits16_dino\", \"vits16\")`                   |                      0.629 |                0.456 |          0.693 |      0.313 |\n| `ViTExtractor(\"vits8_dino\", \"vits8\")`                     |                      0.637 |                0.478 |          0.703 |      0.344 |\n| `ViTExtractor(\"vitb16_dino\", \"vitb16\")`                   |                      0.636 |                0.464 |          0.626 |      0.340 |\n| `ViTExtractor(\"vitb8_dino\", \"vitb8\")`                     |                      0.673 |                0.548 |          0.546 |      0.342 |\n| `ResnetExtractor(\"resnet50_moco_v2\", \"resnet50\")`         |                      0.491 |                0.310 |          0.244 |      0.155 |\n\n*All figures above were obtained on the images with the sizes of 224 x 224.\nNote, that the models above expect the crop of the region of interest rather than the whole picture.\nIt is also important to say that different models expect different preprocessing.\nYou should use `norm_resize_albu_clip` for `ViTCLIPExtractor` and `norm_resize_albu` for all other models\n(note that you can find this transforms in `oml.registry.transforms.TRANSFORMS_REGISTRY`).*\n\n\nYou can specify the desired weights and architecture to automatically download pretrained checkpoint (by the analogue with `torchvision.models`):\n\n[comment]:checkpoint-start\n```python\nimport oml\nfrom oml.models.vit.vit import ViTExtractor\nfrom oml.registry.models import MODELS_REGISTRY\n\n# We are downloading vits16 pretrained on CARS dataset:\nmodel = ViTExtractor(weights=\"vits16_cars\", arch=\"vits16\", normalise_features=False)\n\n# You can also check other available pretrained models...\nprint(list(ViTExtractor.pretrained_models.keys()))\n\n# ...or check other available types of architectures\nprint(MODELS_REGISTRY)\n\n# It's also possible to use `weights` argument to directly pass the path to the checkpoint:\nmodel_from_disk = ViTExtractor(weights=oml.const.CKPT_SAVE_ROOT / \"vits16_cars.ckpt\", arch=\"vits16\", normalise_features=False)\n```\n[comment]:checkpoint-end\n\n## Contributing guide\n\nWe welcome new contributors! Please, see our\n[contributing guide](https://open-metric-learning.readthedocs.io/en/latest/from_readme/contributing.html).\n\n## Acknowledgments\n\n<a href=\"https://github.com/catalyst-team/catalyst\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png\" width=\"100\"/></a>\n\nThe project was started in 2020 as a module for [Catalyst](https://github.com/catalyst-team/catalyst) library.\nI want to thank people who worked with me on that module:\n[Julia Shenshina](https://github.com/julia-shenshina),\n[Nikita Balagansky](https://github.com/elephantmipt),\n[Sergey Kolesnikov](https://github.com/Scitator)\nand others.\n\nI would like to thank people who continue working on this pipeline when it became a separe project:\n[Julia Shenshina](https://github.com/julia-shenshina),\n[Misha Kindulov](https://github.com/b0nce),\n[Aleksei Tarasov](https://github.com/DaloroAT) and\n[Verkhovtsev Leonid](https://github.com/leoromanovich).\n\n<a href=\"https://www.newyorker.de/\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/New_Yorker.svg/1280px-New_Yorker.svg.png\" width=\"100\"/></a>\n\nI also want to thank NewYorker, since the part of functionality was developed (and used) by its computer vision team led by me.\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/OML-Team/open-metric-learning",
            "keywords": "data-science,computer-vision,deep-learning,pytorch,metric-learning,representation-learning,pytorch-lightning",
            "license": "Apache License 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "open-metric-learning",
            "package_url": "https://pypi.org/project/open-metric-learning/",
            "platform": null,
            "project_url": "https://pypi.org/project/open-metric-learning/",
            "project_urls": {
                "Bug Tracker": "https://github.com/OML-Team/open-metric-learning/issues",
                "Homepage": "https://github.com/OML-Team/open-metric-learning"
            },
            "release_url": "https://pypi.org/project/open-metric-learning/0.3.3/",
            "requires_dist": [
                "albumentations (==1.0.3)",
                "gdown (==4.5.1)",
                "grad-cam (==1.3.7)",
                "hydra-core (==1.2.0)",
                "jupyter (==1.0.0)",
                "matplotlib (==3.5.2)",
                "neptune-client (==0.14.2)",
                "numpy (==1.21.6)",
                "omegaconf (==2.2.2)",
                "opencv-python (==4.5.4.60)",
                "pandas (==1.1.5)",
                "pillow (==9.0.0)",
                "pytest (==6.2.2)",
                "python-dotenv (==0.17.0)",
                "pytorch-lightning (==1.5.9)",
                "torch (==1.12.0)",
                "torchvision (==0.13.0)",
                "validators (==0.20.0)"
            ],
            "requires_python": ">=3.7,<4.0",
            "summary": "OML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.",
            "version": "0.3.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15675744,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "e0022968efebd95c247568be1306a6f7",
                    "sha256": "a7be39872bff4ee55a6067c5e1c64d255c7fd3db9a96cbfd497fa819b29aa56c"
                },
                "downloads": -1,
                "filename": "open_metric_learning-0.3.3-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "e0022968efebd95c247568be1306a6f7",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<4.0",
                "size": 158394,
                "upload_time": "2022-11-06T17:10:47",
                "upload_time_iso_8601": "2022-11-06T17:10:47.746805Z",
                "url": "https://files.pythonhosted.org/packages/c7/a0/44f3745b477313b8c07d6df9fc189b32fa5b830282cf09b3b1bba8dc522e/open_metric_learning-0.3.3-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "95e03ff2ee63a6e22d0b56dae56feb5b",
                    "sha256": "ac96bbf6c1ff968896dec4a1dba9a4a5efd0f7c99d82373159c70a5ca06b01e7"
                },
                "downloads": -1,
                "filename": "open-metric-learning-0.3.3.tar.gz",
                "has_sig": false,
                "md5_digest": "95e03ff2ee63a6e22d0b56dae56feb5b",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<4.0",
                "size": 122587,
                "upload_time": "2022-11-06T17:10:49",
                "upload_time_iso_8601": "2022-11-06T17:10:49.792459Z",
                "url": "https://files.pythonhosted.org/packages/6a/79/2735fdf2b248cb86683ef9dc2ca24596f17115285896c98f2cd2498608d6/open-metric-learning-0.3.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}