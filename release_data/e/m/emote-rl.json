{
    "22.0.0": {
        "info": {
            "author": "",
            "author_email": "Embark Studios <python@embark-studios.com>",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "<!-- Allow this file to not have a first line heading -->\n<!-- markdownlint-disable-file MD041 -->\n\n<!-- inline html -->\n<!-- markdownlint-disable-file MD033 -->\n\n<div align=\"center\">\n\n# `\ud83c\udf52 emote`\n\n**E**mbark's **Mo**dular **T**raining **E**ngine - a flexible framework for\nreinforcement learning\n\n[![Embark](https://img.shields.io/badge/embark-open%20source-blueviolet.svg)](https://embark.dev)\n[![Embark](https://img.shields.io/badge/discord-ark-%237289da.svg?logo=discord)](https://discord.gg/dAuKfZS)\n[![Build status](https://badge.buildkite.com/968ac3c0bb075fb878f9f973ed91406c8b257b0f050c197542.svg?theme=github&branch=main)](https://buildkite.com/embark-studios/emote)\n[![Docs status](https://img.shields.io/badge/Docs-latest-brightgreen)](https://static.embark.net/emote-docs/)\n\n\ud83d\udea7 This project is very much **work in progress and not yet ready for production use.** \ud83d\udea7\n\n</div>\n\n\n## What it does\n\nEmote provides a way to build reusable components for creating reinforcement learning algorithms, and a\nlibrary of premade componenents built in this way. It is strongly inspired by the callback setup used\nby Keras and FastAI.\n\nAs an example, let us see how the SAC, the Soft Actor Critic algorithm by\n[Haarnoja et al.](https://arxiv.org/abs/1801.01290) can be written using Emote. The main algorithm in\nSAC is given in [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905) and\nlooks like this:\n\n<div align=\"center\">\n\n![Main SAC algorithm](./docs/haarnoja_sac.png)\n\n</div>\n\nUsing the components provided with Emote, we can write this as\n\n```python\ndevice = torch.device(\"cpu\")\nenv = DictGymWrapper(AsyncVectorEnv(10 * [HitTheMiddle]))\ntable = DictObsTable(spaces=env.dict_space, maxlen=1000, device=device)\nmemory_proxy = TableMemoryProxy(table)\ndataloader = MemoryLoader(table, 100, 2, \"batch_size\")\n\nq1 = QNet(2, 1)\nq2 = QNet(2, 1)\npolicy = Policy(2, 1)\nln_alpha = torch.tensor(1.0, requires_grad=True)\nagent_proxy = FeatureAgentProxy(policy, device)\n\ncallbacks = [\n    QLoss(name=\"q1\", q=q1, opt=Adam(q1.parameters(), lr=8e-3)),\n    QLoss(name=\"q2\", q=q2, opt=Adam(q2.parameters(), lr=8e-3)),\n    PolicyLoss(pi=policy, ln_alpha=ln_alpha, q=q1, opt=Adam(policy.parameters())),\n    AlphaLoss(pi=policy, ln_alpha=ln_alpha, opt=Adam([ln_alpha]), n_actions=1),\n    QTarget(pi=policy, ln_alpha=ln_alpha, q1=q1, q2=q2),\n    SimpleGymCollector(env, agent_proxy, memory_proxy, warmup_steps=500),\n    FinalLossTestCheck([logged_cbs[2]], [10.0], 2000),\n]\n\ntrainer = Trainer(callbacks, dataloader)\ntrainer.train()\n```\n\nHere each callback in the `callbacks` list is its own reusable class that can readily be used\nfor other similar algorithms. The callback classes themselves are very straight forward to write.\nAs an example, here is the `PolicyLoss` callback.\n\n```python\nclass PolicyLoss(LossCallback):\n    def __init__(\n        self,\n        *,\n        pi: nn.Module,\n        ln_alpha: torch.tensor,\n        q: nn.Module,\n        opt: optim.Optimizer,\n        max_grad_norm: float = 10.0,\n        name: str = \"policy\",\n        data_group: str = \"default\",\n    ):\n        super().__init__(\n            name=name,\n            optimizer=opt,\n            network=pi,\n            max_grad_norm=max_grad_norm,\n            data_group=data_group,\n        )\n        self.policy = pi\n        self._ln_alpha = ln_alpha\n        self.q1 = q\n        self.q2 = q2\n\n    def loss(self, observation):\n        p_sample, logp_pi = self.policy(**observation)\n        q_pi_min = self.q1(p_sample, **observation)\n        # using reparameterization trick\n        alpha = torch.exp(self._ln_alpha).detach()\n        policy_loss = alpha * logp_pi - q_pi_min\n        policy_loss = torch.mean(policy_loss)\n        assert policy_loss.dim() == 0\n        return policy_loss\n```\n\n## Installation\n\n\nFor installation and environment handling we use `pdm`. Install it from [pdm](https://pdm.fming.dev/latest/#installation). After `pdm` is set up, set up and activate the emote environment by running\n\n```bash\npdm install\n```\n\n## Contribution\n\n[![Contributor Covenant](https://img.shields.io/badge/contributor%20covenant-v1.4-ff69b4.svg)](../main/CODE_OF_CONDUCT.md)\n\nWe welcome community contributions to this project.\n\nPlease read our [Contributor Guide](CONTRIBUTING.md) for more information on how to get started.\nPlease also read our [Contributor Terms](CONTRIBUTING.md#contributor-terms) before you make any contributions.\n\nAny contribution intentionally submitted for inclusion in an Embark Studios project, shall comply with the Rust standard licensing model (MIT OR Apache 2.0) and therefore be dual licensed as described below, without any additional terms or conditions:\n\n### License\n\nThis contribution is dual licensed under EITHER OF\n\n* Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or <http://www.apache.org/licenses/LICENSE-2.0>)\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or <http://opensource.org/licenses/MIT>)\n\nat your option.\n\nFor clarity, \"your\" refers to Embark or any other licensee/user of the contribution.\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "emote-rl",
            "package_url": "https://pypi.org/project/emote-rl/",
            "platform": null,
            "project_url": "https://pypi.org/project/emote-rl/",
            "project_urls": {
                "repository": "https://github.com/EmbarkStudios/emote"
            },
            "release_url": "https://pypi.org/project/emote-rl/22.0.0/",
            "requires_dist": [
                "setuptools==59.5",
                "tensorboard>=2.8.0",
                "gym[atari,classic_control]~=0.23.0; extra == \"atari\"",
                "box2d~=2.3.10; extra == \"atari\"",
                "pygame~=2.1.0; extra == \"atari\"",
                "gsutil>=4.66; extra == \"ci\"",
                "emote-rl[atari,torch-cpu]; extra == \"ci\"",
                "torch==1.10.2; extra == \"torch-cpu\""
            ],
            "requires_python": "~=3.8",
            "summary": "A modular reinforcement learning library",
            "version": "22.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15608754,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "706ece5acbbaefdaca7386ce85bf5563",
                    "sha256": "6e76675246346c683e98e9f6a7924916096905f11bac81169a54fe241ca4b671"
                },
                "downloads": -1,
                "filename": "emote_rl-22.0.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "706ece5acbbaefdaca7386ce85bf5563",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": "~=3.8",
                "size": 53364,
                "upload_time": "2022-11-01T11:03:43",
                "upload_time_iso_8601": "2022-11-01T11:03:43.004212Z",
                "url": "https://files.pythonhosted.org/packages/0c/f8/d0670a5d74e3b224d8eca42edbb7bb8d50cfc09eb28ad252c5c485be027d/emote_rl-22.0.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "8bbe188cbb7f6a4616bbda1018ac47ac",
                    "sha256": "1b797abcdcac7e5e83af88a27a98ad4ab6661ae603585f04ab94bdcb8a01f314"
                },
                "downloads": -1,
                "filename": "emote-rl-22.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "8bbe188cbb7f6a4616bbda1018ac47ac",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": "~=3.8",
                "size": 49942,
                "upload_time": "2022-11-01T11:03:44",
                "upload_time_iso_8601": "2022-11-01T11:03:44.618587Z",
                "url": "https://files.pythonhosted.org/packages/05/56/1f55cfb390ee9ca7356fc94f80b4a33d9026506945d55a0f8711e48fb0f5/emote-rl-22.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}