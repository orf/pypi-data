{
    "0.0.1": {
        "info": {
            "author": "Wang Ming Rui",
            "author_email": "mingruimingrui@hotmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules",
                "Topic :: Software Development :: Localization"
            ],
            "description": "**ICU-tokenizer** is a python package used to perform universal language\nnormalization and tokenization using the International Components for\nUnicode.\n\n- [Install](#install)\n- [Usage (Python)](#usage-python)\n  - [Sentence splitter](#sentence-splitter)\n  - [Normalizer](#normalizer)\n  - [Tokenizer](#tokenizer)\n\n## Install\n\nSee [./INSTALL.md](./INSTALL.md)\n\n## Usage (Python)\n\n### Sentence splitter\n\n```py\n# To split a paragraph into multiple sentences\n>>> from icu_tokenizer import SentSplitter\n>>> splitter = SentSplitter('zh')\n\n>>> paragraph = \"\"\"\n\u7f8e\u56fd\u6700\u9ad8\u6cd5\u9662\uff08\u82f1\u8bed\uff1aSupreme Court of the United States\uff09\uff0c\u4e00\u822c\u662f\u6307\u7f8e\u56fd\u8054\u90a6\u6700\u9ad8\u6cd5\u9662\uff0c\u662f\u7f8e\u56fd\u6700\u9ad8\u7ea7\u522b\u7684\u8054\u90a6\u6cd5\u9662\uff0c\u4e3a\u7f8e\u56fd\u4e09\u6743\u7ee7\u603b\u7edf\u3001\u56fd\u4f1a\u540e\u6700\u4e3a\u91cd\u8981\u7684\u4e00\u73af\u3002\u6839\u636e1789\u5e74\u300a\u7f8e\u56fd\u5baa\u6cd5\u7b2c\u4e09\u6761\u300b\u7684\u89c4\u5b9a\uff0c\u6700\u9ad8\u6cd5\u9662\u5bf9\u6240\u6709\u8054\u90a6\u6cd5\u9662\u3001\u5dde\u6cd5\u9662\u548c\u6d89\u53ca\u8054\u90a6\u6cd5\u5f8b\u95ee\u9898\u7684\u8bc9\u8bbc\u6848\u4ef6\u5177\u6709\u6700\u7ec8\uff08\u5e76\u4e14\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u6709\u659f\u914c\u51b3\u5b9a\u6743\u7684\uff09\u4e0a\u8bc9\u7ba1\u8f96\u6743\uff0c\u4ee5\u53ca\u5bf9\u5c0f\u8303\u56f4\u6848\u4ef6\u7684\u5177\u6709\u521d\u5ba1\u7ba1\u8f96\u6743\u3002\u5728\u7f8e\u56fd\u7684\u6cd5\u5f8b\u5236\u5ea6\u4e2d\uff0c\u6700\u9ad8\u6cd5\u9662\u901a\u5e38\u662f\u5305\u62ec\u300a\u7f8e\u56fd\u5baa\u6cd5\u300b\u5728\u5185\u7684\u8054\u90a6\u6cd5\u5f8b\u7684\u6700\u7ec8\u89e3\u91ca\u8005\uff0c\u4f46\u4ec5\u5728\u5177\u6709\u7ba1\u8f96\u6743\u7684\u6848\u4ef6\u8303\u56f4\u5185\u3002\u6cd5\u9662\u4e0d\u4eab\u6709\u5224\u5b9a\u653f\u6cbb\u95ee\u9898\u7684\u6743\u529b\uff1b\u653f\u6cbb\u95ee\u9898\u7684\u6267\u6cd5\u673a\u5173\u662f\u884c\u653f\u673a\u5173\uff0c\u800c\u4e0d\u662f\u653f\u5e9c\u7684\u53f8\u6cd5\u90e8\u95e8\u3002\n\"\"\"\n>>> splitter.split(paragraph)\n[\n    '\u7f8e\u56fd\u6700\u9ad8\u6cd5\u9662\uff08\u82f1\u8bed\uff1aSupreme Court of the United States\uff09\uff0c\u4e00\u822c\u662f\u6307\u7f8e\u56fd\u8054\u90a6\u6700\u9ad8\u6cd5\u9662\uff0c\u662f\u7f8e\u56fd\u6700\u9ad8\u7ea7\u522b\u7684\u8054\u90a6\u6cd5\u9662\uff0c\u4e3a\u7f8e\u56fd\u4e09\u6743\u7ee7\u603b\u7edf\u3001\u56fd\u4f1a\u540e\u6700\u4e3a\u91cd\u8981\u7684\u4e00\u73af\u3002',\n    '\u6839\u636e1789\u5e74\u300a\u7f8e\u56fd\u5baa\u6cd5\u7b2c\u4e09\u6761\u300b\u7684\u89c4\u5b9a\uff0c\u6700\u9ad8\u6cd5\u9662\u5bf9\u6240\u6709\u8054\u90a6\u6cd5\u9662\u3001\u5dde\u6cd5\u9662\u548c\u6d89\u53ca\u8054\u90a6\u6cd5\u5f8b\u95ee\u9898\u7684\u8bc9\u8bbc\u6848\u4ef6\u5177\u6709\u6700\u7ec8\uff08\u5e76\u4e14\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u6709\u659f\u914c\u51b3\u5b9a\u6743\u7684\uff09\u4e0a\u8bc9\u7ba1\u8f96\u6743\uff0c\u4ee5\u53ca\u5bf9\u5c0f\u8303\u56f4\u6848\u4ef6\u7684\u5177\u6709\u521d\u5ba1\u7ba1\u8f96\u6743\u3002',\n    '\u5728\u7f8e\u56fd\u7684\u6cd5\u5f8b\u5236\u5ea6\u4e2d\uff0c\u6700\u9ad8\u6cd5\u9662\u901a\u5e38\u662f\u5305\u62ec\u300a\u7f8e\u56fd\u5baa\u6cd5\u300b\u5728\u5185\u7684\u8054\u90a6\u6cd5\u5f8b\u7684\u6700\u7ec8\u89e3\u91ca\u8005\uff0c\u4f46\u4ec5\u5728\u5177\u6709\u7ba1\u8f96\u6743\u7684\u6848\u4ef6\u8303\u56f4\u5185\u3002',\n    '\u6cd5\u9662\u4e0d\u4eab\u6709\u5224\u5b9a\u653f\u6cbb\u95ee\u9898\u7684\u6743\u529b\uff1b\u653f\u6cbb\u95ee\u9898\u7684\u6267\u6cd5\u673a\u5173\u662f\u884c\u653f\u673a\u5173\uff0c\u800c\u4e0d\u662f\u653f\u5e9c\u7684\u53f8\u6cd5\u90e8\u95e8\u3002'\n]\n```\n\n### Normalizer\n\n```py\n# To normalize text\n>>> from icu_tokenizer import Normalizer\n>>> normalizer = Normalizer(lang='en', norm_puncts=True)\n\n>>> text = \"\ud835\udc7b\ud835\udc89\ud835\udc86 \ud835\udc91\ud835\udc93\ud835\udc90\ud835\udc85\ud835\udc96\ud835\udc84\ud835\udc95\ud835\udc94 \ud835\udc9a\ud835\udc90\ud835\udc96 \ud835\udc90\ud835\udc93\ud835\udc85\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc85 \ud835\udc98\ud835\udc8a\ud835\udc8d\ud835\udc8d \ud835\udc83\ud835\udc86 \ud835\udc94\ud835\udc89\ud835\udc8a\ud835\udc91\ud835\udc91\ud835\udc86\ud835\udc85 \ud835\udc85\ud835\udc8a\ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc95\ud835\udc8d\ud835\udc9a \ud835\udc87\ud835\udc93\ud835\udc90\ud835\udc8e \ud835\udc72\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc82.\"\n>>> normalizer.normalize(text)\n\"The products you ordered will be shipped directly from Korea.\"\n\n>>> text = \"\u3010\u3011\uff08\uff09\"\n>>> normalizer.normalize(text)\n\"[]()\"\n```\n\n### Tokenizer\n\n```py\n>>> from icu_tokenizer import Tokenizer\n>>> tokenizer = Tokenizer(lang='th')\n\n>>> text = \"\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e17\u0e35\u0e48\u0e21\u0e35\u0e23\u0e30\u0e14\u0e31\u0e1a\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e41\u0e19\u0e48\u0e19\u0e2d\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e40\u0e0a\u0e48\u0e19\u0e40\u0e14\u0e35\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e20\u0e32\u0e29\u0e32\u0e08\u0e35\u0e19 \u0e41\u0e25\u0e30\u0e2d\u0e2d\u0e01\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e41\u0e22\u0e01\u0e04\u0e33\u0e15\u0e48\u0e2d\u0e04\u0e33\"\n>>> tokenizer.tokenize(text)\n['\u0e20\u0e32\u0e29\u0e32', '\u0e44\u0e17\u0e22', '\u0e40\u0e1b\u0e47\u0e19', '\u0e20\u0e32\u0e29\u0e32', '\u0e17\u0e35\u0e48', '\u0e21\u0e35', '\u0e23\u0e30\u0e14\u0e31\u0e1a', '\u0e40\u0e2a\u0e35\u0e22\u0e07', '\u0e02\u0e2d\u0e07', '\u0e04\u0e33', '\u0e41\u0e19\u0e48\u0e19\u0e2d\u0e19', '\u0e2b\u0e23\u0e37\u0e2d', '\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c', '\u0e40\u0e0a\u0e48\u0e19', '\u0e40\u0e14\u0e35\u0e22\u0e27', '\u0e01\u0e31\u0e1a', '\u0e20\u0e32\u0e29\u0e32', '\u0e08\u0e35\u0e19', '\u0e41\u0e25\u0e30', '\u0e2d\u0e2d\u0e01', '\u0e40\u0e2a\u0e35\u0e22\u0e07', '\u0e41\u0e22\u0e01', '\u0e04\u0e33', '\u0e15\u0e48\u0e2d', '\u0e04\u0e33']\n```",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mingruimingrui/ICU-tokenizer",
            "keywords": "",
            "license": "MIT License",
            "maintainer": "",
            "maintainer_email": "",
            "name": "icu-tokenizer",
            "package_url": "https://pypi.org/project/icu-tokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/icu-tokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/mingruimingrui/ICU-tokenizer"
            },
            "release_url": "https://pypi.org/project/icu-tokenizer/0.0.1/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "ICU based universal language tokenizer",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 7507904,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "ae6f7955f673c7313e67fe205fdeba39",
                    "sha256": "0430f5191697904168769938fe3533ddd12e8afd5d84d43e852e9aee4a8b2447"
                },
                "downloads": -1,
                "filename": "icu_tokenizer-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "ae6f7955f673c7313e67fe205fdeba39",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 11153,
                "upload_time": "2020-06-18T17:41:54",
                "upload_time_iso_8601": "2020-06-18T17:41:54.254901Z",
                "url": "https://files.pythonhosted.org/packages/26/36/d4fdf94a2b58135d4df4b9c12c9602e18a6e1f0267e60802c3bf455e530d/icu_tokenizer-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}