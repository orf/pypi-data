{
    "0.0.3": {
        "info": {
            "author": "",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "![Build Status](https://dataproctemplatesci.com/buildStatus/icon?job=dataproc-templates-build%2Fbuild-job-python&&subject=python-build)\n\n# Dataproc Templates (Python - PySpark)\n\n* [BigQueryToGCS](/python/dataproc_templates/bigquery/README.md) (blogpost [link](https://medium.com/google-cloud/moving-data-from-bigquery-to-gcs-using-gcp-dataproc-serverless-and-pyspark-f6481b86bcd1))\n* [GCSToBigQuery](/python/dataproc_templates/gcs/README.md) (blogpost [link](https://medium.com/@ppaglilla/getting-started-with-dataproc-serverless-pyspark-templates-e32278a6a06e))\n* [GCSToBigTable](/python/dataproc_templates/gcs/README.md)\n* [GCSToJDBC](/python/dataproc_templates/gcs/README.md)\n* [GCSToMongo](/python/dataproc_templates/gcs/README.md) (blogpost [link](https://medium.com/google-cloud/importing-data-from-gcs-to-mongodb-using-dataproc-serverless-fed58904633a))\n* [HiveToBigQuery](/python/dataproc_templates/hive/README.md) (blogpost [link](https://medium.com/google-cloud/processing-data-from-hive-to-bigquery-using-pyspark-and-dataproc-serverless-217c7cb9e4f8))\n* [HiveToGCS](/python/dataproc_templates/hive/README.md)(blogpost [link](https://medium.com/@surjitsh/processing-large-data-tables-from-hive-to-gcs-using-pyspark-and-dataproc-serverless-35d3d16daaf))\n* [HbaseToGCS](/python/dataproc_templates/hbase/README.md)\n* [MongoToGCS](/python/dataproc_templates/mongo/README.md)\n* [JDBCToJDBC](/python/dataproc_templates/jdbc/README.md) (blogpost [link](https://medium.com/google-cloud/migrating-data-from-one-databases-into-another-via-jdbc-using-dataproc-serverless-c5336c409b18))\n* [JDBCToGCS](/python/dataproc_templates/jdbc/README.md) (blogpost [link](https://medium.com/google-cloud/importing-data-from-databases-into-gcs-via-jdbc-using-dataproc-serverless-f330cb0160f0))\n* [JDBCToBigQuery](/python/dataproc_templates/jdbc/README.md) (blogpost [link](https://medium.com/@sjlva/python-fast-export-large-database-tables-using-gcp-serverless-dataproc-bfe77a132485))\n* [RedshiftToGCS](/python/dataproc_templates/redshift/README.md)\n* [TextToBigQuery](/python/dataproc_templates/gcs/README.md)\n* [GCSToGCS](/python/dataproc_templates/gcs/README.md)\n* [SnowflakeoGCS](/python/dataproc_templates/snowflake/README.md)\n\nDataproc Templates (Python - PySpark) submit jobs to Dataproc Serverless using [batches submit pyspark](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark).\n\n## Setting up the local environment\n\nIt is recommended to use a [virtual environment](https://docs.python.org/3/library/venv.html) when setting up the local environment. This setup is not required for submitting templates, only for running and developing locally.\n\n``` bash\n# Create a virtual environment, activate it and install requirements\nmkdir venv\npython -m venv venv/\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n## Running unit tests\n\nUnit tests are developed using [`pytest`](https://docs.pytest.org/en/7.1.x/).\n\nTo run all unit tests, simply run pytest:\n\n``` bash\npytest\n```\n\nTo generate a coverage report, run the tests using coverage\n\n``` bash\ncoverage run \\\n  --source=dataproc_templates \\\n  --module pytest \\\n  --verbose \\\n  test\n\ncoverage report --show-missing\n```\n\n## Submitting templates to Dataproc Serverless\n\nA shell script is provided to:\n- Build the python package\n- Set Dataproc parameters based on environment variables\n- Submit the desired template to Dataproc with the provided template parameters\n\n<hr>\n\nWhen submitting, there are 3 types of properties/parameters for the user to provide.  \n- **Spark properties**: Refer to this [documentation](https://cloud.google.com/dataproc-serverless/docs/concepts/properties) to see the available spark properties.\n- **Each template's specific parameters**: refer to each template's README.\n- **Common arguments**: --template_name and --log_level\n  - The **--log_level** parameter is optional, it defaults to INFO.\n    - Possible choices are the Spark log levels: [\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"].\n\n\n\n\n\n<hr>\n\n**bin/start.sh usage**:\n\n```\n# Set required environment variables\nexport GCP_PROJECT=<project_id>\nexport REGION=<region>\nexport GCS_STAGING_LOCATION=<gs://path>\n\n# Set optional environment variables\nexport SUBNET=<subnet>\nexport JARS=\"gs://additional/dependency.jar\"\nexport HISTORY_SERVER_CLUSTER=projects/{projectId}/regions/{regionId}/clusters/{clusterId}\nexport METASTORE_SERVICE=projects/{projectId}/locations/{regionId}/services/{serviceId}\n\n# Submit to Dataproc passing template parameters\n./bin/start.sh [--properties=<spark.something.key>=<value>] \\\n               -- --template=TEMPLATENAME \\\n                  --log_level=INFO \\\n                  --my.property=\"<value>\" \\\n                  --my.other.property=\"<value>\"\n                  (etc...)\n```\n\n**gcloud CLI usage**:\n\nIt is also possible to submit jobs using the `gcloud` CLI directly. That can be achieved by:\n\n1. Building the `dataproc_templates` package into an `.egg`\n\n``` bash\nPACKAGE_EGG_FILE=dist/dataproc_templates_distribution.egg\npython setup.py bdist_egg --output=${PACKAGE_EGG_FILE}\n```\n\n2. Submitting the job\n  * The `main.py` file should be the main python script\n  * The `.egg` file for the package must be bundled using the `--py-files` flag\n\n```\ngcloud dataproc batches submit pyspark \\\n      --region=<region> \\\n      --project=<project_id> \\\n      --jars=\"<required_jar_dependencies>\" \\\n      --deps-bucket=<gs://path> \\\n      --subnet=<subnet> \\\n      --py-files=${PACKAGE_EGG_FILE} \\\n      [--properties=<spark.something.key>=<value>] \\\n      main.py \\\n      -- --template=TEMPLATENAME \\\n         --log_level=INFO \\\n         --<my.property>=\"<value>\" \\\n         --<my.other.property>=\"<value>\"\n         (etc...)\n```\n\n**Vertex AI usage**:\n\nFollow [Dataproc Templates (Jupyter Notebooks) README](../notebooks/README.md) to submit Dataproc Templates from a Vertex AI notebook.\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/GoogleCloudPlatform/dataproc-templates",
            "keywords": "",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "google-dataproc-templates",
            "package_url": "https://pypi.org/project/google-dataproc-templates/",
            "platform": "Posix; MacOS X; Windows",
            "project_url": "https://pypi.org/project/google-dataproc-templates/",
            "project_urls": {
                "Homepage": "https://github.com/GoogleCloudPlatform/dataproc-templates"
            },
            "release_url": "https://pypi.org/project/google-dataproc-templates/0.0.3/",
            "requires_dist": [
                "pyspark (>=3.2.0)",
                "google-cloud-bigquery (>=3.4.0)"
            ],
            "requires_python": ">=3.7",
            "summary": "Google Dataproc templates written in Python",
            "version": "0.0.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16020971,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "b5ef44209f01019c2577201b9b189183",
                    "sha256": "b2d82abc5f6c2bcf299e39624bc1d5cf8d6d2f38f30b2409279d54e7d24213fa"
                },
                "downloads": -1,
                "filename": "google_dataproc_templates-0.0.3-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "b5ef44209f01019c2577201b9b189183",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 73504,
                "upload_time": "2022-11-29T17:14:16",
                "upload_time_iso_8601": "2022-11-29T17:14:16.696654Z",
                "url": "https://files.pythonhosted.org/packages/5e/6a/51e8e5d167df657d13be5c65496ca910d6cce166bbe1e25fcf4766edea73/google_dataproc_templates-0.0.3-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d22ab1b1fcfb2d2e2008d9b8009730d4",
                    "sha256": "d5eb07a204a9a5f63d5f7272cea9755738d951469308f004c3bf07dedc859bb8"
                },
                "downloads": -1,
                "filename": "google-dataproc-templates-0.0.3.tar.gz",
                "has_sig": false,
                "md5_digest": "d22ab1b1fcfb2d2e2008d9b8009730d4",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 32606,
                "upload_time": "2022-11-29T17:14:20",
                "upload_time_iso_8601": "2022-11-29T17:14:20.008116Z",
                "url": "https://files.pythonhosted.org/packages/2e/03/b97cc17281ef2ddfca78e9009e96cb90d4ad52d32ad6dd84bac998bf52c5/google-dataproc-templates-0.0.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}