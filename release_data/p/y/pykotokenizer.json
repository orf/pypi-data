{
    "0.0.1": {
        "info": {
            "author": "Abdullah Al Imran",
            "author_email": "abdalimran@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/abdalimran/pykotokenizer",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pykotokenizer",
            "package_url": "https://pypi.org/project/pykotokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/pykotokenizer/",
            "project_urls": {
                "Bug Tracker": "https://github.com/pypa/pykotokenizer/issues",
                "Homepage": "https://github.com/abdalimran/pykotokenizer"
            },
            "release_url": "https://pypi.org/project/pykotokenizer/0.0.1/",
            "requires_dist": null,
            "requires_python": "<3.9,>=3.7",
            "summary": "Model-based Korean Text Tokenizer in Python",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12425072,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4c094142f035fe7f02d28ab968f9fddc",
                    "sha256": "6813f34d149e2d77c51df3f53f18ab3f9fd05d8a2bc19f0851e59769435e9833"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "4c094142f035fe7f02d28ab968f9fddc",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": "<3.9,>=3.7",
                "size": 11323075,
                "upload_time": "2021-12-14T14:32:26",
                "upload_time_iso_8601": "2021-12-14T14:32:26.061142Z",
                "url": "https://files.pythonhosted.org/packages/61/59/d583599835531cdc4cb387c68532beb3543642d3e39e5bfb0577cee306ce/pykotokenizer-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "9e41bfe63bdbec6a65491ce9326a6bfe",
                    "sha256": "7d4d1071a4a305e5600683670071af26577856da118be0209c6e8fc0d9371ec6"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "9e41bfe63bdbec6a65491ce9326a6bfe",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": "<3.9,>=3.7",
                "size": 11317638,
                "upload_time": "2021-12-14T14:32:30",
                "upload_time_iso_8601": "2021-12-14T14:32:30.160830Z",
                "url": "https://files.pythonhosted.org/packages/f0/22/328499cefbcfc205654e0d51a3b7676a699268a5444641b97171b92f7649/pykotokenizer-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.2": {
        "info": {
            "author": "Abdullah Al Imran",
            "author_email": "abdalimran@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/abdalimran/pykotokenizer",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pykotokenizer",
            "package_url": "https://pypi.org/project/pykotokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/pykotokenizer/",
            "project_urls": {
                "Bug Tracker": "https://github.com/pypa/pykotokenizer/issues",
                "Homepage": "https://github.com/abdalimran/pykotokenizer"
            },
            "release_url": "https://pypi.org/project/pykotokenizer/0.0.2/",
            "requires_dist": null,
            "requires_python": "<3.9,>=3.6",
            "summary": "Model-based Korean Text Tokenizer in Python",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12425072,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "0937da595b1d5a098da4516ffffad72d",
                    "sha256": "0ee06322920f3769dcb563a809579cb8875b7a0871d8d1f1ea7d49bff90e20d6"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "0937da595b1d5a098da4516ffffad72d",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": "<3.9,>=3.6",
                "size": 11323067,
                "upload_time": "2021-12-21T06:32:50",
                "upload_time_iso_8601": "2021-12-21T06:32:50.642828Z",
                "url": "https://files.pythonhosted.org/packages/26/a0/9423e1f1348f50357297a363a499eebfc37945a5b29d2f6ef5a707734ada/pykotokenizer-0.0.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "a813e7e79409370fb8c51c781effee01",
                    "sha256": "752cb23ea25988e5e1cb2ecc3324e0a0d16452e775a0479804dd7e6fa74df78b"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "a813e7e79409370fb8c51c781effee01",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": "<3.9,>=3.6",
                "size": 11316615,
                "upload_time": "2021-12-21T06:32:54",
                "upload_time_iso_8601": "2021-12-21T06:32:54.898128Z",
                "url": "https://files.pythonhosted.org/packages/79/d9/1384a7baf7c02f83c1569a1d5767020654b5a27a5f51e66a29f3d7c13e4a/pykotokenizer-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.3": {
        "info": {
            "author": "Abdullah Al Imran",
            "author_email": "abdalimran@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3"
            ],
            "description": "# PyKoTokenizer\nPyKoTokenizer is a Korean text tokenizer for Korean Natural Language Processing tasks. It includes deep learning (RNN) model-based word tokenizers as well as morphological analyzer based word tokenizers for Korean language.\n\n## Segmentation of Korean Words\nWritten Korean texts do employ white space characters. However, more often than not,\nKorean words occur in a text concatenated immediately to adjacent words without\nan intervening space character. This low degree of separation of words from\neach other in writing is due somewhat to an abundance of what linguists call \"endoclitics\" \nin the language.\n\nAs the language has been subjected to principled and rigorous study for a few\ndecades, the issue of which strings of sounds, or letters, are words and which are\nnot, has been settled among a small group of selected linguists. This kind of\nadvancement has not been propagated to the general public yet, and nlp\nengineers working on Korean cannot but make do with whatever inconsistent\ngrammars they happen to have access to. Thus, a major source of difficulty in\ndeveloping competent Korean text processors has been, and still is, the notion of a word\nas the smallest syntactic unit.\n\n## How to install\nBefore using this package please make sure you have the following dependencies installed in your system.\n* **Python >= 3.6**\n* **numpy >= 1.19.0**\n* **pandas >= 1.1.5**\n* **tensorflow >= 2.6.2**\n* **h5py >= 3.1.0**\n* **konlpy >= 0.5.2**\n\nUse the following command to install the package:\n```python\npip install pykotokenizer\n```\n\n## How to Use\n\n## Model-based Tokenizers\nBelow, we show examples of using model-based tokenizers.\n\n### Using KoTokenizer\n\n```python\nfrom pykotokenizer import KoTokenizer\n\ntokenizer = KoTokenizer()\n\nkorean_text = \"\uae40\ud615\ud638\uc601\ud654\uc2dc\uc7a5\ubd84\uc11d\uac00\ub294'1987'\uc758\ub124\uc774\ubc84\uc601\ud654\uc815\ubcf4\ub124\ud2f0\uc98c10\uc810\ud3c9\uc5d0\uc11c\uc5b8\uae09\ub41c\ub2e8\uc5b4\ub4e4\uc744\uc9c0\ub09c\ud57412\uc6d427\uc77c\ubd80\ud130\uc62c\ud5741\uc6d410\uc77c\uae4c\uc9c0\ud1b5\uacc4\ud504\ub85c\uadf8\ub7a8R\uacfcKoNLP\ud328\ud0a4\uc9c0\ub85c\ud14d\uc2a4\ud2b8\ub9c8\uc774\ub2dd\ud558\uc5ec\ubd84\uc11d\ud588\ub2e4.\"\n\ntokenizer(korean_text)\n```\n\n**Output:**\n```\n\"\uae40 \ud615\ud638 \uc601\ud654 \uc2dc\uc7a5 \ubd84\uc11d\uac00 \ub294 ' 1987 ' \uc758 \ub124\uc774\ubc84 \uc601\ud654 \uc815\ubcf4 \ub124\ud2f0\uc98c 10 \uc810\ud3c9 \uc5d0\uc11c \uc5b8\uae09 \ub41c \ub2e8\uc5b4 \ub4e4 \uc744 \uc9c0\ub09c \ud574 12 \uc6d4 27 \uc77c \ubd80\ud130 \uc62c\ud574 1 \uc6d4 10 \uc77c \uae4c\uc9c0 \ud1b5\uacc4 \ud504\ub85c\uadf8\ub7a8 R \uacfc KoNLP \ud328\ud0a4\uc9c0 \ub85c \ud14d\uc2a4\ud2b8 \ub9c8\uc774\ub2dd \ud558\uc5ec \ubd84\uc11d \ud588\ub2e4 .\"\n```\n\n### Using KoSpacing\n\n```python\nfrom pykotokenizer import KoSpacing\n\nspacing = KoSpacing()\n\nkorean_text = \"\uae40\ud615\ud638\uc601\ud654\uc2dc\uc7a5\ubd84\uc11d\uac00\ub294'1987'\uc758\ub124\uc774\ubc84\uc601\ud654\uc815\ubcf4\ub124\ud2f0\uc98c10\uc810\ud3c9\uc5d0\uc11c\uc5b8\uae09\ub41c\ub2e8\uc5b4\ub4e4\uc744\uc9c0\ub09c\ud57412\uc6d427\uc77c\ubd80\ud130\uc62c\ud5741\uc6d410\uc77c\uae4c\uc9c0\ud1b5\uacc4\ud504\ub85c\uadf8\ub7a8R\uacfcKoNLP\ud328\ud0a4\uc9c0\ub85c\ud14d\uc2a4\ud2b8\ub9c8\uc774\ub2dd\ud558\uc5ec\ubd84\uc11d\ud588\ub2e4.\"\n\nspacing(korean_text)\n```\n\n**Output:**\n```\n\"\uae40\ud615\ud638 \uc601\ud654\uc2dc\uc7a5 \ubd84\uc11d\uac00\ub294 '1987'\uc758 \ub124\uc774\ubc84 \uc601\ud654 \uc815\ubcf4 \ub124\ud2f0\uc98c 10\uc810 \ud3c9\uc5d0\uc11c \uc5b8\uae09\ub41c \ub2e8\uc5b4\ub4e4\uc744 \uc9c0\ub09c\ud574 12\uc6d4 27\uc77c\ubd80\ud130 \uc62c\ud574 1\uc6d4 10\uc77c\uae4c\uc9c0 \ud1b5\uacc4 \ud504\ub85c\uadf8\ub7a8 R\uacfc KoNLP \ud328\ud0a4\uc9c0\ub85c \ud14d\uc2a4\ud2b8\ub9c8\uc774\ub2dd\ud558\uc5ec \ubd84\uc11d\ud588\ub2e4.\"\n```\n\n## Morphological analyzer based Tokenizers\n\nBelow, we show examples of using morphological analyzer based tokenizers. These tokenizers has ***dependency*** on **[KoNLPy](https://konlpy.org/en/latest/)**. So, please install KoNLPy before using these. To install KoNLPy please visit this link - [https://konlpy.org/en/latest/install/](https://konlpy.org/en/latest/install/) and follow the procedure. KoNLPy requires Java in your system.\n\n### Using KoKkma\n\n```python\nfrom pykotokenizer import KoKkma\n\nkokkma = KoKkma()\n\nkorean_text = \"\uae40\ud615\ud638\uc601\ud654\uc2dc\uc7a5\ubd84\uc11d\uac00\ub294'1987'\uc758\ub124\uc774\ubc84\uc601\ud654\uc815\ubcf4\ub124\ud2f0\uc98c10\uc810\ud3c9\uc5d0\uc11c\uc5b8\uae09\ub41c\ub2e8\uc5b4\ub4e4\uc744\uc9c0\ub09c\ud57412\uc6d427\uc77c\ubd80\ud130\uc62c\ud5741\uc6d410\uc77c\uae4c\uc9c0\ud1b5\uacc4\ud504\ub85c\uadf8\ub7a8R\uacfcKoNLP\ud328\ud0a4\uc9c0\ub85c\ud14d\uc2a4\ud2b8\ub9c8\uc774\ub2dd\ud558\uc5ec\ubd84\uc11d\ud588\ub2e4.\"\n\nkokkma(korean_text)\n```\n\n**Output:**\n```\n\"\uae40 \ud615 \ud638 \uc601\ud654 \uc2dc\uc7a5 \ubd84\uc11d\uac00 \ub294 ' 1987 ' \uc758 \ub124\uc774\ubc84 \uc601\ud654 \uc815\ubcf4 \ub124\ud2f0\uc98c 10 \uc810 \ud3c9 \uc5d0\uc11c \uc5b8\uae09 \ub418 \u3134 \ub2e8\uc5b4 \ub4e4 \uc744 \uc9c0\ub09c\ud574 12 \uc6d4 27 \uc77c \ubd80\ud130 \uc62c\ud574 1 \uc6d4 10 \uc77c \uae4c\uc9c0 \ud1b5\uacc4 \ud504\ub85c\uadf8\ub7a8 R \uacfc KoNLP \ud328\ud0a4\uc9c0 \ub85c \ud14d\uc2a4\ud2b8 \ub9c8\uc774\ub2dd \ud558 \uc5ec \ubd84\uc11d \ud558 \uc5c8 \ub2e4 .\"\n```\n\n### Using KoKomoran\n\n```python\nfrom pykotokenizer import KoKomoran\n\nkokomoran = KoKomoran()\n\nkorean_text = \"\uae40\ud615\ud638\uc601\ud654\uc2dc\uc7a5\ubd84\uc11d\uac00\ub294'1987'\uc758\ub124\uc774\ubc84\uc601\ud654\uc815\ubcf4\ub124\ud2f0\uc98c10\uc810\ud3c9\uc5d0\uc11c\uc5b8\uae09\ub41c\ub2e8\uc5b4\ub4e4\uc744\uc9c0\ub09c\ud57412\uc6d427\uc77c\ubd80\ud130\uc62c\ud5741\uc6d410\uc77c\uae4c\uc9c0\ud1b5\uacc4\ud504\ub85c\uadf8\ub7a8R\uacfcKoNLP\ud328\ud0a4\uc9c0\ub85c\ud14d\uc2a4\ud2b8\ub9c8\uc774\ub2dd\ud558\uc5ec\ubd84\uc11d\ud588\ub2e4.\"\n\nkokomoran(korean_text)\n```\n\n**Output:**\n```\n\"\uae40\ud615\ud638 \uc601\ud654 \uc2dc\uc7a5 \ubd84\uc11d\uac00 \ub294 ' 1987 ' \uc758 \ub124\uc774\ubc84 \uc601\ud654 \uc815\ubcf4 \ub124\ud2f0\uc98c 10 \uc810 \ud3c9 \uc5d0\uc11c \uc5b8\uae09 \ub418 \u3134 \ub2e8\uc5b4 \ub4e4 \uc744 \uc9c0\ub09c\ud574 12\uc6d4 27 \uc77c \ubd80\ud130 \uc62c\ud574 1\uc6d4 10 \uc77c \uae4c\uc9c0 \ud1b5\uacc4 \ud504\ub85c\uadf8\ub7a8 R \uacfc KoNLP \ud328\ud0a4\uc9c0 \ub85c \ud14d\uc2a4\ud2b8 \ub9c8 \uc774\ub2dd \ud558 \uc544 \ubd84\uc11d \ud558 \uc558 \ub2e4 .\"\n```\n\n## Credits\nThis package is a revamped and customized version of the following two sources:\n* KoTokenizer: https://pypi.org/project/hangul-korean/\n* KoSpacing: https://github.com/haven-jeon/PyKoSpacing\n* KoNLPy: https://konlpy.org/en/latest/\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/abdalimran/pykotokenizer",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "pykotokenizer",
            "package_url": "https://pypi.org/project/pykotokenizer/",
            "platform": "",
            "project_url": "https://pypi.org/project/pykotokenizer/",
            "project_urls": {
                "Bug Tracker": "https://github.com/pypa/pykotokenizer/issues",
                "Homepage": "https://github.com/abdalimran/pykotokenizer"
            },
            "release_url": "https://pypi.org/project/pykotokenizer/0.0.3/",
            "requires_dist": null,
            "requires_python": "<3.9,>=3.6",
            "summary": "Model-based Korean Text Tokenizer in Python",
            "version": "0.0.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 12425072,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "73c773b9b583948b9116e3c3fc51c7c1",
                    "sha256": "4e20e6d0afe168530102b005973f0e78a4385abd4c9d6bc9a4713baadd0a7dc1"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.3-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "73c773b9b583948b9116e3c3fc51c7c1",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": "<3.9,>=3.6",
                "size": 11325642,
                "upload_time": "2021-12-28T19:34:55",
                "upload_time_iso_8601": "2021-12-28T19:34:55.852799Z",
                "url": "https://files.pythonhosted.org/packages/3b/51/c94b1251fe8786644242a5b766724620778c6d0e9d6355095676e6468a00/pykotokenizer-0.0.3-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "317894e4b7e6381375b9ed326e7b3c99",
                    "sha256": "da787f7be36c50b459b6735a3fc76a63bbb68f093ddbf25b534bce0ec5efddd1"
                },
                "downloads": -1,
                "filename": "pykotokenizer-0.0.3.tar.gz",
                "has_sig": false,
                "md5_digest": "317894e4b7e6381375b9ed326e7b3c99",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": "<3.9,>=3.6",
                "size": 11318177,
                "upload_time": "2021-12-28T19:34:59",
                "upload_time_iso_8601": "2021-12-28T19:34:59.763523Z",
                "url": "https://files.pythonhosted.org/packages/3b/2e/121126022e2f5f857609dcb5963290ccbb3844f6fc6c083a071a9c3ed305/pykotokenizer-0.0.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}